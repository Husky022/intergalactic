[
{
 "model": "admin.logentry",
 "pk": 1,
 "fields": {
  "action_time": "2021-10-15T05:48:02.551Z",
  "user": 1,
  "content_type": 6,
  "object_id": "4",
  "object_repr": "Научно-популярное",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 2,
 "fields": {
  "action_time": "2021-10-15T05:49:13.792Z",
  "user": 1,
  "content_type": 6,
  "object_id": "5",
  "object_repr": "Сетевые технологии",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 3,
 "fields": {
  "action_time": "2021-10-15T05:49:38.125Z",
  "user": 1,
  "content_type": 6,
  "object_id": "6",
  "object_repr": "Big Data",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 4,
 "fields": {
  "action_time": "2021-10-15T05:53:42.522Z",
  "user": 1,
  "content_type": 7,
  "object_id": "1",
  "object_repr": "Где живут программисты – интерактивная карта - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 5,
 "fields": {
  "action_time": "2021-10-15T05:55:13.867Z",
  "user": 1,
  "content_type": 7,
  "object_id": "1",
  "object_repr": "Где живут программисты – интерактивная карта - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 6,
 "fields": {
  "action_time": "2021-10-15T05:56:21.676Z",
  "user": 1,
  "content_type": 7,
  "object_id": "11",
  "object_repr": "Кто такой системный архитектор - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 7,
 "fields": {
  "action_time": "2021-10-15T05:56:58.201Z",
  "user": 1,
  "content_type": 7,
  "object_id": "11",
  "object_repr": "Кто такой системный архитектор - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 8,
 "fields": {
  "action_time": "2021-10-15T05:58:21.422Z",
  "user": 1,
  "content_type": 7,
  "object_id": "11",
  "object_repr": "Кто такой системный архитектор - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u044d\\u0433 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 9,
 "fields": {
  "action_time": "2021-10-15T05:58:43.896Z",
  "user": 1,
  "content_type": 7,
  "object_id": "1",
  "object_repr": "Где живут программисты – интерактивная карта - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u044d\\u0433 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 10,
 "fields": {
  "action_time": "2021-10-15T06:01:32.260Z",
  "user": 1,
  "content_type": 7,
  "object_id": "13",
  "object_repr": "DevOpsConf: информация к размышлению - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\", \"\\u0422\\u044d\\u0433 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 11,
 "fields": {
  "action_time": "2021-10-15T06:02:46.426Z",
  "user": 1,
  "content_type": 7,
  "object_id": "3",
  "object_repr": "Порараз бирацца: как мы учились писать авто тест на Python и что - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\", \"\\u0422\\u044d\\u0433 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 12,
 "fields": {
  "action_time": "2021-10-15T06:04:06.542Z",
  "user": 1,
  "content_type": 7,
  "object_id": "15",
  "object_repr": "test22 - (Программирование)",
  "action_flag": 3,
  "change_message": ""
 }
},
{
 "model": "admin.logentry",
 "pk": 13,
 "fields": {
  "action_time": "2021-10-15T06:04:10.089Z",
  "user": 1,
  "content_type": 7,
  "object_id": "14",
  "object_repr": "test32 - (Программирование)",
  "action_flag": 3,
  "change_message": ""
 }
},
{
 "model": "admin.logentry",
 "pk": 14,
 "fields": {
  "action_time": "2021-10-15T06:04:23.152Z",
  "user": 1,
  "content_type": 7,
  "object_id": "4",
  "object_repr": "test2 - (Программирование)",
  "action_flag": 3,
  "change_message": ""
 }
},
{
 "model": "admin.logentry",
 "pk": 15,
 "fields": {
  "action_time": "2021-10-15T06:04:29.008Z",
  "user": 1,
  "content_type": 6,
  "object_id": "2",
  "object_repr": "Программирование",
  "action_flag": 3,
  "change_message": ""
 }
},
{
 "model": "admin.logentry",
 "pk": 16,
 "fields": {
  "action_time": "2021-10-15T06:11:02.133Z",
  "user": 1,
  "content_type": 7,
  "object_id": "2",
  "object_repr": "Как правильно верстать 2 - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 17,
 "fields": {
  "action_time": "2021-10-15T06:12:44.013Z",
  "user": 1,
  "content_type": 7,
  "object_id": "10",
  "object_repr": "Опасность редизайна для позиций вашего сайта - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 18,
 "fields": {
  "action_time": "2021-10-15T06:13:59.219Z",
  "user": 1,
  "content_type": 7,
  "object_id": "12",
  "object_repr": "Защищайтесь! Советы по защите дизайна перед заказчиком - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0418\\u043c\\u044f\", \"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 19,
 "fields": {
  "action_time": "2021-10-15T06:15:00.811Z",
  "user": 1,
  "content_type": 7,
  "object_id": "2",
  "object_repr": "Как правильно верстать 2 - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"Image\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 20,
 "fields": {
  "action_time": "2021-10-15T06:17:52.290Z",
  "user": 1,
  "content_type": 7,
  "object_id": "16",
  "object_repr": "Покорение клетки — становление гистологии - (Научно-популярное)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 21,
 "fields": {
  "action_time": "2021-10-15T06:19:51.106Z",
  "user": 1,
  "content_type": 7,
  "object_id": "17",
  "object_repr": "Как я снял кольца Сатурна на ТЕЛЕФОН - (Научно-популярное)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 22,
 "fields": {
  "action_time": "2021-10-15T06:22:04.426Z",
  "user": 1,
  "content_type": 7,
  "object_id": "18",
  "object_repr": "История ЖК-дисплеев с активной матрицей - (Научно-популярное)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 23,
 "fields": {
  "action_time": "2021-10-15T06:24:05.267Z",
  "user": 1,
  "content_type": 7,
  "object_id": "19",
  "object_repr": "Наш мозг не компьютер - (Научно-популярное)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 24,
 "fields": {
  "action_time": "2021-10-15T06:26:10.279Z",
  "user": 1,
  "content_type": 7,
  "object_id": "20",
  "object_repr": "Легко ли собрать выжигатель мозгов? - (Научно-популярное)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 25,
 "fields": {
  "action_time": "2021-10-15T06:28:25.845Z",
  "user": 1,
  "content_type": 7,
  "object_id": "21",
  "object_repr": "Делаем гостевую Wi-Fi сеть в ВУЗе - (Сетевые технологии)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 26,
 "fields": {
  "action_time": "2021-10-15T06:30:38.892Z",
  "user": 1,
  "content_type": 7,
  "object_id": "22",
  "object_repr": "Из-за чего Facebook стал глобально недоступен. - (Сетевые технологии)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 27,
 "fields": {
  "action_time": "2021-10-15T06:31:38.269Z",
  "user": 1,
  "content_type": 7,
  "object_id": "23",
  "object_repr": "Мониторинг сервера Zimbra OSE с помощью Nagios - (Сетевые технологии)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 28,
 "fields": {
  "action_time": "2021-10-15T06:32:04.314Z",
  "user": 1,
  "content_type": 7,
  "object_id": "1",
  "object_repr": "Где живут программисты – интерактивная карта - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 29,
 "fields": {
  "action_time": "2021-10-15T06:32:10.669Z",
  "user": 1,
  "content_type": 7,
  "object_id": "11",
  "object_repr": "Кто такой системный архитектор - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 30,
 "fields": {
  "action_time": "2021-10-15T06:32:15.751Z",
  "user": 1,
  "content_type": 7,
  "object_id": "13",
  "object_repr": "DevOpsConf: информация к размышлению - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 31,
 "fields": {
  "action_time": "2021-10-15T06:32:20.931Z",
  "user": 1,
  "content_type": 7,
  "object_id": "10",
  "object_repr": "Опасность редизайна для позиций вашего сайта - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 32,
 "fields": {
  "action_time": "2021-10-15T06:32:26.981Z",
  "user": 1,
  "content_type": 7,
  "object_id": "2",
  "object_repr": "Как правильно верстать 2 - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 33,
 "fields": {
  "action_time": "2021-10-15T06:32:31.162Z",
  "user": 1,
  "content_type": 7,
  "object_id": "2",
  "object_repr": "Как правильно верстать 2 - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 34,
 "fields": {
  "action_time": "2021-10-15T06:32:36.019Z",
  "user": 1,
  "content_type": 7,
  "object_id": "2",
  "object_repr": "Как правильно верстать 2 - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 35,
 "fields": {
  "action_time": "2021-10-15T06:32:39.702Z",
  "user": 1,
  "content_type": 7,
  "object_id": "10",
  "object_repr": "Опасность редизайна для позиций вашего сайта - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 36,
 "fields": {
  "action_time": "2021-10-15T06:32:44.590Z",
  "user": 1,
  "content_type": 7,
  "object_id": "12",
  "object_repr": "Защищайтесь! Советы по защите дизайна перед заказчиком - (Веб-дизайн)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 37,
 "fields": {
  "action_time": "2021-10-15T06:32:49.599Z",
  "user": 1,
  "content_type": 7,
  "object_id": "3",
  "object_repr": "Порараз бирацца: как мы учились писать авто тест на Python и что - (IT)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 38,
 "fields": {
  "action_time": "2021-10-15T06:32:53.190Z",
  "user": 1,
  "content_type": 7,
  "object_id": "3",
  "object_repr": "Порараз бирацца: как мы учились писать авто тест на Python и что - (IT)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 39,
 "fields": {
  "action_time": "2021-10-15T06:33:00.059Z",
  "user": 1,
  "content_type": 7,
  "object_id": "16",
  "object_repr": "Покорение клетки — становление гистологии - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 40,
 "fields": {
  "action_time": "2021-10-15T06:33:03.798Z",
  "user": 1,
  "content_type": 7,
  "object_id": "16",
  "object_repr": "Покорение клетки — становление гистологии - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 41,
 "fields": {
  "action_time": "2021-10-15T06:33:08.280Z",
  "user": 1,
  "content_type": 7,
  "object_id": "16",
  "object_repr": "Покорение клетки — становление гистологии - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 42,
 "fields": {
  "action_time": "2021-10-15T06:33:12.920Z",
  "user": 1,
  "content_type": 7,
  "object_id": "17",
  "object_repr": "Как я снял кольца Сатурна на ТЕЛЕФОН - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 43,
 "fields": {
  "action_time": "2021-10-15T06:33:16.638Z",
  "user": 1,
  "content_type": 7,
  "object_id": "17",
  "object_repr": "Как я снял кольца Сатурна на ТЕЛЕФОН - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 44,
 "fields": {
  "action_time": "2021-10-15T06:33:21.798Z",
  "user": 1,
  "content_type": 7,
  "object_id": "18",
  "object_repr": "История ЖК-дисплеев с активной матрицей - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 45,
 "fields": {
  "action_time": "2021-10-15T06:33:29.161Z",
  "user": 1,
  "content_type": 7,
  "object_id": "19",
  "object_repr": "Наш мозг не компьютер - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 46,
 "fields": {
  "action_time": "2021-10-15T06:33:38.501Z",
  "user": 1,
  "content_type": 7,
  "object_id": "20",
  "object_repr": "Легко ли собрать выжигатель мозгов? - (Научно-популярное)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 47,
 "fields": {
  "action_time": "2021-10-15T06:33:48.330Z",
  "user": 1,
  "content_type": 7,
  "object_id": "21",
  "object_repr": "Делаем гостевую Wi-Fi сеть в ВУЗе - (Сетевые технологии)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 48,
 "fields": {
  "action_time": "2021-10-15T06:34:01.734Z",
  "user": 1,
  "content_type": 7,
  "object_id": "22",
  "object_repr": "Из-за чего Facebook стал глобально недоступен. - (Сетевые технологии)",
  "action_flag": 2,
  "change_message": "[{\"changed\": {\"fields\": [\"\\u0422\\u0435\\u043a\\u0441\\u0442 \\u0441\\u0442\\u0430\\u0442\\u044c\\u0438\"]}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 49,
 "fields": {
  "action_time": "2021-10-15T06:34:09.125Z",
  "user": 1,
  "content_type": 7,
  "object_id": "23",
  "object_repr": "Мониторинг сервера Zimbra OSE с помощью Nagios - (Сетевые технологии)",
  "action_flag": 2,
  "change_message": "[]"
 }
},
{
 "model": "admin.logentry",
 "pk": 50,
 "fields": {
  "action_time": "2021-10-15T06:37:02.411Z",
  "user": 1,
  "content_type": 7,
  "object_id": "24",
  "object_repr": "pfSense сегодня — VPN - (Сетевые технологии)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 51,
 "fields": {
  "action_time": "2021-10-15T06:40:59.983Z",
  "user": 1,
  "content_type": 7,
  "object_id": "25",
  "object_repr": "Опыт извлечения обучающих данных из генеративных языковых моделей - (Big Data)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 52,
 "fields": {
  "action_time": "2021-10-15T06:42:37.089Z",
  "user": 1,
  "content_type": 7,
  "object_id": "26",
  "object_repr": "Apache Airflow и будущее инжиниринга данных: вопрос и ответы - (Big Data)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 53,
 "fields": {
  "action_time": "2021-10-15T06:43:15.257Z",
  "user": 1,
  "content_type": 7,
  "object_id": "27",
  "object_repr": "Компоненты-шаблоны или «скелеты» в Angular: заимствуем идеи у Material table и Material tree - (Big Data)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "admin.logentry",
 "pk": 54,
 "fields": {
  "action_time": "2021-10-15T06:44:23.637Z",
  "user": 1,
  "content_type": 7,
  "object_id": "28",
  "object_repr": "Нам нужны не дата-саентисты, а дата-инженеры - (Big Data)",
  "action_flag": 1,
  "change_message": "[{\"added\": {}}]"
 }
},
{
 "model": "auth.permission",
 "pk": 1,
 "fields": {
  "name": "Can add log entry",
  "content_type": 1,
  "codename": "add_logentry"
 }
},
{
 "model": "auth.permission",
 "pk": 2,
 "fields": {
  "name": "Can change log entry",
  "content_type": 1,
  "codename": "change_logentry"
 }
},
{
 "model": "auth.permission",
 "pk": 3,
 "fields": {
  "name": "Can delete log entry",
  "content_type": 1,
  "codename": "delete_logentry"
 }
},
{
 "model": "auth.permission",
 "pk": 4,
 "fields": {
  "name": "Can view log entry",
  "content_type": 1,
  "codename": "view_logentry"
 }
},
{
 "model": "auth.permission",
 "pk": 5,
 "fields": {
  "name": "Can add permission",
  "content_type": 2,
  "codename": "add_permission"
 }
},
{
 "model": "auth.permission",
 "pk": 6,
 "fields": {
  "name": "Can change permission",
  "content_type": 2,
  "codename": "change_permission"
 }
},
{
 "model": "auth.permission",
 "pk": 7,
 "fields": {
  "name": "Can delete permission",
  "content_type": 2,
  "codename": "delete_permission"
 }
},
{
 "model": "auth.permission",
 "pk": 8,
 "fields": {
  "name": "Can view permission",
  "content_type": 2,
  "codename": "view_permission"
 }
},
{
 "model": "auth.permission",
 "pk": 9,
 "fields": {
  "name": "Can add group",
  "content_type": 3,
  "codename": "add_group"
 }
},
{
 "model": "auth.permission",
 "pk": 10,
 "fields": {
  "name": "Can change group",
  "content_type": 3,
  "codename": "change_group"
 }
},
{
 "model": "auth.permission",
 "pk": 11,
 "fields": {
  "name": "Can delete group",
  "content_type": 3,
  "codename": "delete_group"
 }
},
{
 "model": "auth.permission",
 "pk": 12,
 "fields": {
  "name": "Can view group",
  "content_type": 3,
  "codename": "view_group"
 }
},
{
 "model": "auth.permission",
 "pk": 13,
 "fields": {
  "name": "Can add content type",
  "content_type": 4,
  "codename": "add_contenttype"
 }
},
{
 "model": "auth.permission",
 "pk": 14,
 "fields": {
  "name": "Can change content type",
  "content_type": 4,
  "codename": "change_contenttype"
 }
},
{
 "model": "auth.permission",
 "pk": 15,
 "fields": {
  "name": "Can delete content type",
  "content_type": 4,
  "codename": "delete_contenttype"
 }
},
{
 "model": "auth.permission",
 "pk": 16,
 "fields": {
  "name": "Can view content type",
  "content_type": 4,
  "codename": "view_contenttype"
 }
},
{
 "model": "auth.permission",
 "pk": 17,
 "fields": {
  "name": "Can add session",
  "content_type": 5,
  "codename": "add_session"
 }
},
{
 "model": "auth.permission",
 "pk": 18,
 "fields": {
  "name": "Can change session",
  "content_type": 5,
  "codename": "change_session"
 }
},
{
 "model": "auth.permission",
 "pk": 19,
 "fields": {
  "name": "Can delete session",
  "content_type": 5,
  "codename": "delete_session"
 }
},
{
 "model": "auth.permission",
 "pk": 20,
 "fields": {
  "name": "Can view session",
  "content_type": 5,
  "codename": "view_session"
 }
},
{
 "model": "auth.permission",
 "pk": 21,
 "fields": {
  "name": "Can add хаб",
  "content_type": 6,
  "codename": "add_hub"
 }
},
{
 "model": "auth.permission",
 "pk": 22,
 "fields": {
  "name": "Can change хаб",
  "content_type": 6,
  "codename": "change_hub"
 }
},
{
 "model": "auth.permission",
 "pk": 23,
 "fields": {
  "name": "Can delete хаб",
  "content_type": 6,
  "codename": "delete_hub"
 }
},
{
 "model": "auth.permission",
 "pk": 24,
 "fields": {
  "name": "Can view хаб",
  "content_type": 6,
  "codename": "view_hub"
 }
},
{
 "model": "auth.permission",
 "pk": 25,
 "fields": {
  "name": "Can add статья",
  "content_type": 7,
  "codename": "add_article"
 }
},
{
 "model": "auth.permission",
 "pk": 26,
 "fields": {
  "name": "Can change статья",
  "content_type": 7,
  "codename": "change_article"
 }
},
{
 "model": "auth.permission",
 "pk": 27,
 "fields": {
  "name": "Can delete статья",
  "content_type": 7,
  "codename": "delete_article"
 }
},
{
 "model": "auth.permission",
 "pk": 28,
 "fields": {
  "name": "Can view статья",
  "content_type": 7,
  "codename": "view_article"
 }
},
{
 "model": "auth.permission",
 "pk": 29,
 "fields": {
  "name": "Can add user",
  "content_type": 8,
  "codename": "add_intergalacticuser"
 }
},
{
 "model": "auth.permission",
 "pk": 30,
 "fields": {
  "name": "Can change user",
  "content_type": 8,
  "codename": "change_intergalacticuser"
 }
},
{
 "model": "auth.permission",
 "pk": 31,
 "fields": {
  "name": "Can delete user",
  "content_type": 8,
  "codename": "delete_intergalacticuser"
 }
},
{
 "model": "auth.permission",
 "pk": 32,
 "fields": {
  "name": "Can view user",
  "content_type": 8,
  "codename": "view_intergalacticuser"
 }
},
{
 "model": "sessions.session",
 "pk": "omykrqjb2suprabbwcq92tuadp4iddcg",
 "fields": {
  "session_data": ".eJxVjDsOwyAQBe9CHaEFFtZOmd5nQHyDkwgkY1dR7h5bcpG0M_Pem1m3rcVuPS12juzKBLv8Mu_CM9VDxIer98ZDq-sye34k_LSdTy2m1-1s_w6K62VfqyBHQJWCAHJyEDR4rfQOyeSIESFIysIYpTTRGMFh0JAgS_SAQhP7fAGzgzZ2:1mbFw0:knjzLTUwFzi4y58nSW93MwZs1XbAwb4EXjpgCxVdZJs",
  "expire_date": "2021-10-29T05:39:12.799Z"
 }
},
{
 "model": "mainapp.hub",
 "pk": 1,
 "fields": {
  "name": "IT",
  "order": 0,
  "is_active": true
 }
},
{
 "model": "mainapp.hub",
 "pk": 3,
 "fields": {
  "name": "Веб-дизайн",
  "order": 0,
  "is_active": true
 }
},
{
 "model": "mainapp.hub",
 "pk": 4,
 "fields": {
  "name": "Научно-популярное",
  "order": 0,
  "is_active": true
 }
},
{
 "model": "mainapp.hub",
 "pk": 5,
 "fields": {
  "name": "Сетевые технологии",
  "order": 0,
  "is_active": true
 }
},
{
 "model": "mainapp.hub",
 "pk": 6,
 "fields": {
  "name": "Big Data",
  "order": 0,
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 1,
 "fields": {
  "name": "Где живут программисты – интерактивная карта",
  "image": "",
  "text": "У вас есть полное представление о том, в каких странах и   мира работают программисты? Кремнивая долина, Сан Франциско – ок. А в Новой Зеландии? А в африке? Хотите узнать больше? Зачем это может быть полезным? Например, вы рассматриваете варианты, куда переехать. При наличии детализации по городам можно увидеть всё разнообразие доступных вариантов. \r\nУзнать что IT есть в самых неожиданных местах, где казалось бы, его и быть не может. Когда я впервые задумался о том, что хочется иметь детализацию на уровне городов, а не стран, стал вопрос, есть ли что-то готовое и где взять данные. Удалось найти много исследований про местные рынки, чаще данные обработаны вручную, например   и   пишут про США, подобных исследований достаточно много. Наиболее близкое исследование с детализацией по городам можно посмотреть  , советую почитать – человек проделал героический сбор профайлов пользователей GitHub + постобработку, но данные уже немного устарели, также визуализация далеко не самая удобная. В качестве исходных данных для начала решил попробовать учитывать количество разработчиков и представителей смежных профессий, поделившихся своим уровнем дохода со всем миром. Исходные данные берутся из нескольких общеизвестных источников типа Glassdoor (но, к сожалению \\\"scraping is always a grey area\\\", поэтому светить настоящие источники опасаюсь) с нормированием по размеру баз. Давайте считать что это пробная альфа-версия или v1. Если у вас есть интерес к теме или идеи где взять наиболее актуальные и полезные данные – давайте обсудим, обязательно сделаю улучшенную версию. А если вы готовы помочь со сбором данных – вообще супер, обязательно приходите в личку! Отбор должностей делал по следующим ключевым словам (предварительно смотрел на самые частотные слова в должностях): 'software engineer', 'developer', 'programmer', 'data scientist', 'data analyst', 'big data', 'data engineer', 'devops', 'machine learning', 'python', 'java', '.net', 'c++', 'c#', 'database', 'cloud engineer', 'backend', 'ios', 'android', 'full stack', 'full-stack', 'sql', 'oracle'. Вот что в результате получилось. Для визуализации использовал Google Data Studio, полная интерактивная карта  . Сам завис в карте, открыл для себя несколько интересных мест, где оказывается есть IT. Карта не отражает состояние IT сферы и рынка труда с идеальной степенью точности, но результат в целом коррелирует с тем, что я вижу в статьях про локальные рынки. Это скорее некоторый ориентир, каждый может интерпретировать результат по своему. Лично для меня ассоциация следующая – полученная карта – показатель того, насколько IT в том или ином месте не просто развито, но еще и \\\"интегрировано\\\" в мировое IT. Жду ваших замечаний и предложений. Что еще хочется добавить на карту/фильтры и т.д.? . Вот   я уже попробовал использовать данные   про пользователей GitHub за 2018 год. P.S. также посетите мой канал   :)",
  "tag": "где работают программисты",
  "hub": 1,
  "author": 1,
  "add_datatime": "2021-10-10T19:36:26Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 2,
 "fields": {
  "name": "Как правильно верстать 2",
  "image": "",
  "text": "Это работа является логическим продолжением моего первого подробного текста для сообщества об актуальных подходах к верстке  . Но, если в первом трактате, внимание уделялось, прежде всего, стилю кода, его качеству и эффектным современным возможностям различных препроцессоров и фреймворков, что демонстрировалось на некоторых конкретных специфических задачах, теперь хочется сфокусироваться на архитектурных или даже организационных аспектах веб-производства. Если вы не читали мой первый текст, но собираетесь при этом прочесть этот — не поленитесь перейти по ссылке и пробежать глазами самые последние разделы каждой из двух частей первого пособия: «Готовые решения» и «Песочницы». Этот текст начинает прямо с этих мест и развивает именно эти идеи: и о пагубности применения раскрученных-популярных «на все готовых» UI-«дизайн-систем»-фреймворков для создания кастомизированных веб-морд любой сложности и, о, по сути, полезности использования хотя бы минимального документирования и явных соглашений при разработке веб-GUI на фронтенде. Но я не стану тратить время, доказывая, что «ни в коем случае нельзя использовать Vuetify или AntDesign» для создания крупных UI-систем с полностью кастомным оформлением. Вам не нужно прикручивать себе   непроницаемый слой плохо кастомизируемого готового GUI для того чтобы написать кнопку или поле ввода! Если вам нужен датапикер — найдите и допилите что-нибудь под себя. Это понимание может только прийти или так и не придти с годами тяжелого опыта, когда вы будете постоянно тратить непростительно много своего времени на то, чтобы написать очевидно отвратительный CSS — «кряки с !important`ами поверх стилей библиотеки», выдумывать чудные костыли на javascript чтобы изменить дефолтное поведение виджетов на кастомное и хитрое-нестандартное затребованное дизайнерами... И, при этом, ваши шаблоны, стили и js-обвязки будут превращаться во все менее читаемые, запутанные нагромождения разнообразно оформленного кода, с различным подходом к наименованию и прочими бедами… Этот текст и написанный для него проект призваны наглядно показать «а как надо?». Верстка по-прежнему остается достаточно свободной дисциплиной, в которой сосуществуют множество совершенно разных методологий, подходов и связанных с ними технических решений. Часто программисты принимают какой-то один определенный способ, и, в результате, стагнируют в общем понимании и развитии. Привычки формируют удобную зону комфорта и это мешает осознать риски в ситуациях для которых они неадекватны. Знаете, какой аргумент мне уже несколько раз приходилось слышать в технической дискуссии от оппонентов защищавших привычную для них систему, технику разметки, но совершенно непригодную, на мой взгляд, для решения текущих насущных задач, в реальной ситуации данного конкретного проекта и сроков? «Ну это же   технология?», «Гугл, Фейсбук, … это используют, чем мы хуже»... Если вы слышите от кого-то, что нечто, не относящееся к действительно используемому всеми нами известному перечню базовых спецификаций — «общепринято», это повод сразу сделать вывод о том, что ваш собеседник имеет мало разнообразного опыта в этом и просто защищает свою лень и нежелание учиться новому. Рядом с аргументом про странные пристрастия акул капитализма обычно следует еще и что-нибудь совершенно несостоятельное про «наши программисты не все знают Stylus», при том что, любой препроцессор CSS даже и тем более начинающий программист способен изучить до базового уровня за один полезный приятный вечер. Мне всегда нравилось менять технологии, синтаксисы — это освежает и придает драйву, интереса в работе. И именно совершенно разнообразный опыт реальной коммерческой практики с различными технологиями сформировал мои представления о том «что хорошо, что плохо» в тех или иных ситуациях. Возможно, я когда-нибудь накоплю достаточно злости и найду время для того чтобы на наглядных вопиющих примерах показать «а почему вот это плохо?». Но эта статья и пример модуля о том «как надо?», что точно будет аккуратно и эффективно. Если вы являетесь упертым сторонником CSS-in-JS-подходов, или, вообще, по-прежнему наносите оформление на разметку в «бородатом» «утилитарном» стиле «много классов с говорящими названиями содержащих одно-два правило» (тут уместно вспомнить уже «продвинутую» — CSS-in-JS — реализацию   с ее оголтелым слоганом-оксюмороном: «“Best practices” don’t actually work».) — предложенная и в моей первой книжке и в ее практическом продолжении — здесь — методология, использующая только любой классический препроцессор как «абстрактный медиатор стилей» для доставки дизайн-констант и прочей стилевой абстракции, и сам компонентный фреймворк для, простите за сплошную тавтологию — аккуратной декларативной компонентности — вам все это не зайдет, наверняка. Например, попытки посадить Styled Components на дизайн-константы приводят к безобразному коду в таких «стилях»... Или утилиты «ишак вижу — ищак пою» «в стиле Taiwind» вполне способны представлять «атомы», но все равно ограничивают гибкость в случае непрогнозируемых и частых изменений, ну или просто — также выглядят излишне-невменяемо уже на самой разметке. Я встречал проекты где прикручено «вообще сразу все» — препроцессор, CSS-модули и Styled Components, Flow к зачем-то “отключенному” CRA, например — и это уже даже нельзя назвать «оверинжинирингом», так как это просто отвратительная глупость и очевидно плохая архитектура. Возможно, раскаяние и отрезвление наступит когда вы опять превысите сроки и бюджет на очередном проекте в пять, да, Карл, пять раз — и такое бывает сплошь и рядом. И/или — дизайнеры, по просьбе клиента которому никак нельзя отказать в очередной раз «все переделают» в макетах, которые «уже сверстаны»… Мотивация Давайте, прежде всего, четко обрисуем проблему которую мы собираемся решать. В реальном мире, в очень многих командах — проектирование практически никак не «дружит» с разработкой, общаясь, по сути — только через конечные «макеты». Даже действительно опытные «фуллы», которых бизнес предпочитает нанимать «и в качестве фронтендеров», на самом то деле, верстают совсем слабо. Кодеры сдают только им самим известно   как собранные страницы на тестирование, которое, в свою очередь — тупо сравнивает их с макетами — замыкая порочный круг. Обычно и, тем более, в условиях жестких дедлайнов и/или в разношерстных, наспех собранных командах из специалистов с совершенно разными скилами и опытом — бывает очень сложно ввести соглашения. Общий фокус процесса неминуемо смещается на то чтобы «сделать хоть как-нибудь», «протолкнув» быстрый гавнокод через тестировщиков… Такая организация работы как раз очень выгодна мракоделам. Мракодел отгораживается «только своей задачей» — «моя хата скраю» — и пишет ее так, как ему удобно, игнорируя best practices и напряжные соглашения, часто вообще — просто изображая работу удаленно — скопировал, чутка перековырял под макеты — ушел гулять, вечером отправил — ждет когда тестировщики обязательно и несколько раз вернут на доработку… «Есть баги — есть работа!». «Переиспользование» превращается в простое копирование и «интуитивную» модификацию «уже готовых кусков» — лавинообразно усугубляя мрачность и запутанность стилей, разметки и js-обвязок для них. И если javascript бывает еще более-менее адекватен, все его «умеют» и понимают — то стили часто превращаются в просто непроходимые дебри и бесконечную излишнюю неоптимизированную колбасу, несмотря на то какие именно технологии используются… При этом руководство, менеджеры которые не ходят читать репо могут пребывать в наивной уверенности что « » или хотя бы — « »... Мне приходилось получать репо с тонной «индусского» — нереально небрежного и явно копированного целыми крупными кусками и даже файлами кода, игнорирующего вполне имеющийся у фирмы единый UI-кит, его константы и компонентность, с посылом от руководства что «все готово», «надо слегка переписать»… Мне приходилось годами исправлять за «тимлидами-сеньорами» проекты с ужасной архитектурой блокирующей быструю доставку именно того, что нужно заказчику, например — сложного современного дизайна… Причем, когда проект начинали — самые важные требования остались проигнорированы, зато с совершенно ненужными в этом случае «модными трендами» покуражились вовсю... Я заметил что с молодыми специалистами часто работать и добиваться качественного результата бывает намного проще чем с закостенелыми самоуверенными «фуллстеками», рефлексующими свою недооцененность и прочее… Ребята помоложе еще не потеряли азарт и интерес к тому что делают, не успели несколько раз перегореть, устать, и большинство как раз стремиться стать настоящими профессионалами, открыты новым идеям, подходам, ищут и перенимают хороший стиль, адекватно реагируют на критику и так далее.. Хотя бывают исключения и с теми, и с другими, конечно… Но хватить абстрактной лиричной боли — перейдем уже к «хорошим практикам по Гамбаряну»… Разрабатывайте визуальный язык и основанные на нем шаблоны своих интерфейсов через простые четкие соглашения и документирование, а не хаотическое копирование!!! По опыту, особенно, если речь идет о «бренной вьюхе» на фронтенде, очень многие программисты используют годную на все времена стремительно интуитивную методологию «программирование через копирование файлов или больших кусков кода» с девизом «да хз что за функция» — и такой уже распиленный системный хардкор-код практически невозможно полностью отрефакторить, точнее — по опыту — только в том случае, если это позволяют удачно выбранные изначально технологии, плюс, конечно же, организационные ресурсы... Некоторые системы в реальных коммерческих ситуациях можно только «подмести» и «заморозить», продолжая, по сути, доставлять мрак при необходимости какой-то видимости прогресса для клиента. Необходимость непрерывно править плохой код коллег, или работать с неадекватными задачам и целям проектов архитектурами после предшественников резко снижает качество жизни программистов. Программисты такие же ленивые, хоть и мыслящие, животные как и все другие люди и коты. Да, мне кажется, что мой полосатый много думает и иногда даже пытается поделиться... Вот даже котик пытается рассказать о результатах своего умственного труда? А многие из вас не то что никогда не оставляют комментариев — в принципе — пишут такой код что    , к которому очевидно любые комментарии излишни, так скажем. Причем, причины такого состояния кода множества проектов всегда одни и те же: «времени совсем не было», «надо было что-то показать», «руководство попросило сделать очень быстро» и тому подобное, «будет время — перепишем»… Поверьте, в реальной коммерции — оплаченного отдельного времени на оптимизацию и рефакторинг практически никогда не бывает. Клиенты в аутсорсе, например, почти никогда не покупают его как дополнительную опцию. Даже если некий продукт очевидно неаккуратно написан, работает со сбоями и совершенно не годен для масштабирования — бизнес практически всегда будет заинтересован, прежде всего, в доставке нового функционала любой ценой, а не в исправлении «уже проданного» — и все что он действительно захочет, чаще всего, это опять — быстро — «как-нибудь подлатать все баги»… Главная задача команды разработчиков в этом смысле — прямо на старте выработать четкую систему приемлемых и удобных для всех участников соглашений, которая с самого начала позволит писать максимально консистентный, понятный, и, самое главное, в идеале — и гибкий, и, одновременно, железно надежный код. Среднестатистический разработчик-мракодел, тянущий лямку в капитализме никогда не возвращается к своему поспешному или даже откровенно, простите мой спесишизм, «индусскому» копипасте-коду для того чтобы улучшить его, кроме ситуаций, когда служба контроля качества вернула некий конкретный баг на доработку. И, конечно, тут совсем нельзя говорить о каком-либо структурном улучшении, так как подобные правки всегда носят не системный, а полностью локальный-частный характер «быстрых фиксов», «кряков» и только усугубляют беспорядок. Не дай бог трогать код по «уже закрытым таскам», «так ведь можно что-нибудь поломать!»... Среднестатистический разработчик-мракодел нацелен, прежде всего, на закрытие своих тасков-багов по трекеру и не интересуются общим состоянием проекта, приходящим чужим кодом. Полезные соглашения его напрягают, он игнорирует или даже активно противиться им. Кроме того, я заметил, что профессиональные мракоделы в командах и на проектах стремятся объединяться в устойчивые группы… Мракоделы копируют мракокод из файла в файл, из проекта в проект, покрывая друг-друга, ломая волю ПМов и регулярно рапортуя через них начальству которое не читает репо что «все почти готово, только кнопку нажать»... Мракоделы даже иногда дорастают до сеньоров и лидов. Тогда они начинают занимаются сомнительным оверинжинирингом, накручивая ненужные, но модные технологии, пока не оказывается что все сроки сорваны и клиент не получил того чего хотел, а код такой что заплатки негде ставить без поллитры не прочитать… Как простому верстальщику противостоять вселенскому мраку? Далее будет изложена простая методика реализации гибкого и масштабируемого GUI веб-интерфейсов, доставки дизайна в код, аккуратное использование которой сводит на минимум возможности добавления низкопробного кода в проекты фронтенда, а также способно помочь более четко, сперва — оценить, и, в дальнейшем, организовать эффективную совместную работу. Вкратце, обобщая можно тезисно описать это как то, что вам нужно добиться от разработчиков участвующих в развитии проекта фронтенда соблюдения следующих основных моментов: Часто встречающийся в современной корпоративной культуре случай когда, предположим, два разных отдела ведут разработку разных интерфейсов основанных на одной визуальной дизайн-системе — очевидно основной когда необходимо построить работу через общую библиотеку-модуль. Антибиблиотека  это точно не еще одна библиотека «готового на все UI». Это, прежде всего, концепт и пример системного подхода, которые могут позволить вам легко поддерживать свои собственные библиотеки только из необходимых компонент, чистого оптимального кода, сопровождаемого необходимой наглядной документацией и с оформлением точно согласующимся с руководством по стилю. Проект содержит некоторый минимальный набор готовых компонент именно в качестве примера. Это то, что встречается практически в любом большом интерфейсе: простые, базовые примеры — обертка для контента, сетки, и пара важных «дорогих» контролов: датапикер с диапазоном на одном листе и кварталами, обертка над сторонним селектом... Мысли о том что написание хардкор-кода или, точнее, написание хардкор-кода еще и с помощью чужого, обычно практически полностью изолированного от остальной системы готового кода как-то   — нужно забыть. Возможно, какие-то тактические задачи, может быть, но точно — все равно ненадежно, и всегда —стратегически создает предпосылки для очень серьезных проблем в будущем, возможно, даже самом скором. Кроме того, исключает возможность получать удовольствие от работы для тех кто трудиться с вами параллельно или будет вынужден иметь дело с этим легаси в дальнейшем. В этом смысле сама идея UI Library Starter противоположна концепции любой обычной популярной библиотеки где «все включено». Она не в том чтобы дать вам возможность лениться, бояться и тратить рабочее время неэффективно мучаясь трудными кастомизациями через   с неизвестным результатом, другими адовыми кряками, превращая свои проекты в неведомы зверушки из скрытых под капотом чужих кривых поделок, нечитаемые лоскутные одеяла повторяющегося кода или запутанные описания одного и того же совсем по-разному, с позорно огромным опасным пулом зависимостей. Идея этого проекта в том, чтобы аккуратно помочь вам начать писать действительно оптимальный, понятный и задокументированный, гибкий и легко масштабируемый код, который вы сможете постоянно переиспользовать и улучшать.  дочернего проекта использующего модуль-библиотеку. Далее — простите мне странные вкрапления моего никакого английского в документации…))) Getting Started Installation Скачайте код   и оформите его в отдельный репозиторий. При выборе имени для нового репозитория необходимо сразу убедиться в том, что оно не занято на  . Пусть это будет ui-library-starter-test. Или, в случае, если вы не планируете менять стиль проекта под свои собственные гайды, но, собираетесь поиграться или даже внести вклад в его развитие, например, предложив еще какие-то важные компоненты — сделайте форк, конечно же. Дальнейшие инструкции относятся к первому случаю — пилим свежую либу с кастомным стилем под конкретные задачи — в этом случае многие могут захотеть удалить эту документацию и почти все компоненты, чтобы не выполнять лишнюю кастомизацию. Customization README.md Поправьте первую строчку в : package.json Далее в   вам необходимо крайне аккуратно переписать актуальной информацией следующие поля, ничего не пропустив: Обратите внимание на имя проекта в конце длинной команды деплоя ! Documentation config Перейдите к документации на VuePress и сконфигурируйте ее под себя : Connecting fonts Перепишите имя шрифта и переменные начертаний если требуется в файле : Поместите папку с правильным шрифтом рядом с папкой в . Пропишите правильные импорты и пути в файле кастомизации документации на VuePress : Удалите директорию со старым шрифтом . Сleaning project Если вы хотите получить полностью чистую документацию — произведите следующую очистку папок и файлов. Удалите все папки и файлы в кроме директорий , и — если желаете оставить Песочницу. И файла , который нужно оставить, но очистить: Удалите директорию . Вычистите импорты тестовых компонент в индексном файле : Вы можете выбрать какие компоненты оставить или даже удалить их все, если уверенны в себе и не нуждаетесь в наглядных примерах под рукой. Вернитесь к конфигурации документации и отразите изменения в . Style setting Запустите разработку документации командой: Прочитайте раздел Constants этой документации. Вам необходимо настроить препроцессор вашей библиотеки в точном соответствии с вашим руководством по фирменному стилю. Adding a component После того как стили библиотеки настроены вы можете добавлять свои специфические компоненты. Выберете имя для компонента в PascalCase стиле написания, предположим это . Некоторые имена могут оказаться зарезервированы VuePress. , например. Самая достойная замена видится как . Добавьте директорию . Добавьте в нее индексный файл c импортом-экспортом  И сам компонент  Добавьте экспорт в индексный файл библиотеки : Добавьте документацию компонента в файл : И далее — рендер-тест и исходный код по аналогии с другими файлами. Добавьте компонент в конфигурацию VuePress : Using third party modules Используйте только относительные пути для импорта чего-либо в javascript ваших компонентов. Не используйте «абсолютные» алиасы: В реальных проектах вам потребуется очень часто закрывать «самые дорогие требования» с помощью аккуратно подобранных подходящих готовых решений. В таких случаях логично будет создавать обертку над чужим модулем, предоставляющую всю необходимую кастомизацию. Пример этого: . Установите и импортируйте модуль как обычно в главном файле : Так как мы используем глобальные стили собственной кастомизации модуля — невозможно будет защитить стили перекастомизации в SFC-обертке с помощью : Use the sandbox Используйте специальный компонент и роут документации Sandbox как экспериментальную площадку и холст для создания новых компонент на простых мокках или тестирования взаимодействия между ними. Хотя, очевидно, некоторые компоненты, такие как, например, лейаут — удобнее создавать непосредственно в проекте и уже после этого переносить в библиотеку. Library publishing Зарегистрируйтесь на и подтвердите регистрацию (дождитесь письма на почту). Connecting to projects Вы можете либо использовать стартовый шаблон для новых проектов , тогда вам придется заменить библиотеку: Либо установить библиотеку как любой другой модуль в любой другой проект: Организация стилей дочерних проектов может или иметь подобную библиотеке структуру или любую другую (например, если вы внедряете бибилиотеку в старый проект). Единственное требование: первый импорт в основном файле — основного файла библиотеки. Второй — подключение шрифтов и стилизация и . проекта использующего библиотеку: проекта использующего библиотеку: Практически единственный повод что-то поменять в этом файле — крайне маловероятная ситуация — замена или добавление шрифта в гайдлайн. Предполагается что отредактировать пути шрифтов придется только один раз — при подключении библиотеки под определенный стиль. Подключите все это к главному шаблону : Исправьте имя библиотеки в импортах в точку входа если вы брали готовый репо или подключите: Исправьте имя или добавьте команду в : Updating in projects Обновляйте библиотеку до последней версии в проектах: Constants _stylebase.styl Глобальный медиатор стилей собирает не компилируемые , компилируемые сущности и необходимые глобально кастомизации используемых сторонних модулей (но, те которые позволяют это сделать без — стоит разместить в SFC-обертках). библиотеки: Теперь можно использовать всю эту кухню на компонентах: Это можно назвать «глобальными стилями-невидимками», что-то такое — они, с одной стороны — участвуют в правильном оформлении везде, но, при этом, «их не видно». Мы предоставляем глобальные константы гайдлайна и всю прочую мощь препроцессора всем компонентным системам — библиотеке и всем ее «читателям». Mixins and placeholders UI Library Starter дает надежду на то, что в вашей разметке код будет полностью оптимальный и консистентный. Это способно сделать даже крупную систему из нескольких проектов базирующихся на одном визуальном языке — и полностью управляемой, и, в тоже время — гибкой. Не копируйте код кусками по компонентам — оптимизируйте очевидно одинаковые наборы в миксинах и плейсхолдерах! Или вы можете делать примеси без параметров для того, чтобы — забегая вперед (см. раздел Breakpoints) — такой же набор стал доступен внутри медиа-миксинов  : Теперь: Colors Абстрагируйте все цвета из гайдлайна в короткие имена-маркеры. : В любом месте кода препроцессора или секции стилей SFC (при условии импорта стилевой базы) библиотеки или дочерних проектов вы можете передавать правильные цвета: Легко поддерживать тестовый компонент наглядно демонстрирующий палитру в  . Breakpoints Переменные-брекпоинты лучше называть более интуитивно-понятно. : Основные точки перехода: в стилевой базе препроцессора в и в константах скриптов библиотеки в — должны соответствовать друг-другу. : В препроцессоре — мощнейшее, очень удобное средство — построенные на брекпоинтах примеси принимающие контент: Использование в любом блоке стилей SFC: В строгой традиции запрещается использование любых глобальных классов со стилями, за исключением анимаций для Vue и вынужденных кастомизаций действительно необходимых сторонних модулей где «классический ад с »))). Мы стараемся минимизировать количество зависимостей и «точечно» закрываем самые «дорогие», неподъемные по ресурсам проблемные места. Точки перехода скриптов обрабатываются специальным модулем-помощником для экрана через matchMedia: Для того чтобы компоненты могли всегда верно определять типоразмер устройства предоставлена общая функциональность обновляющая переменные в событии ресайза. Этот миксин может быть невероятно полезен и на этапе конечной сборки адаптивных видов — в дочерних проектах. : На любых компонентах или видах в библиотеке: В проектах: Тестовый компонент в  . Typography Абстрагируйте все гарнитуры из гайдлайна в короткие имена-маркеры. : Others : Можно получить из этого миксины для более лаконичного синтаксиса : Теперь можно: Переменные все равно могут пригодиться если дизайнеры захотят сделать новый цвет с разрешенной прозрачностью: Анимации Единственные глобально компилируемые стилевые классы которые в строгой традиции разрешено использовать — для анимаций Vue. Вы можете добавлять их после соответсвующих в специальном файле стилевой базы : Теперь в разметке: Structure Мы сегодня многое поняли Чистый код начинается с четких концепций и годной архитектуры, эффективная работа команды — с аккуратной коммуникации и внятных соглашений. Я потратил всего один вечер для того чтобы написать сам пример модуля-библиотеки на Vue со Stylus (без компонент). Точно тоже самое можно быстро легко осуществить для связки любого современного компонентного фреймворка и препроцессора. Даже если вы не хотите, или по каким-то внешним причинам — не можете — выделить атомарный и подробно-компонентный UI своего интерфейса в модуль-библиотеку, изложенные выше подходы все равно способны помочь вам и вашей команде писать действительно красивые и удобные проекты фронтенда с по-настоящему качественной версткой. Удачи в написании собственных библиотек!",
  "tag": "HTML",
  "hub": 3,
  "author": 1,
  "add_datatime": "2021-10-10T19:37:51.106Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 3,
 "fields": {
  "name": "Порараз бирацца: как мы учились писать авто тест на Python и что",
  "image": "",
  "text": "Привет, Хабр! Меня зовут Артем Иванюта, в «Магните» я занимаюсь тестированием информационных систем закупок. В статье я расскажу, как наша команда запускала автотесты web-интерфейсов силами одного сотрудника, как мы вписали их в CI/CD-процесс и с чем столкнулись, решая задачу. Кстати, вы наверняка уже догадались, но все-таки скажу — да, я и есть тот самый «один сотрудник». Так что никакого кликбейта. Одиннадцать друзей Иванюты В нашей команде 11 человек, мы отвечаем за тестирование 15 информационных систем. Всего в «Магните» их больше 600. Мы занимаемся тестированием web-инструментов цепочки поставок розничной сети. Это, например: В масштабах «Магнита» это 25 000 пользователей — наших сотрудников, 4 500 — контрагентов и 16 000 торговых точек. Мы производим релизы ежедневно, а сам цикл в среднем составляет от 2 до 4 недель. По сути от нас зависит своевременная поставка товаров тысяч поставщиков на полки 16 тысяч магазинов (и много чего еще). Восстание машин: срываем релизы Мой путь в компании начался в 2014 году с отдела технического сопровождения торговых точек. Я занимался удаленной поддержкой и настройкой оборудования в гипермаркетах «Магнит». Это были кассы, системы эквайринга, серверы. В 2018 году отдел тестирования запустил внутреннюю школу тестировщиков. За месяц я освоил базу и навыки, а затем перешел в команду тестирования web-интерфейсов. Пришел я как раз вовремя: поток входящих задач начал стремительно расти. Компания пошла по пути DevOps. Запускались новые системы, серьезно обновлялись основные. К началу 2019 года объем тестирования увеличился вдвое. Так количество тикетов на каждого возросло в среднем с 30 до 70 в месяц, а горизонт планирования релизов сдвинулся на 2 месяца вперед. При этом количество людей в команде оставалось прежним — все те же 11 друзей Иванюты:) Когда графики релизов начали срываться на регулярной основе, а рутинные операции  занимать больше 50% времени, мы поняли, что дальше так не потянем. Тогда и было принято решение часть сценариев покрыть автотестами. Путь в питонисты До 2019 года никто из нашей команды не занимался автотестами, 100% тестов обрабатывались вручную. Никто из нашей команды не умел кодить. И конечно мы не могли снижать темп основных задач. Поэтому всей командой уйти в обучение автоматизации тестирования тоже было нельзя. Решили, что в разведку пойдет кто-то один. Мне было интересно попробовать: я изучил опыт сообщества и остановился на python. Python считается универсальным языком, поскольку подходит под множество задач. К примеру, на Python написан Instagram, его используют в аналитике данных, запуске космических кораблей и... в автоматизации тестирования. Чтобы скорость работы команды не снижалась, приняли совместное решение — я иду обучаться, команда забирает 80% моих задач. Какие цели мы поставили для внедрения автотестов: Вот так архитектурно он выстроен у нас: Так мы договорились о целях и тактике. Следующая задача — понять, что именно покрывать автоматизацией. Автоматизируем регресс Мы обновляем информационные системы «Магнита» под условия бизнеса практически ежедневно: Во-первых, мы следуем конкретному плану изменений и обновлений систем. Такой план формируем внутри ИТ ежемесячно для каждой системы. Во-вторых, пользователи от бизнеса направляют запросы на новый функционал. В среднем на сотрудника приходится по 2–3 системы в день. Получается, попытка автоматизировать все кейсы сразу была бы обречена на провал: сил могло хватить только на актуализацию тестовых сценариев. После анализа всей базы тестовых случаев для автоматизации выбрали регрессионные сценарии. Для нас регресс — один из самых трудоемких этапов тестирования, так как он направлен на проверку и выявление ошибок в статичном функционале. Такой функционал или не меняется, или меняется редко. Значит, каждый день мы выполняем именно типовые проверки. В каждом регрессионном сценарии более 100 кейсов. Зачастую кейсы связаны между собой. Поэтому первое, что я сделал, — провел анализ тестовых случаев на предмет критичности и взаимосвязи функциональности. В TMS системе TestRail создал 3 группы приоритета регрессионных сценариев для последующей автоматизации: Теперь расскажу про пошаговую настройку процесса и инструментов. Запускаем змея на Pytest Я использовал Python и его фреймворк Pytest. Оба инструмента широко используются в мире тестирования, по ним накоплена база знаний, есть много плагинов. Поэтому вместе с Selenium WebDriver такой набор удовлетворяет всем потребностям в тестировании web-интерфейсов. Для работы с базой данных я использовал библиотеку SQLAlchemy. Это одна из самых популярных библиотек для работы с СУБД для Python. При создании структуры проекта брал паттерн Page Object. Этот шаблон проектирования позволяет разделить код тестов и описание страниц. Так тесты приобретают читаемый вид, а методы работы со страницей мы можем переиспользовать. Результат: упрощаем поддержку кода и уменьшаем его количество. Структура проекта выглядит так: Папки: Файлы: Помещаем в base_page методы или локаторы, которые используются в кейсах для разных страниц: Как в итоге выглядит тест? Лаконично :) В pages мы расписываем сами методы. Клик на кнопку: Локаторы отдельно: Для поиска элементов на странице я использовал язык запросов XPath. Часть элементов страницы имеют уникальные name или id, которые указали разработчики фронта, поэтому к ним легко обращаться: А другая часть не имеет таких уникальных идентификаторов, поэтому здесь помогают возможности языка XPath: Запуск и закрытие браузера вынесены в conftest.py. Добавляем функцию, например так: И фикстуру: В самом тесте мы не расписываем авторизацию и переход на нужную вкладку, это все выносится в setup с помощью фикстуры: В параметрах фикстуры уже можно указать, когда его использовать. В данном случае наш setup запускается перед каждой функцией (тестом) с помощью параметра  . Для каждого проекта есть свой job в Gitlab CI, который запускается при необходимости. Последняя версия проекта тянется с репозитория. Прогон самих тестов происходит на Selenoid. С помощью графической оболочки можно увидеть информацию о запущенных браузерах, их версиях, разрешении. Развернув интерфейс тестирования можно увидеть, что происходит в режиме реального времени: Selenoid создаёт для каждого теста чистый браузер, в котором выполняется автотест. После его завершения происходит удаление данных с помощью Python. Тесты не зависят друг от друга и могут запускаться в несколько потоков. Тестировать приложение можно одновременно с нескольких браузеров. Для запуска тестов в потоках используется pytest-xdist плагин. В команду запуска добавляется параметр -n <количество потоков>. Фреймворк Pytest позволяет выбрать часть кейсов или пропускать те, которые сейчас не нужны. Например, для кейсов по одной вкладке приложения ставим маркировку @pytest.mark и запускаем тестирование нужной вкладки приложения. Фикстуры позволяют запустить тестирование в нужной версии браузера. Основным инструментом получения отчетности служит TestRail. Каждый автотест связан с кейсом в TestRail. Для связи использую глобальный идентификатор кейса через плагин pytest_testrail. Настройка выполняется в конфигурации testrail.cfg на основе .В начале кейса нужно добавить строчку вида  . Так выглядит результат выполнения автотестов, переданный в TestRail: В названии передаю id джобы и название интерфейса, который тестирую. Вижу полную информацию по тестам, которые прошли успешно: и детальную информацию по ошибкам, если они есть: Файлы с автотестами добавляю в репозиторий, где ведется разработка самого приложения. При пуше видим последнюю версию тестов. Далее добавляю stage: autotest в файл конфига сборки .gitlab-ci.yml. Команда для запуска может выглядеть например так: Переменная   добавлена для идентификации сборки в TestRail. После пуша в ветку запускается сборка по алгоритму. Описание храним в файле gitlab-ci.yml. Если стадия автотестов пройдёт успешно, то актуальная версия выкатит в продакшн. Автотесты во спасение или Выход из штопора План сработал, мы смогли запустить автоматизацию и разгрузить команду: Этот год стал челленджем для всей команды и для меня лично. Мы сделали выбор в пользу TestRail из-за его интеграции с основным инструментом тестирования Pytest. Плюсом стало наличие готовых фреймворков. Gitlab CI используется в разработке информационных систем коммерции и полностью соответствует нашему запросу. Поэтому предпочли тому же Jenkins. По хорошей традиции запустили школу «Автоматизации тестирования» на практике, наш опыт может быть полезным другим командам тестирования в «Магните». Первые студенты-тестеры завершили обучение. Так практические навыки распространяются в компании. Наша команда покрыла автотестами регресс на web-приложениях, сейчас мы готовы автоматизировать функциональное тестирование. Впереди более сложные задачи, и конечно нам будет полезен ваш опыт: С какими проблемами перехода на автоматизацию тестирования столкнулись вы? Как решали сложные кейсы автоматизации функционального тестирования?",
  "tag": "Тестирование IT-систем",
  "hub": 1,
  "author": 1,
  "add_datatime": "2021-10-11T19:37:51.106Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 10,
 "fields": {
  "name": "Опасность редизайна для позиций вашего сайта",
  "image": "1633894671.1069.jpeg",
  "text": "Всегда ли обновление дизайна — это хорошо для сайта с точки зрения SEO? Нет, не всегда. Более того, часто редизайн влечет за собой только ухудшение позиций. Так может, лучше не трогать то, что и так работает? Частично эта тема поднималась в одной из предыдущих публикаций:  . На этот раз я расскажу более прицельно о смене дизайна и его последствиях. Опыт показывает, что изменение и обновление дизайна сайта раз в несколько лет — решение в целом правильное. Оно помогает улучшить юзабилити, увеличить конверсии и поднять позиции. Но почему тогда многие владельцы сайтов сталкиваются с совершенно обратным эффектом? Больше всего рискуют потерять часть трафика ресурсы, которые получают его преимущественно из органического поиска. И причина не только в том, что пользователям неудобно или непривычно пользоваться сайтом после обновления. Но и в том, что при допущении технических ошибок значительная часть контента может оказаться недоступной для поисковых ботов и для посетителей сайта. А уж если за редизайном следует и смена CMS, риски еще больше повышаются. Не проводите редизайн, если это на самом деле не нужно Сомнительные причины для проведения редизайна могут быть разными: Подобные «причины» — это глупости и профанация. Если уж вы задумываетесь о редизайне, убедитесь, что он принесет пользу: Если никакого реального профита вы не получите, рекомендуем повременить, лучше проанализировать конкурентов, составить Customer Journey Map и не действовать наобум. Грамотный редизайн с точки зрения SEO-продвижения Сделать красиво и удобно не значит сделать хорошо. Не все это понимают и отталкиваются только от визуала. А потом удивляются, что после всех переделок сайт или отдельные страницы проседают. Самый разумный способ избежать подобных проблем — предотвратить их еще на начальном этапе. Для начала неплохо бы провести аудит сайта, чтобы заранее выявить существующие проблемы, исправить их, а не усугубить в дальнейшем. Кроме того, мы в Elit-Web советуем следующее: В некоторых случаях разумно запустить сайт в обновленном дизайне на поддомене типа  , предложить юзерам оценить нововведения и получить от них обратную связь. При этом у пользователей должна оставаться возможность вернуться на старую версию. Если помните, по схожему пути в свое время шли «ВКонтакте», Facebook и другие популярные площадки. Изначально обновленные версии делали доступными по желанию с возможностью откатиться. А принудительный перевод всех на новый дизайн начинался позже. Это полезно крупным площадкам, онлайн-сервисам, сервисам банкинга, eCommerce и т. д. Остальным лучше обойтись без смены домена и сохранить старые страницы, визуально обновив их. ТОП-5 ошибок при редизайне сайта, бьющих по его позициям Если редизайн позволяет обойтись без смены URL-адресов, не меняйте их без веской на то причины. Если все же необходимо менять URL, то убедитесь, что вы не выбрасываете свои обратные ссылки и при перенаправлении старых URL сохраняете полномочия ссылок. Падение трафика вполне может быть вызвано некорректной настройкой robots.txt. Обязательно перепроверяйте файл в день запуска обновленной версии сайта. Не используйте «тяжелые» изображения в неподходящих форматах и переизбыток анимаций. Чем легче страница, тем лучше. Но не переусердствуйте. Пикселизированные и размытые картинки — еще хуже, чем медленная загрузка. Любые структурные изменения сайта требуют обновления Sitemap. Обязательно нужно вносить в карту сайта новые адреса страниц, чтобы ускорить индексацию. Также необходимо проверить, нет ли битых ссылок и не попали ли в индекс технические страницы. Что же в итоге? Редизайн — это хорошо. И никто не говорит, что сайты нужно оставлять в прежнем виде из года в год. Но если уж вы решили обновляться, для начала не помешает проконсультироваться с SEO-специалистами и разработать правильную стратегию. Иначе за красивой картинкой будут скрываться потеря трафика и впустую потраченные деньги. А вам это определенно не нужно.",
  "tag": "HTML",
  "hub": 3,
  "author": 1,
  "add_datatime": "2021-10-10T19:37:51.106Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 11,
 "fields": {
  "name": "Кто такой системный архитектор",
  "image": "",
  "text": "— Почему я здесь? — Твоя жизнь — это сумма остатков неуравновешенного уравнения, свойственного программированию Матрицы. Ты — возможный результат аномалии, которую, несмотря на мои искренние усилия, мне не удалось устранить из того, что в противном случае было бы гармонией математической точности.    Мир развивается. Прогресс не стоит на месте. То, что когда-то казалось фантастикой, сейчас становится обыденностью. Всё стремится к технологической сингулярности, совершенству и удобству — минимум действий, максимум возможностей. Это неспроста, ведь вычислительные платформы усложняются и множатся, возникают новые инструменты для преодоления тех или иных проблем и задач. И так сложилось, что сегодня любое крупное или не очень цифровое решение является сложной структурой, которая разработана под конкретные запросы и требования заказчика. Потому, чтобы не было проблем, а проект отработан четко, нужны люди с соответствующей квалификацией. А значит, сегодня мы поговорим за профессию 21 века — системного архитектора. Работа, связанная с проектированием IT-инфраструктуры информационных систем, высоко ценится на рынке труда. Ведь условия в нашем быстро меняющемся мире таковы, что цифровые нововведения становятся все более и более распространенными, они внедряются не только на корпоративном уровне, но и банально, даже в обычном быту. Следовательно, появляется необходимость в специалистах, которые могут проанализировать все процессы использования цифровых технологий на разных уровнях и создать единую архитектуру организации.    А в чём заключается работа IT-архитектора? Множество вещей предстоит сделать системному архитектору во время работы над проектом, но большинство из них определяются надобностью в данный момент, сложностью самого проекта и, конечно же, квалификацией самого архитектора, но даже из довольно массивного перечня задач можно выделить основные:        У архитектора нет фокуса на какой-то конкретный язык программирования, в рабочем процессе его задача, как главного инженера — быть важным связующим звеном между отделами, ведущими разные разработки, помогающим продумать и сделать так, чтобы эти самые разработки корректно работали друг с другом в рамках одного проекта, несмотря на самостоятельное развитие.   Олег Филимошин — архитектор Timeweb Cloud  По своей сути, выражаясь художественно, системные архитекторы — это первокрасные кардиологи-хирурги от мира IT, проводя высококлассные операции на «сердце» IT-инфраструктуры. Потому, крупный бизнес, чья инфраструктура построена на взаимодействии между технологическими элементами, не выжила бы в столь суровом мире цифровых технологий.      Какие знания будут полезны системному архитектору?   Требования к кандидатам на должность инженера проекта довольно высокие, что уже можно понять по сфере деятельности данной профессии. Есть ряд обязательных и желательных навыков, которыми должен обладать человек, претендующий на это место. Рассмотрим самые важные аспекты.  Одного знания языков программирования недостаточно, поскольку главное требование —иметь практический опыт, то есть напрямую участвовать в разработке. В вакансиях вы часто увидите такие требования:         Вдобавок ко всему этому необходимо понимать устройство IT-системы, ее ключевые сервисы, взаимосвязь их, домены и другие составляющие. Вы уже знаете, что системный архитектор самостоятельно курирует проект, значит, должен находить язык с нанятыми сотрудниками и заказчиками, если с ними придется общаться.  К часто требуемым навыкам еще можно отнести качества общего характера, то есть умение отстаивать свою точку зрения, настаивать на решениях, защищать позицию и искать компромиссы между сторонами.   Каким образом можно попасть на должность системного архитектора?   Добро пожаловать в профессию  Высшее образование в нынешнее время не во всех ситуациях является определяющим требованием, но специальное техническое точно может оказаться полезным. Больше всего внимание обращают именно на практические умения и понимание работы в целом. Очень часто компании выбирают на эту должность именно тех сотрудников, которые имеют опыт работы с подобными проектами. Или вовсе повышают сотрудников, которые уже работает в данной компании, так как им проще будет руководить, опираясь на уже имеющиеся знания о проекте.   Существует ли на этой должности «карьерная лестница»?   В рамках самой работы главным инженером проекта может только возрастать объёмность и сложность проектов, а соответственно и оплата. Но сама по себе подобная работа позволяет набрать достаточно опыта в любом направлении, которое будет интересно, за счёт того, что приходится следить и организовывать совместную работу многих отделов проекта, попутно в ней участвуя. Набрав нужных знаний и получив достаточно навыков можно выбрать любое направление в IT сфере и развиваться в нём дальше.   Сколько зарабатывают системные архитекторы?   Это вопрос, который наверняка волнует любого человека, ведь сама по себе работа весьма непростая, а значит и заработная плата должна быть соизмеримой. На следующем скриншоте вы видите выдачу четырех последних загруженных вакансий на Headhunter по Москве. Если же самому заглянуть на сайт, то вряд ли вы найдёте зарплату меньше 150 тыс. р., а основная масса компаний предлагает зарплату в районе 300-400 тысяч. Немногие вакансии в IT сфере могут так же хорошо оплачиваться, как системный архитектор.    Сравнить, допустим, можно с PHP-разработчиком, чья оплата труда в среднм составляет 150-200 тыс. рублей. Как другой пример можно взять должность технического директора, также посмотрев вакансии по Москве, чья зарплата начинается от 5 тыс. долларов, но которая относится к высшему менджменту и требует участия во всех до единого технических процессах.    Откликаются на эти вакансии не так много соискателей, в некоторых случаях можно вполне себе оказаться первым, и всё потому, что у многие разработчики не имеют достаточного опыта и навыков, чтобы к тому же быть ещё и человеком, понимающем в бизнесе. Опытных архитекторов тоже не хватает, для того, чтобы была сильная конкуренция на данную вакансию.     Вместо заключения Системный архитектор — это один из самых важных участников IT-инфраструктуры, отвечающий за большое количество технических процессов. Без его организационной работы зачастую не представляется возможным довести проект за конца.    Для этой работы вы должны уметь работать в рамках всех других должностей. Тяжёлые проекты позволяют быстрее построить свою карьеру, но зачастую излишне напряжённая работа приводит к выгоранию.  Если устали работать руками, «нажимая кнопки» и готовы взвалить на себя ответственность за себя и того парня, то это то, что вам нужно. Это работа неплохо нагружает «технический склад ума», а также позволяет проявить творческий подход к проекту, общаясь с профессионалами и большими начальниками, а то и мир спасая от какого-нибудь техно-краха. Если всё это вам близко и подходит, дерзайте. Проявляйте инициативу, развивайте кругозор и интересуйтесь «железом» во всех его проявлениях и смыслах. Ведь за вычислительными системами — весь современный мир и будущее!  «Вместо заключения» — Задачи и понимание должности системного архитектора отличается от компании к компании. Узнать, какие задачи выполняет архитектор в Timeweb и чем это отличается от CTO и тимлида можно в новом выпуске подкаста",
  "tag": "Управление проектами",
  "hub": 1,
  "author": 1,
  "add_datatime": "2021-10-10T19:36:26Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 12,
 "fields": {
  "name": "Защищайтесь! Советы по защите дизайна перед заказчиком",
  "image": "1633894671.1069.jpeg",
  "text": "Привет, меня зовут Маша Челоногова, я руководитель группы дизайна онлайн-сервисов в Lamoda. Сейчас мы расширяем команду, проводим много собеседований и сталкиваемся с тем, что многих кандидатов заводит в тупик простой вопрос: умеете ли вы защищать свой дизайн? Есть распространенное мнение, что если вкусы дизайнера и заказчика совпадают, тогда будет «мир-дружба-жвачка». Но это не так. Роль дизайнера эволюционировала, поэтому оценка результата работы становится более сложной задачей. А есть продуктовые дизайнеры, целью которых является решение конкретных бизнес-задач. А где есть решение задач, там нет места субъективизму. На первый план выходит то, насколько эффективно выполнена задача, в каком контексте, и решает ли она проблемы заказчика — там уже появляются другие категории оценки, а не только «хорошо или плохо», «нравится или не нравится». В этой статье я хочу поделиться небольшими советами, которые помогают в мире продуктовых дизайнеров, отстаивать свое предложение перед заказчиком.  Чем продуктовый дизайнер отличается от других Продуктовый дизайнер — молодая профессия. Главное отличие продуктового дизайнера от других — сильное погружение в продукт. У него есть глубинное понимание бизнес-процессов, какие задачи стоят перед продуктом, какие есть ограничения. Более четкого определения пока нет. Поэтому иногда даже дизайнера в агентстве, который поверхностно знает про бизнес клиента, но делает для него дизайн сайта или приложения, могут называть продуктовым. Бывает и наоборот — продуктового дизайнера внутри компании называют UI/UX-дизайнером, хотя он продумывает всю логику и концепцию целиком, а не только рисует кнопочки для интерфейса. Продуктовый дизайнер всегда работает в плотной связке с продакт-оунером, разработчиками, опирается на данные продуктовой аналитики и может предложить наиболее эффективный дизайн в зависимости от скорости разработки или трудности реализации. Типовая задача может звучать так: «Улучшить конверсию формы заказа». В таком случае для начала необходимо изучить и проанализировать текущую форму, чтобы затем выдвинуть определенные гипотезы по ее улучшению. Для продуктового дизайнера, как и для продакт-оунера, важны конверсии, средний чек, воронки и другие метрики, которые и будут являться результатом его работы. Но одно дело сделать дизайн, а другое — защитить его. С этим могут возникнуть трудности. И тут в бой вступает правильная коммуникация с заказчиком. Ниже перечислены две рекомендации, которые способны облегчить эту задачу. Больше, больше информации! В то время как заказчик не обязан разбираться в тонкостях дизайна, дизайнер должен знать наверняка, какой результат ожидает получить заказчик. Поэтому на первом этапе очень важно общаться с клиентом, задавать много вопросов: зачем нужен дизайн, какую бизнес-задачу он должен решить, по каким критериям и показателям будет измеряться результат. У вашего дизайна больше шансов на успех, если вы изначально получите задачу в правильном виде. Предположим, дизайнера просят нарисовать интерфейс — это довольно расплывчатая задача. И чем меньше деталей будет знать дизайнер, тем более шаблонным и поверхностным получится решение. И уж тем более не факт, что оно будет решать задачи заказчика. Самое важное — узнать, какая проблема стоит, почему человек пришел к дизайнеру, зачем ему это нужно. Бывают случаи, когда приходит заказчик и говорит: «Я хочу сделать сайт». А в процессе общения выясняется, что у него маленький магазинчик, и на самом деле сайт не нужен — достаточно инстаграм-аккаунта. Другими словами, вы можете предложить совершенно другое решение на самом старте, которое будет максимально решать потребности клиента. Не всегда запрос, с которым приходит клиент, решает его задачу. К тому же, когда вы задаете вопросы, заказчик видит, что вам небезразлично, и дает максимум информации, которой владеет. Чтобы перед началом работы у нас было, от чего отталкиваться, мы в Lamoda просим продакт-оунера ответить на вопросы. Он заполняет карточку в Jira, в которой описывает, что нужно сделать, для чего, когда, какие есть ограничения — например, разработка или какие-то юридические моменты. Когда продакт-оунер ее заполняет, то еще раз обдумывает, каким должен быть дизайн и по каким критериям его оценивать. Учим заказчика грамотно давать фидбэк Многие дизайнеры жалуются, что клиенты дают невнятную обратную связь. Но нельзя требовать, чтобы все люди разговаривали на языке дизайнеров. Это неправильно — ожидать от людей сразу качественный фидбэк. Этому нужно обучать. Одно время я работала в В2В-компании, у которой не было опыта работы с дизайнером. Нужно было сделать интерфейс распознавания текстов в облачной системе. Я выполнила задачу, показала дизайн продакт-оунеру, а он говорит: «Это не то». Тут можно было бы разозлиться и в сердцах хлопнуть дверью. Но так как у человека до этого не было опыта работы с дизайнерами, он думал, что если скажет, что это не то, то я пойду и сделаю то. И мне пришлось учить его давать обратную связь так, чтобы в итоге я сделала подходящий вариант. Дизайн, который вы предложите в первый раз, может быть неидеальным и даже не самым хорошим. Но у вас не должно быть цели защитить его во что бы то ни стало. Дизайнер с заказчиком находятся на одной линии, а не напротив друг друга. Вам нужно облегчить опыт пользователя и улучшить бизнес-показатели. Своими комментариями заказчик поможет сделать его максимально эффективным, потому что знает, как работает его бизнес, какие показатели ему нужно улучшить, какая у него целевая аудитория. Попробуйте задать наводящие вопросы, которые должны вас обоих навести на правильные мысли. Если продолжать пример с интерфейсом, то это может звучать так: Как проверить свой дизайн перед защитой Проверять свой дизайн до защиты перед заказчиком — нужный и важный шаг, который не стоит пропускать. Есть два ключевых метода, которые помогают понять, выполняет ли он свою задачу: качественные и количественные исследования.  проводятся на небольшом количестве респондентов. Самый простой и доступный вариант — коридорное тестирование. Обычно это выглядит так: дизайнер показывает прототип людям, которые находятся рядом (в офисе или даже дома), задает вопросы или предлагает выполнить несложное задание. По сути это UX-исследование на минималках: так можно проверить, насколько понятен текст, иконка и другие элементы дизайна. Подходите к человеку и спрашиваете: «Как думаешь, что эта иконка обозначает?». Или показываете текст и говорите: «Прочитай и расскажи, что ты понял». По ответу становится понятно, решает ли дизайн задачу или нет, нужны ли правки. Иногда уже на первом человеке можно понять, получаем ли мы нужный на результат или нет. Однако некоторые предпочитают собрать фидбэк от большего количества человек и уже после этого что-то менять. Но даже в случае с коридорным исследованием нужно следовать правилам. Например, нельзя задавать вопросы, которые подразумевают ответы «да» или «нет». Нельзя спрашивать, нравится или не нравится дизайн. При такой формулировке человек будет давать социально одобряемые ответы и собрать полноценную обратную связь не получится. Более подробно о том, как проводить такие исследования, можно почитать в книге «Об интерфейсе» Алана Купера. Бывают более сложные UX-исследования. В некоторых компаниях есть целые UX-лаборатории с eye-трекингами (устройствами, которые следят за глазами), микрофонами и другим оборудованием, которое следит за реакцией человека. У нас в Lamoda тоже есть  , через которую проходят почти все задачи. Полноценное UX-исследование более трудоемкое. По его ходу документируется все, вплоть до эмоций: что человек говорит, что делает, как реагирует. Например, в игровой индустрии респонденту могут надеть шапочку для энцефалограммы, с помощью которой измеряют усталость, концентрацию, эмоциональный фон и ментальную нагрузку. UX-исследования помогают выявить все слабые места: например, люди не понимают текст, не видят кнопки или, наоборот, в дизайне все ясно и понятно. Это позволяет дизайнеру прийти на защиту с проверенными аргументами, которые помогут ему отстоять ту или иную точку зрения перед заказчиком. Но представим, что заказчику по-прежнему хочется видеть другой дизайн. На этот случай есть второй метод —  . Он позволяет понять эффективность дизайна на больших цифрах. То есть дизайнер делает второй вариант по комментариям заказчика, а потом запускается А/В-тест: аудитория делится на два сегмента, которым показывают разные варианты дизайна. Дальше смотрим, какие показатели у одной и другой аудитории, например, конвертация в действие или продажи. По итогу можно сделать выводы: «В первом случае люди покупают больше, а те, которые видели другой вариант, не изменили своего поведения». Это уже очень сильный аргумент — с деньгами очень трудно спорить. К количественному методу относятся и опросы, но для них берется большая выборка от 500 человек — только тогда можно сослаться на статистическую значимость. Не всегда их можно провести самостоятельно, для таких исследований есть готовые внешние сервисы. Обосновать и защитить свой дизайн в большинстве случаев можно. Но иногда продуктовые дизайнеры сталкиваются с барьерами внутри компании, которые не позволяют в защите работы использовать те навыки и методы, о которых я рассказала выше. В такой ситуации приходится использовать всю силу своего красноречия, чтобы отстоять дизайн.",
  "tag": "HTML",
  "hub": 3,
  "author": 1,
  "add_datatime": "2021-10-10T19:37:51.106Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 13,
 "fields": {
  "name": "DevOpsConf: информация к размышлению",
  "image": "",
  "text": "В начале лета я участвовал в конференции DevOpsConf. Мероприятие оказалось очень полезным и ценным. И дело не только в том, что DevOpsConf дала возможность узнать что-то новое из технологий или опыта. Конференция натолкнула на несколько принципиальных выводов. Этот пост – не про итоги DevOpsConf и даже не про содержание докладов. Зачем их пересказывать? Проще воспользоваться записью и послушать самостоятельно. Я же хочу поделиться мыслями о DevOps, которые пришли в голову, пока я слушал доклады коллег. DevOps: человек или машина? Несколько лет назад многие компании искали DevOps-инженеров и не очень четко себе представляли, чем они занимаются. Не верите? Посмотрите архивные посты на Хабре: среди них можно найти такие темы как «Кто такой DevOps-инженер и чем он занимается?». Так было пять лет назад, четыре года, три… В одном объявлении о поиске такого специалиста в необходимые навыки умудрились запихнуть все, что можно, начиная и знания git и заканчивая непонятным требованием «понимания практики DevOps».  На самом деле, у каждого DevOps есть своя специализация. Одни хорошо шарят в облаках, другие разбираются в том, как строить отказоустойчивые системы, третьи могут быть гуру микросервисов. Объединить все эти роли и сделать DevOps-инженера многоруком Шивой просто невозможно.  Во-первых, это размывает границы ответственности такого специалиста, а во-вторых, выгореть с таким набором обязанностей можно буквально за неделю. И с таким отношением к DevOps-инженерам нужно что-то делать. Пора определяться не в профессии, а с самой профессией. Минутка терминологии Я сразу коснусь моментов, которые будут важны дальше и позволю себе потратить еще немного букв на описание терминологии. Начнем с ITIL. Раньше эта аббревиатура описывала специальную библиотеку книг, в которых собирались лучшие практики на тему инфраструктуры информационных технологий. Сегодня под ITIL понимают сложную методологию практик, которая покрывает весь процесс разработки софта. Ее ненавидят из-за этого буквально все, потому что прочитать все книги, входящие в библиотеку, могли только те, кто на внедрении ITIL зарабатывает. Еще одна нужная нам аббревиатура – SRE. В Википедии говорится, что это (Site Reliability Engineering) «набор методов, показателей и предписывающих способов обеспечения показателей систем (именно в таком значении здесь используется слово site, в вовсе не для обозначения веб-сайта). На самом деле SRE – практики, которые придумали инженеры Google для других инженеров, чтобы разрабатывать и поддерживать отказоустойчивые системы. SRE vs ITIL На конференции прошла интересная дискуссия между сторонниками SRE и ITIL. Они рассматривали эти подходы с разных сторон. И были высказаны очень интересные суждения. Если рассматривать SRE и ITIL как два разных  , то оказывается, что ITIL требует автоматизировать рутину, а SRE считает, что нужно автоматизировать вообще все. Поспорить можно с обеими точками зрения. ITIL: при автоматизации рутинных операций все равно останутся некоторые неавтоматизированные функции, и в них очень велик риск ошибок, связанных с ручной работой. SRE: при такой автоматизации стоимость разработки и поддержки систем станет неподъемной. Второй пункт, в котором видны различия в подходах – подготовка к авариям. SRE говорит о том, что у нас есть Chaos Engineering, где, если утрировать, мы рандомно отключаем сервера из сети в production, отслеживаем реакцию систем на отключения и анализируем, как изменить систему таким образом, чтобы она выдерживала подобные отказы.  ITIL же считает, что на все случаи жизни есть Disaster Recovery Plan, в котором описываются все необходимые действия при возникновении аварии. В случае Chaos Engineering не всякий бизнес согласует разрушение прода, пусть и контролируемое. Работает - не трожь. А при DevOps и быстрых поставках релизы у нас выкатываются гораздо чаще, чем обновляется Disaster Recovery Plan, что делает его неактуальным. Третий момент – разность в подходах к работе комитета по изменениям. ITIL требует, чтобы все изменения согласовывали в специальном комитете с учетом мнений всех заинтересованных сторон. По моему опыту такая работа очень сильно усложняет жизнь: приходится ждать всех согласований, которые могут занимать недели, а в сам комитет зачастую попадают люди, которые толком не разбираются, что и как мы хотим изменить. Хотя, процесс согласования можно и упростить. Например – при выкатывании отдельных изменений, не приводящих к даунтайму, просто уведомлять о них. И это – уже SRE-подход. Мораль: в чистом виде использовать методологию ITIL или методологию SRE не получится. Нужно искать золотую середину, потому что и у того, и у другого подхода масса проблемных мест, которые не описывают все возможные ситуации. В отдельных случаях хорошо работает ITIL, в других – SRE. DevSecOps Еще одна сторона работы DevOps – согласование изменений с безопасниками. Это может стать очень серьезной головной болью. Сначала они тратят время на проверку уже готового релиза, присылают pdf на 500 страниц со списком уязвимостей, а потом вся команда разработки дружно занимается внесением исправлений в архитектуру и код. Так происходит, если безопасники и разработчики живут в разных мирах. Мы нашли способ избежать проблем: в нашей команде работает security champion с опытом в ИБ, который непосредственно участвует в разработке и на ранних этапах определяет риски и уязвимости. Например, он внедрил инструмент, который еще на этапе разработки сканирует облака и дает рекомендации по оптимизации. В каждый спринт он добавляет пару задач на устранение наиболее критичных уязвимостей, и мы их ликвидируем. Читайте книжки! Десять лет назад DevOps-инженеры учились интуитивно: просто «добирали» в разных источниках знания, которых не хватало на практике. Если не работал один подход, можно было попробовать другой. Это, конечно, интересно, но такое обучение требует не только времени, но и других ресурсов. Многое, как известно, придумали до нас. И даже описали это в умных книжках. И именно их можно порекомендовать. Первая из них – «Руководство по DevOps». Здесь описано как выстроить команду и управлять ею, как обучать сотрудников и находит на это время. Эта книжка – про процессы. На бумаге она, кстати, закончилась, но можно почитать электронную версию. Вторая книжка – «Site Reliability Engineering», тот самый набор практик от Google с конкретными рекомендациями, вплоть до организации дежурств. Она есть в открытом варианте на английском, но есть и в русском переводе на бумаге. Многие (я про нее слышал на многих конференциях в разных банках) советуют книгу «Accelerate». Купить ее можно на Amazon или на Ozon, если вы предпочитаете читать на русском. Я рекомендациям верю, поэтому тоже эту книгу упомяну. Наши разработчики немножко знакомы с инфраструктурой, знают базовые возможности Kubernetes и многое другое. Но такой подход должен работать и в обратную сторону. Если ты SRE, то учись кодить, чтобы в случае аварии на проде самостоятельно ее ликвидировать или помогать разработчикам оптимизировать софт под нагрузку. Мелочи, но тоже очень важные И еще несколько полезных советов, которые я услышал на конференции. Они прозвучали в разных докладах, и формируют достаточно четкую картинку правильных подходов к DevOps. Технический директор «Флант» Дмитрий Столяров рассказывал о том, почему нужно не делать Kubernetes, а использовать его. Эта история – про нас. В моей предыдущей компании мы поддерживали свой кубер со всеми вытекающими, в МВидео же используем готовые облачные решения, и всей сопутствующей работой занимается провайдер. И это перекликается с одним из тезисов Дмитрия: если нужно сделать систему надежной, то из нее скорее нужно что-то убрать, нежели добавить.  . . Это – отсылка к докладу коллег из Х5 Retail Group. Они рассказывали о том, как внедряли систему трейсинга, стремились сделать проект быстро и сравнивали разные инструменты. Они в итоге выбрали платное решение. У нас похожий опыт, но мы выбрали опенсорсную платформу, и получилось у нас не менее надежно и не менее быстро. Поэтому, прежде покупать что-то коробочное, стоит посмотреть на Open Source. , и DevOps, и Java – самые разные сообщества по интересам. Когда в компанию, особенно крупную, приходит новый человек, он сталкивается со множеством систем, не понимает, кто за что отвечает, не ориентируется в документации. Все это делает очень высоким «порог входа». Упрощает проблему онбординга новичков комьюнити: митапы, общие тематические чаты, вики и т. п. И последнее. Многие считают, что участвовать в таких конференциях – напрасно тратить время. Не согласен. Я не только узнал много полезного для себя. Доклады и обсуждения, которые я прослушал, очень помогли решить очень важную задачу: я убедился в том, что мы движемся в правильном направлении. Добиться такого понимания можно и без конференции, но обойдется оно в этом случае дорого. Потому что опыт может оказаться и отрицательным.   В нашей команде  . Приходите, будет интересно.",
  "tag": "DevOps",
  "hub": 1,
  "author": 1,
  "add_datatime": "2021-10-10T19:36:26Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 16,
 "fields": {
  "name": "Покорение клетки — становление гистологии",
  "image": "1634278672.286357.png",
  "text": "Наиболее распространённый в конце XVIII века микроскоп системы Адамса, состоял из двух окуляров и двух объективов и давал увеличение от 300 до 1000 крат. С таким микроскопом работали Мирбель, Молденгауэр и Мейен, но для своих исследований они использовали более слабую апертуру, позволяющую увеличивать рассматриваемый объект от 150 до 300 крат. Дальнейшее увеличение становилось бесполезным, так как поле зрения искривлялось, и изображение получалось туманным, с резко выраженной хроматизацией. Изначально эти дефекты объяснялись низким качеством пришлифовки линз. Но это было правильно только отчасти, а главный дефект заключался, по всей видимости, в хроматической и сферической аберрации. Первыми на это обратили внимание оптики (теоретики и механики), работавшие с исправлением аберрации. В результате их исследований уже в 1754 году английский оптик Доллонд благодаря работам русского математика Эйлера смог создать ахроматический объектив для телескопа, состоявший из двух линз. Одна из них была сделана из кронгласа – совершенно прозрачного стекла, не содержащего свинца. А вторая линза была выполнена из флингласса – стекла, выплавленного из кремнезёма, калия и свинца, преломляющего сильнее кронгасса. Открытие Доллонда позволило делать ахроматические линзы для микроскопов. Но прежде чем сделать такой микроскоп, необходимо было решить две непростые технические задачи: во-первых, добиться получения ахроматической пары из очень маленьких короткофокусных линз и, во-вторых, найти сравнительно недорогой способ изготовления флингласса. Решению этих задач положили начало русские учёные. Первым, как уже было сказано выше, был математик Эйлер. Он опубликовал работы, дающие теоретическое обоснование для ахроматических объективов микроскопов. Десять лет спустя, после публикации работ Эйлера, академик Эпинус собрал первый ахроматический микроскоп. Но он оказался очень громоздким: длина тубуса была от 85 до 120 сантиметров, а предел полезного увеличения достигал 180 крат. Мастера-стеклодувы конца XVIII века вели усердные поиски флинтгласа высокого качества. Но вопрос решил любитель – часовых дел мастер из Швейцарии по фамилии Гинан. Он работал в компании с известным физиком Фраунгофером и добился получения флинтгласа высочайшего качества. Один вопрос был решён, но другой – изготовление очень маленьких короткофокусных линз – всё ещё оставался открытым. Например, ахроматический микроскоп Фраунгофера, созданный им в 1811 году, давал увеличение всего в 120 крат. Совершенно ясно, что ни этот микроскоп, ни микроскоп Эпинуса ничего существенного натуралистам дать не могли. В 1824 году французский оптик Селлинг предоставил в Парижскую Академию микроскоп, сконструированный мастерами Винцентом и Шарлем Шевалье. Это был совершенно новый прибор, в котором объектив состоял не из одной, а из нескольких пар свинчивающихся вместе линз. Это значительно повышало даваемое ими увеличение. Кроме того, сферическая аберрация в этом микроскопе была снижена до минимума благодаря высокому качеству флингласса. Однако и этот микроскоп был непомерно большим, а потому неудобным. Чтобы решить эту проблему, в 1827 году итальянский оптик и астроном Амичи собрал микроскоп нового типа. Тубус этого микроскопа был расположен горизонтально на штативе с круглой подножкой, объектив крепился к тубусу под прямым углом, а над объективом в особом колене была расположена призма, которая переводила лучи, идущие от исследуемого объекта из вертикального положения в горизонтальное. К началу 30-х годов XIX века эти микроскопы были лучшими, но стоили непомерно дорого. Оптики продолжали ломать голову над усовершенствованием этих инструментов, не забывая о необходимости понизить их стоимость, и сделать микроскоп доступным для научных работ. Уже в те годы существовали несколько фирм, занятых изготовлением микроскопов. В Вене работала фирма Плесселя, в Берлине – фирма Шика, в Париже – Шевалье и Гартната. Во Франции преимуществом пользовались приборы Шевалье, но в ходу были и инструменты производства Шика и Плесселя. Штатив микроскопа Плесселя был устроен по типу штатива Адамса: состоял из треножника и колонки, к которой на шарнирах прикреплялись штанга, несущая трубку с объективом, зеркальце и предметный столик с микрометрическим винтом, двигавшим столик по горизонтали. Чтобы осветить какой-либо объект, пользовались призмой, две грани, которой имели сферическую поверхность. Призма эта была расположена на вершине специальной колонки, вставленной в одну из ножек штатива. Примерно так же был устроен микроскоп Шика. Микроскопы этих оптиков давали полезное увеличение до 300 крат. Фактически увеличение могло достигать 1500 – 2400 крат, но им не пользовались, так как при больших увеличениях изображение получалось тёмным. Несколько позже добились полезного увеличения в 450 – 500 крат. Именно с такими приборами приступили к штурму тайны клетки Шлейден и Шванн. Сын врача Матиас Шлейден начал свой путь как доктор юридических наук, он читал лекции в университетах Иены, Дерпта и других городов. Однако нигде не задерживался надолго. Непоседливая натура, страстный боец за научные ценности, он обладал всеми достоинствами и недостатками этого типа людей и был полной противоположностью своему другу, спокойному и уравновешенному Шванну. Шлейден утверждал, что природа познаётся путём созерцания её многообразных форм, красок и действий, что органом, познающим мир, является глаз – источник истинных и ложных представлений о вселенной, поэтому он придавал огромное значение микроскопу.  Поэтому он останавливался на описании различных микроскопов, подробно рассказывая, как надо ими пользоваться, и подчёркивая, что при микроскопических исследованиях ошибается лишь исследующий разум, но не глаз и не микроскоп.   – цитировал он изречение Гёте. По мнению Шлейдена, есть одно существенное условие, без которого ни нормальный глаз, ни даже сверхсильный микроскоп не могут гарантировать исследователя от ошибок: всякий научный работник, приступая к своей специальной теме, обязан досконально знать всё, что было изучено по данному вопросу до него. Шлейденом была проведена огромная самостоятельная работа по строению растительной клетки, её составу, возникновению, росту и жизнедеятельности. Он объединил в своей работе сложившийся до него фактический материал, подверг его всесторонней критике, очистил от заблуждений и присовокупил к этому свои многочисленные наблюдения, создав более или менее стройное и законченное учение, скреплённое несколькими важными обобщениями, открывшими путь к дальнейшему развитию цитологии. Шлейден утверждал, что первичным формообразующим продуктом в минеральном царстве является кристалл, а в растительном – клетка. Клетка – элементарный орган любого растения. Простейшее растение состоит из единственной клетки, а все остальные растения почти целиком сложены из отдельных клеток и их модификаций. На этом он, однако, не остановился и взял на себя смелость категорически заявить, что жизнь любого растения должна заключаться в жизни составляющих его клеток. Вот как Шлейден формулирует максиму, обязательную для любого ботаника: «Всякая гипотеза, всякая индукция должна быть безусловно отброшена, если она не стремится объяснить все процессы, протекающие в растении, как результат совершающихся в отдельных клетках изменений». Шлейдену был хорошо известен сложный состав клетки, разнообразие находящихся в ней органических и неорганических, жидких и оформленных веществ. Своеобразное взаимодействие между содержимым клетки, с одной стороны, а физическими и химическими силами – с другой, он называл жизнью. В самом жизненном процессе Шлейден различает следующие моменты: поглощение пищи и её ассимиляцию, образование секретов, удаление ненужных для жизни продуктов, оформление ассимилированных веществ, внутриклеточное движение, возникновение новых клеток, их рост и смерть. Большое внимание Шлейден уделяет вопросам происхождения и размножения клеток. В одной из работ Шлейдена размножение клеток сводится к их нарождению из бесструктурного жидкого вещества, которое он называет цитобластемой. Она, согласно автору, содержит в себе сахар, декстрин, слизь и заключается внутри клетки, где из неё образуются новые клетки. Существенным элементом цитобластемы, по Шлейдену, являются зернышки слизи, состоящие из какого-то азотистого вещества. На основании своих наблюдений над растительными клетками он делает вывод о невозможности превращения одних типов клеток в другие. В те годы были уже известны многие виды одноклеточных животных и растений, а также подвижные споры «тайнобрачных». Даже сейчас сложно провести строгую грань между представителями животного и растительного царств, занимающими низшую ступень биологической лестницы. Но ещё труднее это было сделать во времена Шлейдена, когда считалось, что одноклеточное растение может превращаться в одноклеточное животное и наоборот. Такие «фантазии» Шленйден называл «грёзами больной науки». И по поводу такого предположения говорил:  В числе выдающихся учёных, способствовавших прогрессу цитологии, одним из первых является Карл Негели. Уроженец Швейцарии, ученик Окена и Деканоля, сотрудник Шлейдена по редактированию ботанического журнала и автор целого ряда глубоко проработанных трудов, Карл Негели является одним из крупнейших ботаников XIX века, открывших много нового в области естествознания, высказавших много оригинальных по различным вопросам морфологии и физиологии растений. В своей работе «Об истории развития цветковых пылинок» в 1842 году Негели наглядно показал, как возникают эти пылинки благодаря делению производящих их клеток. Этим было положено начало целой серии других работ на тему происхождения клеток. Надо было разобраться, как протекает размножение одноклеточных водорослей, как образуются споры и половые клетки у некоторых «тайнобрачных», в чём сущность этих процессов. То, что удалось наблюдать Негели на протяжении нескольких лет, и то, что нашло отражение в его статьях, привело к заключению об ошибочности взглядов Шлейдена. Стало ясно, что растительные клетки размножаются делением. Негели даже удалось в некоторых случаях наблюдать деление ядер, но из-за недостатка технических средств он не мог проследить и оценить сложную картину клеточного деления в деталях. Да и нельзя этого требовать от учёного, производившего свои исследования в 40-х годах XIX века. Негли и Шлейден сделали много для развития учения о клетке и тканях, но не меньше сделал Теодор Шанн. Сын книготорговца, ученик и ассистент Иоганнеса Мюллера после защиты диссертации о дыхании зародыша курицы стал профессором в университете города Лувен. За долгие годы научной деятельности он прославил себя многими трудами: ставил опыты с искусственным пищеварением, изучал сократимость мышечных волокон под влиянием механического раздражения, изучал процессы брожения и, вопреки мнению таких авторитетных учёных, как Берцеллиус и Либих, связывал этот процесс с жизнедеятельностью дрожжевых грибков. Он открыл пепсин, интересовался вопросом самопроизвольного зарождения, провёл ряд наблюдений над регенерацией нервных клеток, изучал строение нервных окончаний, производил исследования инфузорий, открыл в спинной хорде зародыша лягушки ядросодержащие клетки, похожие на клетки растений. Встретившись со Шлейденом, Шванн познакомился с его взглядами на растительные клетки. В ходе многочисленных дискуссий выяснилось сходство их взглядов на индуктивный, экспериментальный и генетический методы изучения жизненных процессов, на значение эмбриогенеза и гистогенеза для понимания строения организма. Так было положено начало прочной дружбе между двумя выдающимися учёными второй четверти XIX века. В начале 1838 года Шванн сделал краткий доклад о клеточном строении животных. В августе и декабре того же года он представил в парижской Академии первые два раздела своей книги. А в 1839 году, после написания третьей части, содержащей теорию, вышла в свет и сама книга под названием «Микроскопические исследования о сходстве в строении и росте животных и растений». Это было значительное событие в истории биологии, которое показало широкие перспективы для дальнейшего развития и процветания. В предисловии к своему труду Шванн написал значимые для своего времени слова: Далее Шванн указывал на то, что ботаника и зоология долгое время были несколько изолированы друг от друга и выводы и обобщения одной из этих дисциплин мало сказывались на выводах другой.  Укреплению и углублению этой связи посвящает Шванн свой труд. Его книга ставит задачей объединить оба царства живой природы, исходя из единства тех законов, согласно которым возникают, растут и развиваются элементарные структурные единицы растений и животных. Книга Шванна состоит из трёх частей, не считая небольшого предисловия и введения, в котором даётся краткий обзор того, что сделали для цитологии Шанн и Шлейден. Первая часть посвящена микроструктуре хорды и хряща. От хорды вполне естественен переход к изучению структуры хряща, и Шванн принимается за решение этого вопроса. Материалом ему служат плавниковые лучи плотвы и хрящ зародыша лягушки. Ему удаётся установить наличие клеток и межклеточного вещества. Форму клеток Шванн описывает как полиэдрическую, с несколько закругленными углами. Клетки окружены нежной оболочкой, которая со временем утолщается, и содержат ядро. Межклеточное вещество возникает благодаря слиянию нескольких клеток. Согласно наблюдениям Шванна, размножение клеток хряща происходит так же, как описывал деление растительных клеток Шлейден. Первую часть своей книги Шванн завершил так:  Вторая, самая объёмная, рассказывает о клетках как об основном строительном материале всего живого. Она начинается с яйца – «клетки с мелкозернистым содержимым, одетой оболочкой». «Зародышевый пузырёк», открытый Пуркинье, Шванн рассматривал как ядро. На «зародышевое пятно» внутри ядра обратил внимание Р. Вагнер. Шванн классифицировал животные ткани.  Это, по мнению автора, зависит в свою очередь от степени их самостоятельности. На основании этого классифицирующего признака Шванн подразделяет все клетки на пять типов. К первому типу относятся изолированные самостоятельные клетки, например, клетки крови и яйцеклетки. Ко второму типу Шванн причисляет «самостоятельные клетки, объединённые в ткани, то есть плоский и цилиндрический, бокаловидный и реснитчатый эпителий». Описанию этого типа тканей Шванн посвятил несколько страниц, перечисляя данные, которые получил в ходе своих многочисленных опытов. Пигментным клеткам Шванн также уделяет большое внимание, описывая их способность менять форму. По словам автора, пигментные клетки могут сжиматься в тёмный шарик или принимать форму звезды. Он описывает ряд переходов между двумя этими формами. В эту же группу Шванн относит клетки эпидермиса, которые образуют роговую ткань ногтей, волос, когтей и перьев, а также составляют основную массу хрусталика глаза. Это утверждение базируется на основании наблюдения над восьмидневным зародышем цыплёнка, глаз которого состоит из крупных прозрачных клеток и лишён характерных для сформировавшегося хрусталика волокон. Шванн утверждает, что данный тип представляет собой наиболее примитивные элементы. Являясь таковыми, они весьма сходны с паренхимой растений, и сходство это простирается так далеко, что  Большая часть животных клеток отличается от вполне развитых растительных клеток тем, что они мягче, нежнее последних. И, наконец, для клеток данного класса характерно наличие более или менее ясно выраженных оболочек. Нигде они так чётко не отграничены от содержимого клеток, как здесь. Шванн находит, что ткань хорды представляет собой как бы естественный переход от одного класса к следующему. Сюда он относит хрящевую и костную ткани, а также специализированные ткани, из которых строятся различные части зубов: эмаль, дентин, костное вещество зуба и пульпа. Подробно описав во вступительном разделе своего труда гистологическое строение хряща и его окостенение, Шванн к определённому выводу об этом процессе и о костных клетках так и не пришёл. Для него несомненным было только одно: костная ткань исключительно богата твёрдым, пропитанным известью межклеточным веществом, которое состоит из тончайших пластинок. Среди этого вещества разбросаны маленькие полости с расходящимися от них канальцами.  – писал Шванн. Таким образом, он не описывал костные клетки, но допускал их существование по аналогии с другими тканями. Давая характеристику тканям III класса, Шванн предполагал, что имеет дело со слиянием клеточных оболочек, образующим довольно мощное межклеточное вещество. Иначе всё происходит у тканей IV класса. В этом случае сливаются не только оболочки, но и большая часть самих клеток. Это различные формы соединительной ткани, развитие которой он прослеживает как у эмбрионов, так и у взрослых животных. Наиболее точную характеристику этой группы даёт сам Шванн:  Эта цитата ценна не только тем, что в ней описан способ образования соединительной ткани, но и ясно сформулирован взгляд Шванна на процесс образования клеток из цитобластемы, аналогичный взгляду Шлейдена. К пятой группе Шванн относит мускульную и нервную ткани. Как верно отмечает Шванн, они представляют собой наиболее специализированные ткани. В третьей части научного труда идёт речь о взглядах Шванна на строение клетки – цитологию.  Таким образом, можно смело утверждать, что именно Шванн является основоположником гистологии как самостоятельной науки. Но не в этом заключается самое замечательное открытие этого ученого. В соавторстве со своим другом и коллегой Шледеном они смогли сформулировать клеточный закон. Клеточная теория содержит три главных обобщения: теорию образования клеток, доказательство клеточного строения всех живых организмов и частей организма и распространение этих двух принципов на рост и развитие животных и растений. Три этих обобщения можно сформулировать в виде следующих «законов»: Одной из основ клеточной теории было представление, высказанное Шлейденом и воспринятое Шванном, о свободном образовании клеток из бесструктурного вещества, находящегося внутри клеток или вне их в виде специального клеткообразующего вещества (цитобластемы). Однако в 1838 году в свет вышла работа Моля «О развитии устриц», где впервые в истории было описано деление клеток. Видным критиком теории образования клеток из бесструктурного вещества был немецкий патолог Рудольф Вирхов, крупный исследователь в различных областях науки, общественный деятель, великолепный оратор и прекрасный популяризатор. Своё убеждение он сформулировал в виде латинизма «Omnis cellula e cellula» («Каждая клетка [происходит только] из клетки»). В возрасте восемнадцати лет Вирхов поступил в медико-хирургический институт Берлина. Его учителями были знаменитый клиницист Лука Шёнлейн и выдающийся клиницист Иоганнес Мюллер.  Таков девиз Шенлейна, выдвинутый им против натурфилософии, вторгнувшейся в пределы точного знания и подчинившей себе медицину. Он его строго проводил в жизнь и у постели больного, и за секционным столом, и в лаборатории, где впервые стали производиться микроскопические и химические исследования для определения болезни и знакомства с её ходом. Врач-мыслитель, стяжавший славу исключительно даровитого клинициста, он и студентов учил мыслить. Впрочем, значительно больше Вирхов получил от другого своего учителя – Иоганнеса Мюллера, поражавшего студентов широтой своих научных знаний и независимостью мышления. В 1834 году Вирхов защитил диссертацию на соискание степени доктора медицины, после этого перед ним открылась широкая дорога.  Получив должность ассистента при патологоанатомическом институте больницы в Берлине, он полностью отдался изучению патологической анатомии и выполнил ряд работ, ставших классическими. Свои выводы Вирхов подкреплял опытами на животных, внедрив в практику метод экспериментальной патологии. В 1847 году Вирхов и его коллега Рейнгардт выпустили первый номер журнала «Архив патологической анатомии, физиологии и клинической медицины», в котором опубликовали статьи по научной медицине. Передовая статья принадлежала самому Вирхову. Вскоре в Германии настали тревожные дни. В Силезии разразилась эпидемия голодного тифа (так в те времена называли сыпной тиф). Вирхов был командирован туда для изучения эпидемии и борьбы с ней. Сообщение, представленное Вирховым по итогам командировки, пришлось не по вкусу прусскому правительству. Слухи об этом дошли до Вюрцбурга, и там решили пригласить опального профессора в свой университет на кафедру патологической анатомии. Вирхов колебался и обратился за советом к своему учителю Шёнлейну. После разговора с ним Вирхов покинул Берлин и переселился в Вюрцбург. Именно там начался новый этап жизни Вирхова. Пять лет (с 1850 по 1855 г.г.) Вирхов посвятил разработке клеточной теории. Ему предстояло дать последний ответ на две главные проблемы. Во-первых, требовалось доказать, что все ткани действительно сложены из клеток, и, во-вторых, нужно было опровергнуть теорию новообразования клеток из бесструктурного вещества. В ходе первых работ Вирхову удалось не только обнаружить, но и изолировать клетки сначала костной, затем хрящевой, а потом и соединительной тканей. Его выводы положили конец сомнениям в том, что клетки являются строительными элементами животных тканей. Вслед за этим Вирхов предпринял серию наблюдений для окончательного выяснения вопроса о происхождении клеток. Наконец, более обстоятельное знакомство с микроскопической структурой больных тканей и органов привело его к убеждению, что местом, где разыгрываются патологические процессы, служат сами клетки и примыкающие к ним «клеточные территории» и что ненормальная деятельность клеток, вызванная изменением обычных условий их жизни, служит источником различных заболеваний. Это было основным пунктом его выводов. Наблюдая как нормальный, так и патологический рост костей, Вирхов показал, что рост костной ткани во всех случаях сводится к размножению клеток делением. Затем к такому же выводу он пришёл, изучая процесс возникновения и роста различных болезненных новообразований. И тут, как он и полагал, всё дело в чрезмерном или ненормальном размножении клеток, входящих в состав тканей. Так была поставлена последняя точка в вопросе происхождения клеток. Даже у самых упорных приверженцев теории происхождения клеток из бесструктурного вещества после экспериментов и наблюдений Вирхова не осталось никаких сомнений в том, что клетка может появиться только из другой клетки. Но научные открытия не исчерпывают деятельность Вирхова. Часто даже в настоящее время приходится слышать, что серьёзная научная работа несовместима с общественной и политической деятельностью. Вся жизнь Вирхова служит блестящим опровержением этой точки зрения. Он отдавал много сил и внимания вопросам общественной медицины, гигиены и санитарии в больницах, школах, рабочих жилищах и принимал деятельное участие в муниципальной и политической жизни Германии. В молодости Вирхов был настроен революционно. Смело протестовал против правового и политического строя Пруссии, принимал участие в революции 1848 года. Будучи неизменным и деятельным членом партии «свободомыслящих», переименованной впоследствии в партию «прогрессистов», он долго держался на позициях либерализма и боролся с реакционными мероприятиями правительства и самого Бисмарка. Но постепенно вместе с ростом классовых противоречий в Германии его радикализм поблёк. Он всё чаще и чаще настаивал на необходимости реформ, а не революции и открыто выступил против крепнущей немецкой социал-демократии. Этот постепенный отход от убеждений юных лет шёл у Вирхова рука об руку с «попранием» научного мировоззрения. Так, ещё до появления теории Дарвина он высказывал эволюционный взгляд на природу, затем встал на защиту учения Дарвина, а в старости признал «опасным» преподавание дарвинизма в школах. Популяризация науки входила в число общественных обязанностей Вирхова. Физиологические, гигиенические, общебиологические, научно-философские и медицинские вопросы – вот предметы его общедоступных статей и речей. Всякий, даже специальный вопрос оживал в его изложении, облекаясь в интересные и увлекательные формы. В своей статье, посвящённой лихорадке, он проявил свою всестороннюю образованность, приводя ссылки на историю, мифологию и народные предрассудки и излагая эту тему красивым, лёгким языком. Дальнейшее развитие цитологии и гистологии шло бок о бок с новыми открытиями в области микроскопической техники. Были разработаны методы окраски тканей, позволившие изучить мельчайшие детали в их строении. Изобретение электронного микроскопа позволило учёным заглянуть в ультраструктурное устройство клетки, а успехи в молекулярной биологии сделали понятными многие механизмы функционирования клеток. Современный микроскопист в своей оборудованной по последнему слову науки лаборатории может пожать плечами и спросить:  Бритва, игла, вода, этиловый спирт – вот их орудия производства. Исследуемый образец «обрабатывался» мацерацией. Срезы делались бритвой. Препараты просветлялись скипидаром, фиксировались спиртом или бихроматом калия, сохранялись высушенными между двумя стеклами, не подвергались окраске. Чтобы добиться блестящих результатов при таких трудных условиях работы, Этим учёным нужна была только особая волевая закалка и безграничная любовь к исследованию.",
  "tag": "гистология",
  "hub": 4,
  "author": 1,
  "add_datatime": "2021-10-15T06:17:52.287Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 17,
 "fields": {
  "name": "Как я снял кольца Сатурна на ТЕЛЕФОН",
  "image": "",
  "text": "Данный материал является продолжением моей первой статьи  . Тогда речь шла о снимке, сделанном на ( ) Huawei P40 Pro Plus.  В комментариях к той статье нашлись как скептики, так и оптимисты. Первые называли треки спутников артефактами, вторые предлагали снять кольца Сатурна. Чтобы ответить на критику первых, и поэкспериментировать с предложением вторых, в прошлую пятницу я решил повторить опыт.  Как и в прошлый раз из дополнительного оборудования с собой у меня был только штатив. Но если тогда я выбирал параметры съемки наобум, не особо рассчитывая на результат, то теперь я хотя бы знал что для снимка мне не нужна длинная выдержка. После всего нескольких попыток удалось подобрать необходимые настройки, и, как говорится, результат на лицо: Легко различимы Ганимед, Европа и Каллисто. Для проверки заходим в Stellarium. К сожалению, спутник Ио опять попал в блик Юпитера. Видимо, из-за своей близости к планете для мобильной астрофотографии он пока недостижим. Тем не менее, спор про \\\"артефакты\\\" на этом можно смело закрыть. Кстати, как оказалось, в предыдущей статье я непреднамеренно допустил ошибку. Тогда я думал что снимок был сделан с максимальным увеличением, а оказалось, что это было всего 10х! Просто забыл к моменту публикации какое было увеличение, и сам не поверил что хватит такого \\\"либерального\\\" зума, доступного сегодня почти каждому. Из-за этого и разница в масштабе между старым и новым снимком. Если с Юпитером разобрались, самое время испытать всю мощь мобильных технологий и сфотографировать кольца Сатурна! Как показала практика, в отличие от спутников Юпитера, которые из-за яркости родительской планеты можно рассмотреть только с относительно длинной выдержкой, для съемки колец Сатурна нужно ставить выдержку на порядок короче, иначе получается просто яркое пятно. Деталей тут не хватает, но для человека знакомого с астрономией всё вполне узнаваемо. Снова сравниваем с программой Stellarium. Конечно, кто-то скажет что ему не видно промежутка между планетой и кольцами, или что всё это один большой артефакт. Чтоб не тратить время на споры, оставляю для всех желающих оригиналы фотографий и Юпитера и Сатурна по  А вот так выглядит Сатурн и Юпитер уже с использованием простенького телескопа за 20к руб (МАК 90 SP OTA). Снимки сложены из 200 кадров (видеозапись) с помощью программы AutoStakkert. Очень интересно чтобы кто-то из читателей попробовал провести собственный эксперимент и поделился результатами. Я же в дальнейшем попробую снять какие-нибудь объекты глубокого космоса. UPD: Сначала я написал что Ио, к сожалению, не получится никак снять, но как оказалось уже на снимках от 09.10.2021 Ио есть. Пруф. Такие дела.",
  "tag": "астрономия",
  "hub": 4,
  "author": 1,
  "add_datatime": "2021-10-15T06:19:51.103Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 18,
 "fields": {
  "name": "История ЖК-дисплеев с активной матрицей",
  "image": "1634278924.422863.jpeg",
  "text": "1962 В этом году появился первый тонкоплёночный транзистор (thin-film transistor, TFT), — изобретателем, получившим множество патентов, связанных с технологией электронно-лучевых трубок. Его работа, вдохновлённая предыдущими инновациями, стала фундаментом, приведшим к созданию современной технологии производства дисплеев. RCA использовала изобретение в качестве основы технологии создания дисплеев на жидких кристаллах, которую в дальнейшем усовершенствовал её конкурент, компания Westinghouse. Изобретение ЖК-дисплея с активной матрицей как образец истории изобретателей В истории электроники не было сюжета прекрасней, чем рассказ об изобретателе (или группе изобретателей), разработавшем что-то великолепное, компания которого отказалась от его проекта из опасений, что оно не соответствует её потребностям.Вот несколько таких историй, ставших известными: , новатор в истории штрихкода, над разработкой устройств для железнодорожных вагонов, но в конечном итоге компания отказалась от его идеи, поэтому он решил двигаться самостоятельно и добился огромного успеха. , один из первых примеров графического интерфейса пользователя, игнорировался компанией Xerox до начала 1980-х, когда один из посетителей Xerox PARC, руководитель Apple Стив Джобс, для Apple Lisa и Macintosh. самостоятельно разработала множество базовых концепций цифровой камеры, но изобретателю Стиву Сассону сначала , и только потом Kodak с запозданием начала использовать устройство, изобретённое сотрудником компании. Наша история будет похожей, только речь в ней идёт о том самом экране, на который, скорее всего, вы сейчас смотрите, особенно если он изготовлен по технологии ЖК-дисплеев. В 1970-х годах пара инженеров Westinghouse, Питер Броди и Фан Чэнь Ло, разработали первый ЖК-экран на активной матрице. Родившийся в Венгрии Броди заинтересовался новой экспериментальной технологией тонкоплёночных транзисторов, считавшейся потенциальным способом визуального отображения содержимого в более компактном, нежели ЭЛТ, виде. В заявке на патент изобретатели подчеркнули, что технология реализуема, но требует другого технического базиса вместо кремния, который обычно используется в транзисторах. «Уже очевидно, что твёрдотельные плоскопанельные дисплеи концептуально реализуемы», — . «Попытки использования для этого кремниевой технологии ограничены размером кремниевых пластин, что не позволяет создавать дисплеи большой площади». Поэтому вместо кремния авторы использовали тонкоплёночные транзисторы на стеклянной подложке, что позволило устройству быть прочным, но более тонким, и в то же время пропускать свет. Тонкая плёнка крепилась на слое изолятора с электродом, пропускающим напряжение по экрану. Устройство площадью около сорока квадратных сантиметров могло отображать объекты с разрешением 20 строк на дюйм. (Для сравнения: MacBook Air имеет разрешение примерно 227 строк на дюйм.) Сегодня увидеть отдельные транзисторы на экране довольно сложно без, допустим, микроскопа, но в 1970-х это было очень легко, поэтому , то описал его как «похожий на бумагу-миллиметровку паттерн, имеющий 14400 точек пересечения». Хотя разработчики признавали, что устройство было довольно грубым, а «разрешение позволяло отображать только силуэты букв, чисел и простых изображений», оно продемонстрировало потенциал плоских экранов, которые однажды заменят громоздкие ЭЛТ-дисплеи. В статье Броди сказал, что его скромное устройство является «вероятно, самой крупной в мире интегральной схемой», а не просто экраном. Как указано в заявке на патент, это был не единственный тип тонкого экрана — например, существовала плазменная технология, получившая популярность в телевизорах в начале 2000-х; на её основе были созданы терминалы , известные своим оранжевым оттенком изображения. Но это стало только отправной точкой технологии, которая осталась с нами. К середине 1990-х цветные дисплеи с активной матрицей стали привычными для ноутбуков благодаря сочетанию ярких цветов и малой толщины. Однако несмотря на то, что концепция была придумана в отделе исследований и разработок американской компании и совершенствовалась другими компаниями, почти все панели даже на самом рассвете их популярности производились японскими изготовителями. В чём же заключалась проблема? Разработанная Броди и Ло технология так и не получила развития в Westinghouse; частично это было вызвано тем, что компания постепенно уходила с рынка телевизоров, потому что столкнулась на нём со сложностями. , из-за быстрого развития ноутбуков с цветным экраном на компьютерном рынке Westinghouse в начале 1970-х прекратила продавать телевизоры и закрыла исследовательский отдел компании, позволивший Броди и его команде разработать устройство. На самом деле, эксперименты Westinghouse с плоскопанельными ЖК-дисплеями завершились в 1970-х; то же самое произошло и с другими крупными американскими компаниями. «И крупные корпорации, и стартапы с венчурным капиталом уходили из этой области, обычно это было вызвано производственными сложностями», — писали Ричард Флорида и Дэвид Броуди. Наблюдатели из Westinghouse, дававшие интервью Time, сказали, что технология была отличной, но разработчики часто пропускали дедлайны; Уильям Коутс, работавший в отделе потребительской электроники, сообщил, что в результате это оттолкнуло компанию от использования инновационной технологии. «Мы постоянно не укладывались в графики и в бюджеты», — говорит он. Из этого можно извлечь такой урок: если кто-то не справляется с управлением, но у него есть хорошая идея, то найдите ему менеджера получше. 180 Такое количество миллисекунд требуется для обновления экрана на пассивной матрице; для сравнения: , в то время экрану на активной матрице требовалось от 15 до 30 миллисекунд. На тот момент в ноутбуках постепенно набирали популярность дисплеи с пассивной матрицей, потому что низкокачественные экраны значительно снижали цену ноутбуков, стоивших тогда как подержанный автомобиль. Однако в статье утверждалось, что успех экранов с пассивной матрицей продлится недолго. «Даже самые упорные сторонники технологии цветных дисплеев с пассивной матрицей признают, что будущее цвета в портативных устройствах скорее всего будет связано с активной матрицей», — . «Как только масштабы производства TFT-дисплеев с активной матрицей станут выше, цены неминуемо начнут снижаться». ЖК-панели в основном производились в Азии из-за нежелания крупных технологических компаний инвестировать в них Изучая рост популярности ЖК-экранов с активной матрицей, я поразился схожести тенденций между ЖК и eInk. Часто электронные чернила становились решением в поисках задачи, которому не хватало инвестиций, чтобы попасть на мейнстримный рынок вне рынка электронных книг, на котором они медленно совершенствовались в течение многих лет. Но для популярности eInk недоставало возможности отображения цветов, несмотря на множество попыток, например, ; из-за этого им не удавалось привлечь внимание производителей, несмотря на серьёзные инвестиции крупных компаний. С другой стороны, проблема ЖК-дисплеев с активной матрицей заключалась не столько в отсутствии интереса к продукту, сколько в нежелании больших компаний вкладываться в него. В частности, это отразилось и в том, чем занялся Броди, когда Westinghouse навсегда отказалась от его разработок. Броди основал собственную компанию Panelvision, пытаясь развивать и поставить на коммерческие рельсы технологию активных матриц, которую в то время старались разрабатывать и другие компании. Технология активной матрицы имела ключевое преимущество перед многими другими типами дисплейных технологий, использовавшихся в то время в компьютерных экранах — широкие углы обзора. Низкокачественные ЖК-дисплеи, например, те, которые использовали технологии пассивной матрицы, сталкивались с проблемами низкого качества освещения и размытия, и их нельзя было использовать на улице. «При увеличении количества строк возникает всё больше сложностей с адресацией каждого элемента, между ними возникает взаимное влияние», — . «Другими словами, для активации ЖК-элементов нужно подать на строку достаточно сильный заряд, но не такой сильный, чтобы активировались соседние пиксели». В статье Броди совершенно верно предсказывает, что при увеличении масштабов производства рынок ЖК-экранов будет становиться всё менее дорогим. Но существовала проблема — в конечном итоге, крупномасштабной разработкой этих технологий стала заниматься не компания Броди. Вскоре после интервью его компания была продана, а сам он покинул её, и столкнулся с ещё большими сложностями поиска лиц, заинтересованных в его новой компании Magnascreen. Частично это было вызвано тем, что появились мировые конкуренты, внедрявшие более мощные инновации. Например, Matsushita (теперь называющаяся Panasonic) и Hitachi в 1980-х ; кульминацией их исследований стала разработка в 1990-х технологии (IPS). Панели IPS используются в ноутбуках даже сегодня. Но существовали и более обширные культурные проблемы, нанёсшие ущерб американским производителям TFT-дисплеев: , в процессе поиска инвестиций Броди столкнулся со множеством препятствий, потому что технологические компании хотели видеть в Panelvision поставщика, способного создать технологию для их устройств; они не хотели сложностей с инвестициями в разработку самой технологии. (Мешало и то, что Panelvision находилась в Питтсбурге, который из Кремниевой долины казался дальше, чем Япония.) Эта проблема достаточно широко распространена — , многие исследовательские работы проводятся в западных странах, но производства в них не так много. «Некоторые американские и европейские компании активно участвуют в исследованиях и разработках, внося большой вклад в понимание физики устройства и технологии процессов», — объясняет автор Юэ Ко. «Однако они построили очень мало заводов для крупномасштабного производства». Частично это было вызвано тем, что создать качественный ЖК-дисплей было сложно (позже с подобными сложностями столкнулись и производители цветных eInk-дисплеев). Однако японские компании без сомнений шли на подобные инвестиции, и в результате прежнее поколение крупных технологических компаний попросту уступила фундаментальную технологию другой части мира. Флорида и Броуди пишут: Разумеется, изобретённые в одной стране технологии не обязаны в ней оставаться. На самом деле, глобализация чаще всего является благом, потому что её преимущества помогают всем. Но странно, что потенциал этой фундаментальной технологии, которую вы скорее всего используете для чтения этой статьи, был, по сути, отвергнут целой страной из-за нежелания инвестировать в производство. 1987 В этом году двое исследователей из Eastman Kodak, Чин Тан и Стивен Ван Слайк, (organic light-emitting diode, OLED), в котором использовались два слоя тонких органических компонентов для того, чтобы дисплей мог генерировать свет на уровне пикселей, а не благодаря подсветке. Эта технология, разработанная на основе инноваций, созданных десятки лет назад в таких организациях, как RCA, а позже усовершенствованных для обеспечения поддержки полноцветных экранов, стала ключевым элементом современных смартфонов и телевизоров верхнего ценового сегмента. (И в отличие от разработчиков ЖК-технологии с активной матрицей, Kodak с японской компанией Sanyo, однако .) Нежелание инвестировать в фабрики и производство помогло американским компаниям избежать естественного риска использования непроверенной технологии. Но в то же время это дало отдельной части мира контроль над процессом производства важнейших компонентов. И это означает, что если возникнут проблемы ( чипов для дисплеев), такой контроль сделает нас более подверженными риску. Очевидно, что я не хочу сказать, что люди, принимающие решения об инвестициях, думают именно так — в первую очередь они думают о собственных нуждах, а не о рынке в целом. Но это заставляет задуматься, как бы выглядела отрасль технологий, если бы её важнейший компонент не был так быстро отдан в руки единственной части мира. Вероятнее всего, мир выиграл бы от того, если бы дисплейные технологии разрабатывались и совершенствовались в разных местах. По крайней мере, одно можно считать истинным — как справедливо предсказал Питер Броди сорок лет назад о своём уходе из Westinghouse: «Электронно-лучевая трубка, подобно динозаврам, скоро вымрет, и причина этого будет такой же: слишком большая масса и слишком маленький мозг». В этом он был абсолютно прав, и он оказался значительно прозорливее, чем считали его работодатели и инвесторы. Почему они не видели того, что видел он?",
  "tag": "история ИТ",
  "hub": 4,
  "author": 1,
  "add_datatime": "2021-10-15T06:22:04.423Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 19,
 "fields": {
  "name": "Наш мозг не компьютер",
  "image": "",
  "text": "Как только не объяснял человек природу своего сознания и мышления на протяжении истории, начиная с библейской легенды о «сотворении тела из глины и заселении его духом» и заканчивая довлеющей на сегодня парадигмой «наш мозг — это обработчик информации, подобный компьютеру». И если все прошлые аналогии уже практически исчерпали веру в себя, то с последней вопрос стоит остро, ведь многие специалисты по нейробиологии не согласны и считают ее в корне ошибочной.  Как бы ни старались ученые-исследователи головного мозга и когнитивные психологи, им никогда не найти в человеческом мозге копию 5-й симфонии Бетховена – равно как копии слов, изображений, грамматических правил или любых других видов экзогенных стимулов. Бесспорно, человеческий мозг устроен сложно, но он не содержит большую часть из того, что, как многим кажется, в нем есть – даже таких простых вещей как «воспоминания».   Наше ошибочное понимание головного мозга уходит глубоко в историю, но с момента изобретения электронных вычислительных машин в 40-е годы все стало еще более запутанным. Уже более половины столетия психологи, лингвисты, нейроученые и другие эксперты по человеческому поведению утверждают, что наш мозг работает подобно компьютеру.  Чтобы понять, насколько бессмысленна эта идея, рассмотрим работу мозга младенцев. Вследствие эволюционного развития человек, как и все другие виды млекопитающих, приходит в мир подготовленным к эффективному взаимодействию с ним. Зрение младенца хоть и размыто, но в нем особое внимание выделяется лицам, среди которых он способен быстро распознать свою мать. Кроме того, новорожденный уделяет больше внимания голосам, нежели прочим звукам, и может различать базовые речевые единицы. Мы без всякого сомнения созданы для социального взаимодействия.   Здоровый младенец также изначально обеспечен набором рефлексов – готовыми реакциями на конкретные стимулы, необходимыми для его выживания. Он поворачивает голову в направлении того, что касается его щеки и начинает сосать то, что попадает ему в рот. При погружении в воду он задерживает дыхание. Если положить что-либо в его ручки, он схватывает это так сильно, что практически может выдержать собственный вес.   Самое же важное в том, что новорожденные изначально снабжены мощными механизмами обучения, которые позволяют им быстро меняться для наиболее эффективного взаимодействия с окружающим миром, даже если этот мир сильно отличается от того, в котором проживали их далекие предки.   Органы чувств, рефлексы и механизмы обучения – вот наш стартовый комплект. И если задуматься, то это очень много. Будь мы при рождении лишены любой из этих способностей, то с выживанием бы у нас возникли серьезные трудности.  А вот чего у нас от рождения нет: информации, данных, правил, программного обеспечения, знаний, лексического запаса, представлений, алгоритмов, моделей, воспоминаний, процессоров, подзадач, декодеров, символов или буферов – элементов дизайна, позволяющих компьютерам демонстрировать околоразумное поведение. Причем мы не просто не рождаемся со всеми этими вещами, но и не вырабатываем их – никогда.  Мы не храним слова или правила, которые указывают нам, как этими словами оперировать. Мы не создаем представления визуальных стимулов, не сохраняем их в буфере краткосрочной памяти, и не переносим затем эти представления в устройство долгосрочного хранения. Мы не извлекаем информацию, образы или слова из регистров памяти. Компьютеры делают все это, но не живые организмы.  Компьютеры в прямом смысле обрабатывают информацию – числа, буквы, слова, формулы, изображения. Сначала эту информацию необходимо закодировать в пригодный для использования компьютерами формат, то есть в паттерны из нулей и единиц (биты), организованные в небольшие фрагменты (байты). На моем ПК каждый байт содержит 8 бит, и определенный паттерн этих битов означает букву  d , другой букву  o , еще один букву  g . Бок-о-бок эти три байта формируют слово  dog .   Одно простое изображение – скажем, фотография моего кота Генри на рабочем столе – представлен особым паттерном из миллиона таких байтов (одним мегабайтом), окруженным специальными символами, которые указывают компьютеру, что нужно ожидать изображение, а не слово.  Компьютеры в прямом смысле перемещают эти паттерны с места на место в различных областях физических хранилищ, вытравленных в электронных компонентах. Иногда они также копируют эти паттерны, а иногда различным образом преобразуют их – например, когда мы исправляем ошибки в документе или ретушируем фотографию. Правила, которыми компьютеры руководствуются для перемещения, копирования и работы с этими массивами данных, также хранятся в самих компьютерах. В комплексе такой набор правил называется «программой» или «алгоритмом».   Совокупные группы алгоритмов, реализующие нужные нам действия – покупку акций на бирже или назначение свидания через сайт знакомств – называются «приложением».  Прошу прощения за это базовое введение во всем известные компьютерные принципы, но мне хочется ясно выразить мысль: компьютеры действительно работают с символьными представлениями слова. Они действительно сохраняют и извлекают. Они на самом деле обрабатывают. У них на самом деле есть физические воспоминания. Все их действия, без исключения, реально управляются алгоритмами.  Если же говорить о людях, то они этого не делают – никогда не делали и никогда не будут. Почему же так много ученых умов рассуждают о нашей ментальной жизни, сравнивая нас с компьютерами?   В своей книге «In Our Own Image» (2015) эксперт по искусственному интеллекту Джордж Заркадакис описывает шесть различных аналогий, которые люди придумывали в течение последних 2,000 лет в попытке описать человеческий разум.  Первая из них, сохраненная в Библии, гласит, что люди были сотворены из глины или грязи, которую затем разумный Бог населил духом. Этот дух и объяснял нашу разумность.  Появление гидравлической инженерии в третьем веке до н.э. привело к популяризации гидравлической модели человеческого разума. Ее идея заключалась в том, что за наше физическое и ментальное функционирование отвечает поток различных телесных жидкостей. Гидравлическая метафора довлела на протяжении более 1,600 лет и все это время препятствовала медицинской практике.   К 16-му веку была выработана модель на основе пружин и шестеренок, в результате чего ведущие мыслители той эпохи, такие как Рене Декарт, утвердили, что человек является сложной машиной. В 17-м веке философ Томас Хоббс предположил, что мышление порождается за счет мелких механических движений в мозге. К 18-му веку открытия в области электричества и химии привели к появлению новых теорий о природе человеческого разума – опять же, по своей сути очень образных. В середине 19-го века, вдохновленный последними достижениями в сфере средств связи, немецкий физик Германн фон Гельмгольц сравнил мозг человека с телеграфом.   Математик Джон фон Нейман прямо заявил, что функция человеческой нервной системы «на первый взгляд цифровая», проведя параллель между компонентами вычислительных машин того времени и компонентами человеческого мозга.  Каждая из приведенных аналогий отражала передовое мышление своей эпохи. Вполне предсказуемо, что спустя всего несколько лет после восхода компьютерных технологий в 40-х годах 20-го столетия работу мозга сравнили с компьютером. В этой модели роль аппаратного обеспечения выполнял сам мозг, а мысли служили его программным наполнением.   Знаковым событием, положившим начало того, что теперь именуется «когнитивной наукой», стала публикация психолога Джорджа Миллера «Language and Communication» (1951). Миллер предположил, что ментальный мир можно тщательно изучить с помощью принципов из теории информации, вычислительной науки и лингвистики.  Пика своей выразительности эта модель достигла в небольшой книге «The Computer and the Brain» (1958), в которой математик Джон фон Нейман заявил, что функция человеческой нервной системы «prima facie цифровая». Нейман признавал, что фактически о роли мозга в функционировании мышления и памяти известно мало, но все же одну за другой проводил параллели между его компонентами и компонентами вычислительных машин того времени.  Движимое последующими достижениями в компьютерных технологиях и исследованиях мозга, амбициозное междисциплинарное стремление понять природу человеческого разума развилось и утвердилось в виде идеи, что люди, подобно компьютерам, являются обработчиками информации.   Сегодня же этой идее привержены тысячи исследователей, она поглощает миллиарды долларов финансирования и уже породила множество литературы, включая как технические, так и массовые статьи и книги.   К примеру, в своей книге «How to create a mind: The Secret of Human Thought Revealed» (2013) Рэй Курцвейл иллюстрирует эту модель, рассуждая на тему «алгоритмов» мозга, а также его принципов «обработки данных», и даже отмечает его внешнее структурное сходство с интегральными схемами.  Сегодня же это представление, что человеческий разум является просто обработчиком информации, доминирует и в быту, и в научных кругах. Оно всплывает практически во всех дискурсах на тему разумного человеческого поведения, также как в прошлые эпохи подобные дискурсы на проходили без отсылки к духу или божественности. Достоверность данной модели в современном мире, как правило, принимается без оспаривания.   Но эта аналогия, в конце концов, является просто очередной аналогией – историей, которую мы рассказываем друг-другу, чтобы придать смысл тому, чего на самом деле не понимаем. Так что, как это было со всеми предшествовавшими ей сравнительными моделями, однажды на ее смену либо придет другая, либо воцарится истинное знание.   Где-то год назад во время визита в один очень престижный исследовательский институт мирового уровня я призвал местных ученых описать разумность человеческого поведения без какой-либо отсылки к аспектам «обработки информации». Они не смогли. А когда я вежливо вернулся к этому вопросу в ходе последующего электронного общения, то даже спустя несколько месяцев им все еще было нечего сказать. Они видели проблему. Они не отбрасывали этот вызов как несущественный. Но при этом и не могли предложить альтернативу. Говоря иначе, аналогия об «обработке информации» очень «назойлива». Она загромождает наше мышление языком и идеями, настолько сильными, что рассуждать в их обход уже не получается.  Причем определить ложность, стоящую за аналогией с обработкой информации, достаточно просто. Она основана на ошибочном силлогизме, в котором из двух разумных предпосылок делается ложный вывод. Разумная предпосылка #1: все компьютеры способны к разумному поведению. Разумная предпосылка #2: все компьютеры являются обработчиками информации. Ложный вывод: все сущности, способные к разумному поведению, являются обработчиками информации.  Если отбросить формальный язык, то идея, что люди должны являться обработчиками информации только потому, что ими являются компьютеры, просто глупа. И когда однажды от этой аналогии все же откажутся, для историков она будет однозначно выглядеть именно таковой, ровно как сейчас для нас выглядит глупым сравнение людей с гидравлическими машинами.   Но раз эта метафора столь глупа, почему же она так назойлива? Что мешает ее отклонить, подобно ветви, преградившей наш путь? Есть ли способ понять человеческий разум, не опираясь на шаткий костыль интеллекта? И какую мы уже успели заплатить цену за столь долгое использование конкретно этого костыля? Им руководствовались многие ученые умы и мыслители в разных областях науки в течение десятилетий. Какой ценой?   Уже не первый год я практикую в классах любопытное упражнение, приглашая к доске студента с просьбой нарисовать детальное изображение долларовой купюры – максимально детальное. Когда студент заканчивает, я закрываю это изображение листом бумаги, достаю соответствующую купюру из бумажника, креплю ее на доске и прошу студента повторить процесс уже по образцу. Когда он в очередной раз завершает рисование, я раскрываю первый рисунок и прошу аудиторию прокомментировать отличия.   Предположив, что подобную демонстрацию вы могли никогда не видеть, а также для того, чтобы помочь представить ее результат, я попросил Джинни Хён, одну из моих интернов в институте, где провожу исследования, нарисовать два таких изображения. Вот ее рисунок «из памяти»:   А вот что она нарисовала следом по образцу купюры:    Джинни, как наверняка и вы, была удивлена результатом, но это типичная ситуация. Как видите, рисунок из памяти совсем никчемен в сравнении с его альтернативой, полученной по образцу несмотря на то, что Джинни видела эту купюру тысячи раз.   В чем же проблема? Разве у нас в «регистре памяти» не хранится «представление» этой денежной единицы? Разве мы не можем просто «извлечь» ее и использовать для рисования? Очевидно, нет. И даже за тысячу лет нейроисследований мы не сможем найти в человеческом мозге сохраненного представления долларовой купюры по одной простой причине – его там нет.   Многие научные труды по исследованию головного мозга сообщают нам, что даже в наиболее обыденных, связанных с памятью задачах задействуется  по несколько областей мозга . В случаях, когда человек подвержен сильным эмоциям, повышается активность миллионов нейронов.  В 2016 году группа специалистов из Университета Торонто под руководством нейропсихолога Брайана Левина провела  исследование  выживших после авиакатастрофы людей.   В результате специалисты выяснили, что воспоминания пассажиров о крушении вызывали повышенную активность в «миндалевидном теле, медиальной височной доле, переднем и заднем отделе средней части мозга, а также в зрительной коре».  Определенный круг ученых утверждает идею, что конкретные воспоминания неким образом сохраняются в отдельных нейронах. Но это абсурд, поскольку подобное утверждение, наоборот, переносит проблему с пониманием памяти на еще более сложный уровень: как и где тогда сохраняются воспоминания в клетке?  Так что же происходит, когда Джинни рисует изображение доллара при его отсутствии? Если бы она его раньше не видела, то ее первый рисунок наверняка бы даже не был похож на второе изображение. Тем не менее то, что она уже видела эту купюру ранее, некоторым образом изменило Дженни. В частности, ее мозг изменился так, что позволил ей визуализировать изображение доллара – то есть воспроизводить опыт его наблюдения, по крайней мере, до определенной степени.   Разница между двумя полученными изображениями напоминает нам, что визуализация чего-либо намного менее точна, чем прямое видение объекта. Именно поэтому мы гораздо лучше справляемся с узнаванием, чем со вспоминанием. Когда мы что-либо вспоминаем, то стараемся перепрожить некий опыт. Но если мы что-либо узнаем, то нам требуется просто определить сам факт проживания в прошлом этого опыта.   Вы можете не согласиться с предложенной демонстрацией. Джинни видела доллары ранее, но не предпринимала намеренного усилия по «запоминанию» деталей этих купюр. Можно заявить, что если бы она старалась это сделать, то наверняка нарисовала бы второе изображение и в отсутствии образца.  Но даже в таком случае изображение доллара не сохраняется в голове Джинни в каком бы то ни было смысле. Она просто стала более подготовлена к его точному воспроизведению аналогично тому, как пианист с помощью практики становится более подготовлен к выступлению на концерте, не прибегая к «проглатыванию» копии партитуры.  Приведенное мной упражнение может стать основой для построения свободной от аналогий теории о разумности человеческого поведения.   В ходе своей жизни и взаимодействия с миром мы постоянно изменяемся под воздействием различных опытов, среди которых стоит выделить три основных:        Человек преуспевает в жизни, если меняется сообразно этим опытам – если выучивает на память стих или песню, если вырабатывает навык следовать инструкциям, если правильно реагирует на второстепенные и первостепенные стимулы, если избегает поведения, за которое наказывают, и чаще ведет себя так, чтобы его награждали.   Несмотря на вводящие в заблуждение утверждения, никто в действительности не знает, как именно меняется наш мозг после заучивания песни или стишка. Но однозначно ничто из этого в нем не «сохраняется». Мозг просто претерпевает упорядоченные изменения, открывающие в нас возможность петь песню или читать стих в определенных условиях. Когда дело доходит до действия, ни песня, ни стих ни в каком смысле не «извлекаются» откуда-то из мозга, как не извлекаются мои движения, когда я стучу пальцами по столу. Мы просто поем или читаем – без всякого извлечения.  Несколько лет назад я  спросил  Эрика Кэндела, нейроученого из Университета Колумбии – получившего Нобелевскую премию за обнаружение химических изменений, происходящих в синапсах нейронов морских зайцев после заучивания ими чего-либо – как долго, по его мнению, нам еще предстоит разбираться в принципе работы человеческой памяти. Он, не задумываясь, ответил: «Сто лет». Тогда мне не пришло в голову спросить его о том, не замедляет ли наш прогресс аналогия с обработкой информации, но некоторые нейроученые уже всерьез начинают сомневаться в ее достоверности и непоколебимости.  Ряд когнитивистов – в частности, Энтони Чемеро из Университета Цинцинатти, автор «Radical Embodied Cognitive Science» (2009) – полностью отвергают идею, что человеческий мозг функционирует аналогично компьютеру. Распространенная модель гласит, что мы, подобно компьютерам, составляем картину мира путем выполнения вычислений над его ментальными представлениями. Однако Чемеро и прочие единомышленники описывают другой способ интерпретации разумного поведения – а именно, как прямого взаимодействия между организмами и их миром.  Разительность отличий между аналогией с обработкой информации и тем, что некоторые сейчас называют «непрезентативной теорией функционирования человека», удачно демонстрируется сопоставлением двух способов объяснения процесса ловли бейсболистом летящего мяча.   Это красиво описывает Майкл МакБит со своими коллегами из Университета штата Аризона в работе 1995 года,  опубликованной в журнале «Science» . Модель обработки информации требует, чтобы игрок оценивал различные начальные условия полета меча – силу толчка, угол траектории его полета и все прочее – затем создавал и анализировал внутреннюю модель пути, вдоль которого мяч вероятнее всего будет лететь, после чего использовал эту модель для направления движений и их непрерывной подстройки во времени, чтобы в итоге мяч перехватить.  Все бы было легко и просто, функционируй мы как компьютеры, но МакБит и его коллеги привели простую альтернативу: чтобы поймать мяч, игроку просто нужно продолжать двигаться так, чтобы мяч постоянно оставался видим относительно основной базы и окружающей обстановки (говоря технически, находился на «линейной оптической траектории»). Звучит сложновато, но на деле процесс очень прост и совершенно лишен вычислений, представлений и алгоритмов.  Два увлеченных профессора из Лидского университета Беккета в Великобритании – Эндрю Уилсон и Сабрина Голонка – причисляют пример с бейсболом ко многим другим, которые можно легко и осмысленно рассмотреть вне теории обработки информации. Они уже много лет  ведут блог  о том, что называют «более целостным и естественным подходом к научному изучению человеческого поведения…разнящимся с доминирующим подходом когнитивной нейробиологии».  И все же это далеко не организованное движение. Ведущие идеи когнитивной науки продолжают зиждиться на аналогии об обработке информации, и некоторые из влиятельных мировых мыслителей даже сделали серьезные прогнозы относительно будущего человека, которое зависит от действительности данной аналогии.  Один из прогнозов – высказанный, среди прочих, футуристом Курцвейлом, физиком Стивеном Хокингом и нейроученым Рэндалом Коеном – гласит, что ввиду предположительного сходства человеческого разума с компьютерным ПО наши сознания вскоре можно будет загружать в компьютер, среди электронных цепей которого мы получим безграничное интеллектуальное могущество и, весьма вероятно, бессмертие. На основе этой идеи даже был написан сюжет для дистопичного кинофильма «Превосходство» (2014), где в роли подобного Курцвелу ученого снялся Джонни Депп. Согласно сюжету, разум этого ученого был загружен в интернет, что привело к чудовищным последствиям для всего человечества.  К счастью, поскольку аналогия с обработкой информации не верна даже отчасти, нам никогда не придется переживать о возможном бесчинстве человеческого разума в киберпространстве. Правда, увы, бессмертия посредством загрузки в компьютер мы тоже не получим. Хотя причина не только в отсутствии программ разумности в мозге – есть здесь проблема и поглубже, назовем ее проблемой уникальности, которая вдохновляет и расстраивает одновременно.   Раз в мозге не существует ни «банков памяти», ни «представлений» стимулов, а для функционирования в мире нам требуется лишь его упорядоченное изменение на основе проживаемого опыта, то нет оснований полагать, что один и тот же опыт повлияет на двух людей одинаково. Если мы с вами посетим концерт, где прослушаем, к примеру, 5-ю симфонию Бетховена, то изменения в моем мозге определенно будут отличаться от изменений в вашем. Эти изменения, какими бы они ни были, строятся на уже имеющейся уникальной структуре нейронов, каждая из которых вырабатывается индивидуальным жизненным опытом.  Именно поэтому, как Сэр Фредерик Бартлетт показал в своей книге «Remembering» (1932), никогда два человека не перескажут только что услышанную ими историю одинаково. И поэтому с течением времени их пересказы будут все более и более отличаться. Никакой «копии» истории никогда не создается. Вместо этого каждый человек, услышав эту историю, в некоторой степени меняется – достаточно для того, чтобы при последующей просьбе пересказать ее (иногда спустя дни, месяцы и даже годы, после того, как Бартлетт впервые прочел ему эту историю) – он может до некоторой степени перепрожить опыт ее прослушивания, хотя уже не так хорошо (вспомним картинку доллара на доске).  Полагаю, что это вдохновляет, так как означает, что каждый из нас поистине уникален, причем не только в генах, но даже в происходящих со временем изменениях мозга. Хотя есть здесь и удручающая сторона, ведь перед нейроучеными это ставит чрезвычайно сложную задачу. При проживании любого опыта изменение мозга может затрагивать тысячи нейронов, миллионы и даже всю его структуру, а сам паттерн изменения будет индивидуален для каждого человека.   Все серьезно настолько, что даже будь у нас возможность получить снимок состояния всех 86 миллиардов нейронов, после чего имитировать состояние этих нейронов на компьютере, то подобный обширный шаблон не имел бы никакого значения вне самого тела, его породившего. В этом, пожалуй, и кроется наиболее глубокое заблуждение аналогии с обработкой информации, исказившее наше представление о функционировании человека. Если компьютеры хранят точные копии информации – копии, способные оставаться в неизменном виде длительные промежутки времени, даже при отключении питания – то мозг поддерживает работу интеллекта только пока остается жив. У него нет кнопки вкл/выкл — либо мозг работает, либо мы исчезаем.   Скажу более, как указал нейробиолог Стивен Роуз в книге «The Future of the Brain» (2005), снимок текущего состояния человеческого мозга окажется бессмысленным, если не знать всю историю жизни его владельца – в том числе детали социального контекста, в котором он вырос.   Представьте себе сложность задачи. Чтобы понять хотя бы базовые принципы поддержания мозгом работы интеллекта, нам может потребоваться знать не только текущее состояние 86 миллиардов нейронов и 100 триллионов их связей, не только различия в силе этих связей и не только состояния более, чем 1,000 белков, существующих в каждой точки сопряжения, но и то, какую роль играет ежемоментная активность мозга во всей целостной системе.   А теперь прибавьте сюда индивидуальность каждого мозга, отчасти обусловленную уникальностью истории жизни каждого человека, и тогда прогнозы Кэндела покажутся чрезвычайно оптимистичными. (Нейроученый Кеннет Миллер в своей авторской колонке журнала «The New York Times» предположил, что науке потребуются столетия, только чтобы разобраться в базовых принципах организации нейронных связей).  Тем временем огромные средства выделяются на исследования мозга, зачастую основанные на ложных идеях и обещаниях. Самый скандальный случай провала эксперимента в области нейронауки был изложен в докладе,  опубликованном в журнале «Scientific American» , где речь идет о проекте «Human Brain» с бюджетом в $1.3 миллиарда, запущенном Евросоюзом в 2013 году.  Проект основал израильский харизматичный нейробиолог Генри Маркрам, которому удалось убедить членов Евросоюза в том, что к 2023 году он сможет создать компьютерную модель всего головного мозга человека. По его словам, эта модель должна была произвести революцию в области лечения болезни Альцгеймера и других психических недугов. Маркрам получил полное одобрение своей программы практически без ограничений, а также $1.3 миллиарда поддержки. Однако не прошло и двух лет, как проект зашел в тупик, и Маркрама попросили отступиться.   Мы организмы, не компьютеры, и это необходимо принять. Лучше сосредоточиться на попытках понять себя, но не нагромождать это понимание излишним интеллектуальным багажом. Аналогия с обработкой информации существует уже пол века, но за все это время практически не внесла полезных открытий. Не пора ли уже нажать  DELETE ?",
  "tag": "Мозг",
  "hub": 4,
  "author": 1,
  "add_datatime": "2021-10-15T06:24:05.264Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 20,
 "fields": {
  "name": "Легко ли собрать выжигатель мозгов?",
  "image": "1634279170.274157.jpeg",
  "text": "Несмотря на то, что почти вся статья будет посвящена электромагнитным полям, мы начнем издалека, с такого феномена как температура. Температура играет ключевую роль в биологических процессах, определяя как скорость их протекания, так и вовсе саму возможность их существования. Именно её будет измерять доктор, чтобы быстро оценить состояние вашего здоровья. Ведь повышенная температура тела — это естественная защитная реакция организма на болезнь, а результаты её измерения могут служить в качестве простейшего двоичного индикатора болен/не болен. На самом деле диагностические возможности этим не ограничиваются, и температура может рассказать о состоянии здоровья человека намного больше. Например, собрав достаточно данных с поверхности тела, возможно выявить даже специфичные заболевания, вроде рака груди : Понимание того, что температура работает и в обратную сторону - для лечения пришло к людям очень давно, благо уж само тело человека подсказывало копать в этом направлении. Одним из пионеров в области лечения путём манипуляций с температурой заслуженно считается арабский учёный и врач Абу-ль-Касим аз-Захрави. Около тысячи лет назад он одним из первых решил, что неплохо было бы попробовать задействовать в медицине повышенную температуру, и чтобы не мелочиться, сразу взял раскалённый металл: Сегодня такие методы лечения кажутся варварскими, однако на тот момент это был самый топ приёмов медицины, которые могли помочь порой в самых безнадежных случаях. Абу-ль-Касим, будучи человеком учёным не просто тыкал в людей горячей кочергой, но еще и проводил исследования влияния типа металла, температуры, формы прижигающих элементов, а также погодных условий на результаты лечения, пытаясь при этом выделить наиболее важные факторы для повышения результативности терапии. Но прогресс не стоял на месте и вот уже Луиджи Гальвани со своей женой показывают пораженным зрителям как дергаются лапки мертвой лягушки под воздействием протекающего через них электрического тока: Температура тут на первый взгляд, казалось бы, ни при чём. Но после открытия настолько живительного действия электричества его не преминули использовать где только можно, и в первую очередь конечно же в медицине. Со времен аз-Захрави менять формат методов особо не стали и взамен прижиганию ( ) изобрели Гальванокаустику ( ). Суть процедуры можно описать точно такой же картинкой: Да-да, постоянный электрический ток использовался для совершения всё той же процедуры резкого локального увеличения температуры, проще говоря прижигания. Несмотря на, казалось бы, очевидное сходство двух этих методов испепеления болячек, электричество всё же показало себя более гибким инструментом в плане контроля процедуры и её безопасности. Если раскалённая кочерга прижигает только поверхностные регионы, ток уже способен проникать вглубь тканей, нагревать их объемно, и кроме того, может быть легко отрегулирован или же быстро отключён в случае необходимости. Гальванокаустикой вполне успешно, хоть и всё ещё весьма болезненно лечили новообразования, аневризмы и даже грибок. Постепенно она переросла в то что сегодня известно под названием электрохирургия, но мы эту ветку эволюции технологий пока пропустим, ведь всякие поля гораздо интереснее. Как только появились первые генераторы, способные вырабатывать переменный ток, обнаружился интересный факт: всё те же опыты с лягушачьими частями показывали, что при достижении определенной частоты генератора или выше её (порог был определён в районе 10 кГц) лапки переставали дёргаться, хотя ток при этом всё ещё как-то тёк по цепи. И здесь в нашей истории появляются такая неоднозначная персона как Арсен Д'Арсонваль [ ]. Д'Арсонваль исследовал влияние низкочастотных токов на тело человека, в том числе, пропуская их через себя любимого. Он заметил, что несмотря на то, что протекание тока никак не ощущается, через некоторое время кожа начинает потеть, а кровяное давление растёт, как будто бы повышается температура тела. Он пошёл дальше и построил ряд устройств для воспроизведения этого эффекта, наиболее интересными из которых являются катушки и кушетки для людей и животных. Примечательны они тем, что взаимодействие с пациентом в этих установках впервые происходило без прямого подключения и только за счет полей. Это вызывало благоговейный трепет у широкой публики, которая уже тогда была неравнодушна ко всему беспроводному и непонятному. Однако Арсен был не единственный, кто увлекался подобными интересными вещами. Ведь все мы знаем, что как только речь заходит об электричестве - там обязательно появляется он.  Как и его заокеанский коллега, Никола Тесла также был замечен в экспериментах с применением электричества в медицине. И также как и Д'Арсонваль, был прямым виновником того, что в газетах того времени низкочастотным токам и полям приписывались свойства новой панацеи, способной излечивать что угодно вплоть до туберкулёза. Основания для этого Тесла почерпнул из своих экспериментов, когда обнаружил что микробы (как и туберкулёзные палочки) погибают при пропускании через них тока (ну а кто б не погиб). Такую мелочь как невозможность повторить то же самое внутри легких пациентов при этом даже не рассматривали и, что закономерно, короткие клинические испытания полностью провалились [ ]. Тесла вообще был великий затейник, и кроме электричества предлагал лечиться ещё и вибраторами (нет, не такими) и даже тестировал их на Марке Твене [ ]. Стоит отметить, что интересной чертой, которая словно нить связывает вместе страницы истории любого вида терапии, является желание людей получить на основе каких-либо физических явлений простой в эксплуатации механизм   совершенствования или излечения человеческого тела. Будь то  Кузнецова, какой-нибудь там \\\"Алмаг\\\" или укус радиоактивного паука, люди обязательно ждут что это будет непременно полезно для здоровья. А особенно когда речь идёт о вещах интуитивно непонятных, вроде электромагнитных полей. Потому с поправкой на время и человеческую психологию, надо учитывать, что никто из пионеров электротерапии не особо вникал в механизм действия предлагаемых ими лечебных процедур. Суть исследований того времени сводилась к тому, чтобы найти желающих, посадить на какую-нибудь \\\"конденсаторную кушетку\\\", сунуть в руку электрод, включить генератор и посмотреть, что будет. По желанию потом можно собрать небольшую статистику и сделать на её основе очень далеко идущие выводы. Данная методика, разумеется, не вызывает ни малейших вопросов у любого здравомыслящего человека, а потому даже сегодня практически в любой аптеке можно купить так называемые «аппараты Д'Арсонваля», с всё таким же сомнительным функционалом как и много лет тому назад. Короче говоря, в итоге так сложилось, что если Тесла стал идолом каких-нибудь эфиропоклонников, то Д'Арсонваль - превратился в лейбл шарлатанов от медицины. Но вернёмся обратно в прошлое.  Поколения учёных, гонимых жаждой открытий и взращённых на желании громкой славы их предшественников, ещё сильнее вгрызлись зубами в гранит науки в поисках влияния электромагнитных полей на бренное человеческое тело. Но всё, что удавалось наблюдать в экспериментах, раз за разом оказывались либо прямым, либо косвенным результатом действия растущей температуры. А громкие заявления персон, утверждавших об открытии новых неизвестных механизмов к 30-м годам всех уже порядком подзадолбали, и научный мир стал требовать солидных доказательств, прежде чем начинать разбираться в очередном последствии неграмотности авторов. В итоге выяснилось, что тело в электрическом плане мало чем отличается от резистора, и как подобает этому электронному компоненту, выделяет тепло, когда через него протекают токи разной природы, в том числе и наведённые внешними низкочастотными полями. Именно нагрев и приводил к повышению давления и потоотделения у пациентов Д'Арсонваля, так как таким способом их организм просто пытался себя охладить. Но учёные – пытливые ребята и просто так их не остановить. Многочисленные попытки найти хоть какие-либо значимые не термальные либо долговременные эффекты, оказываемые на организмы живых существ, продолжались (да и продолжаются по сей день, чего уж там). Причём даже не так важно, негативные эффекты или позитивные – при любом раскладе автор открытия был бы в выигрыше, не от врачей так хоть от военных. Однако никаких чудесных излечений болезней, контроля сознания, лучей смерти и прочих фантастических плюшек обнаружить так и не удавалось. Реальность как обычно оказалась серой и унылой, как панельная девятиэтажка на окраине провинциального города. Однако и то, что было выяснено в процессе изучения всё равно пошло в дело: в итоге работы Д'Арсонваля и Теслы переросли в направление терапевтической медицины, названное диатермия, где тело пациента уже целенаправленно прогревались радиоизлучением. Большой скачок для диатермии дало появление дешёвых и доступных источников высокочастотного излучения - радиоламп и магнетронов. Аппарат Raytheon microterm с картинки выше был первым из одобренных для применения в клинической практике. Внутри металлического корпуса был размещен магнетрон с рабочей частотой 2.45 ГГц (практически как в микроволновке у вас на кухне, но слабее по мощности). По гибкому волноводу микроволновая энергия от магнетрона подавалась внутрь параболического рефлектора, который фокусировал поля в теле пациента для локального прогрева. И обычно таким образом прогревали суставы.      Несмотря на то, что технологии стали в разы лучше со времён Д'Арсонваля и Теслы, дух раздолбайства еще не до конца покинул умы медицинских экспериментаторов. Интересный момент касается того, что рабочая частота первых аппаратов вроде того, что представлен выше, была выбрана практически произвольно. Первая причина — это то, что магнетрон для частоты 2.45 ГГц будет не слишком большим по размеру, а вторая состоит в том, что действующая на тот момент Федеральная Комиссия Связи США (FCC) выделила для медицинского применения только этот диапазон (ну и еще один в районе 27 МГц) исходя из каких-то своих странных предположений. То есть самый ключевой параметр, который надо учитывать при нагреве живого человека полями – рабочую частоту выбрали буквально пальцем в небо, просто потому что. У кухонных микроволновок кстати ровно такая же история. Величина рабочей частоты там тоже скорее исторически сложилась, а не была выведена в результате кропотливых изысканий в секретных лабораториях и поиска пиков поглощения воды, как некоторые ошибочно полагают.  Безусловно, термический эффект полей был уже для всех устоявшимся фактом, однако было мало данных о том, как именно электромагнитные поля фокусируются внутри организма, какой размер нагреваемой области и где она, собственно, будет находится. И тем не менее, это не останавливало никого, процедура пользовалась популярностью. Разумеется, когда военные, бюрократия и технический прогресс дали такую возможность, исследователи взялись проверять различные частоты и уровни мощности электромагнитного излучения, а также что происходит с ними внутри человека для дальнейшего применения сих знаний в медицине. И на этом месте прервём ненадолго урок истории и поговорим немного о физике - убийце чудес.  Посмотрим глубже в процессы взаимодействия электромагнитных полей и человеческого организма с Эвереста наших сегодняшних знаний. Так как частота электромагнитной волны обратно пропорциональна её физическим размерам, а точнее длине, то вполне очевидно, что волны с частотами, например 27 МГц и 5 ГГц, будут взаимодействовать с тканями и органами человека совершенно по-разному: В общих чертах, если говорить о радиодиапазоне (а он огромен: от 0,03 Гц до 3 ТГц, поэтому обобщение очень смелое), то чем выше частоты - тем меньше тело человека похоже на упомянутый резистор и больше на конденсатор. Всё дело в том, что наш организм состоит в основном из диэлектриков. А диэлектрики - это материалы, которые плохо проводят ток в привычном всем понимании, как например металлы, но зато способны накапливать в себе энергию электрического поля за счет смещения зарядов в своих молекулярных структурах. Процесс смещения зарядов под действием внешнего поля, по сути, тоже является током, хоть и другой природы (его зовут ток смещения). Именно благодаря смещению зарядов переменный ток может течь сквозь конденсатор, хотя этот элемент вроде как представляет собой разрыв для электрической цепи: кусок воздуха или какой-нибудь там непроводящей керамики. В человеческом теле внешнее переменное электрическое поле заставляет шевелиться, крутиться и двигаться всякие ионы и полярные молекулы, а также растягивает электронные оболочки. Это приводит к возникновению вторичных полей, перераспределению и рассеиванию энергии, проще говоря к нагреву. Таким образом для высокочастотных полей мы представляем собой не простой конденсатор как в электронике, а скорее его комбинацию с резистором - он хоть и накапливает энергию, но при этом еще и кипятится.     Но конденсатор – элемент как правило маленький, а человек по сравнению с ним довольно большой. Это значит, что некоторые волны, подходящие по размеру, могут вполне помещаться внутри тела или его части целиком. А совсем высокочастотные и даже не один а много раз. Когда такое происходит, и электромагнитная волна находится внутри среды, она всё ещё распространяется, но при этом уменьшает свою скорость (а ещё свои физические размеры). На анимации ниже показан примерный механизм того, как это работает на простой модельке с шариками и пружинами: Для вакуума или воздуха мы легко посчитаем длину волны зная скорость света, но в диэлектриках всё становится сложнее, так как нужно вводить специальную поправку на то самое уменьшение скорости и длины волны. Эту поправку назвали относительной диэлектрической проницаемостью материала и научились её измерять разными способами. Например, пихать кусочек исследуемого материала между обкладками конденсатора и проверять насколько изменилась его ёмкость. Соответственно проницаемость является относительной по отношению к таковой у вакуума, что вполне логично, ведь разных материалов много, а вакуум - один. Если примерять эту величину для умозрительной модельки с шариками с картинки выше, то она будет отвечать за жесткость и длину пружин: чем больше будет способность гипотетических пружин растягиваться и сжиматься - тем выше диэлектрическая проницаемость материала и тем дольше по времени волна будет проходить сквозь него.  Но как уже было сказано, прохождение волны сквозь диэлектрик связано с потерями энергии и соответственно сопровождается нагревом последнего. Пружинная модель весьма показательна даже в этом плане, так как те же пружинки при колебаниях тоже нагреваются, хоть и совсем чуть-чуть. Зная это, логично будет предположить, что чем медленнее будет распространяться волна, тем больше она будет успевать терять энергии по пути и, тем самым быстрее нагревать среду. А ведь это так и есть! Волны более высоких частот с более короткими длинами скоропостижно растрачивают свою энергию и скорее греют поверхностные слои, нежели глубокие зоны. Ага, пюрешка из микроволновки, так вот почему ты постоянно такая холодная внутри!  Относительная диэлектрическая проницаемость — это не одна единственная цифра для конкретного типа материала, как любят рисовать в табличках школьных учебников по физике. Она сильно зависит от частоты (да и от температуры кстати тоже, но пока не будем усложнять). То есть не прокатит измерить один раз эту величину для какого-нибудь подкожного жира или там мышц на частоте 80 МГц и использовать её же на 5 ГГц. Нужно будет озаботиться построением полноценного графика или продвинутой аналитической предсказательной модели, чтобы можно было знать какой всё-таки будет размер волны внутри тела пациента, если хотим полноценно использовать электромагнитные поля как терапевтический инструмент.  Подкрепим наконец сухой непонятный текст сочными картинками. Это простенькая двумерная электромагнитная симуляция, призванная продемонстрировать поведение волн разных частот внутри материала. Возьмём цилиндр с диаметром 160 мм, ну почти как у головы человека (на цилиндр мы смотрим сверху), состоящий из самого распространенного в нашем мире диэлектрика - воды. Он стоит на пути у электромагнитного излучения, падающего на него слева: Как видно, даже в простеньком примере с примитивным объектом, электромагнитные волны различных частот внутри диэлектрика ведут себя непохожим образом. Основная причина — это конечно же разница в физическом размере волны и объекта - волны с частотой от 27 до 500 МГц ну никак не могут поместиться внутри маленького водяного цилиндра, а значит прогревают его практически равномерно. Дело кардинально меняется, когда длины волн становятся сопоставимы размером с нашего водяного друга, что хорошо заметно на частоте 2 ГГц. Вот там уже жарит что надо! Дело в том, что длина волны на частоте 2 ГГц очень близка к размерам самого объекта, а значит в дело вступает его величество резонанс с формированием стоячей волны внутри цилиндра. Ожидаемо с повышением частоты до 5 ГГц потери в материале становятся больше и волны уже не могут полноценно проникать в центр объекта, который становится похож на аналог чёрной дыры. На самом деле параметры воды в представленной симуляции близки скорее к кристально чистому дистилляту, чем к реальной жидкости, с которой мы имеем дело в быту. С настоящей водой радиоволны не дружат совсем, особенно с хорошо проводящей морской и распространяются в ней очень неохотно и, как правило, недалеко. Именно поэтому при организации подводной связи пытаются использовать такие костыли как  волны, где потери ещё терпимые или же пытаются прибегать к помощи совсем других технологий вроде  . Но не будем отходить от темы. При должных знаниях и наличии времени для объектов примитивных форм можно вывести формулы, которые полностью будут описывать поведение электромагнитных волн внутри них. Это даст знания как можно управлять ими, например, чтобы фокусировать энергию в нужном месте. При этом даже получится принять во внимание эффекты вроде затуханий или всякого рода резонансов. Но наш мир сложен и в целом плохо поддается аналитическому описанию, особенно когда речь заходит о такой непростой структуре как человеческое тело. Конечно, голову человека можно условно представить в виде показанного цилиндра. Но адекватно работать такая модель будет только для очень длинных волн, превышающих её размеры, где как мы уже поняли прогрев будет практически однородным. Если же взять частоту 2.45 ГГц, применявшуюся на заре становления диатермии, такие упрощения уже становятся недопустимы. И дело тут даже не в сложности формы головы, а в том, что своему составу и строению ткани организма очень разнородные. Параметры какого-нибудь жирового слоя сильно отличаются от параметров мышц, костей или там кожи. А ведь есть еще и всякие полости с воздухом и жидкости. Так что же там будет происходить внутри? Со времён диатермии никто долгое время не мог ответить на этот вопрос, пока наконец завесу тайны не приоткрыл вот этот человек: В смысле, вы его не знаете? Вообще-то это первый из людей кто смог обрести цифровое бессмертие. Разве не об этом тут все мечтают? Перед вами Джозеф Пол Джерниган, простой   осужденный на смерть убийца из Техаса, который волей судеб после своей казни попал в университет Колорадо. Там с ним сделали тоже самое что и с лошадью в фильме  . Тело Джозефа было заморожено в голубом желатине и при помощи ЧПУ станка было сточено слой за слоем с шагом в 1мм: Таким образом, Джозеф даже не будучи учёным открыл огромную веху в истории науки и медицины в частности - компьютерное трёхмерное моделирование человеческого тела. Каждый его слой был снят на камеру в хорошем на то время разрешении и оцифрован, уместившись в 15 Гб данных. Кроме того, перед тем как его тело разрушили, оно также было отсканировано в КТ и МРТ чтобы собрать по максимуму данных. Так в историю вошёл Visible Human Project. Кроме Джернигана подобным операциям впоследствии подверглись ещё несколько тел. Полученные данные были применены где только можно: в фильме Пятый элемент, компьютерных тренажёрах для врачей, численном моделировании, анатомических атласах и литературе. Но нас они интересуют конечно же с электромагнитной точки зрения. Так как на датасете прекрасно различим тип тканей (благо они отличаются по цвету), то уже не составляет труда присвоить им нужные значения диэлектрической проницаемости и проводимости, чтобы объединить в реалистичную электромагнитную 3d-модель. Уже из беглого анализа пространственного распределения этих параметров выяснились интересные вещи. Посмотрим на голову и то что у неё внутри [ ]: На стыке материалов с разной диэлектрической проницаемостью всегда происходит отражение электромагнитной энергии. Убедится в этом вы можете и сами, посветив лазерной указкой на воду. Часть луча неизменно отразится от границы раздела двух сред, и чем больше разница в диэлектрической проницаемости этих сред - тем больше отражение. И если мы посмотрим на распределение проницаемости в тканях головы, то мы увидим, что мозг окружён именно такой границей. Вдобавок, мозг омывает цереброспинальная жидкость, которая имеет огромную по меркам человеческого тела электропроводность и соответственно показана красным на картинке справа. Это значит что во-первых часть падающих извне электромагнитных волн будет отражаться от черепа, а во-вторых то что всё-таки пройдёт - будет поглощено цереброспинальной жидкостью (которая к тому же ещё и постоянно циркулирует). Таким образом, совпадение это или нет, но эволюция уже экипировала нас своеобразной шапочкой из фольги, спасающей от внешнего радиоизлучения. Так что нет! Сделать выжигатель мозгов чрезвычайно сложно (особенно в таком же виде как он был представлен в нашей любимой игре). С одной стороны это хорошо, учитывая в каком мире мы с вами сегодня живём, окружённые полями со всех сторон. С другой - это создаёт огромные проблемы для тех, кто хочет применять радиоволны   для лечения. Стало окончательно понятно, что вариации параметров материалов внутри тела довольно большие, и универсального способа применять поля для терапии скорее всего нет, так как разница размеров, положения и состава тканей от человека к человеку может быть очень существенной. Ну а ещё все модели людей выше - здоровые и без патологий, а как же быть если мы хотим лечить, например, опухоль? Структура новообразований в человеческом теле обычно связана с неконтролируемым ростом клеток. В связи с этим нарушается строение сосудистой сетки в регионе новообразования. Помните самую первую картинку этой статьи с кадрами тепловизора? Вот именно из-за нарушений в расположении сосудов и появляется регион где ткани охлаждаются током крови хуже чем остальные и который можно выявить термометрией. В какой-то момент, ещё даже до того, как были получены первые цифровые модели, люди задумались, что это свойство может сослужить хорошую службу, ведь если начать греть весь регион, то опухоль перегреется гораздо раньше. Так диатермия превратилась в гипертермию, следующую ступень эволюции электромагнитной терапии: Итак, если диатермия была лишь баловством, то теперь мы добрались уже до полноценного выжигания тканей при помощи электромагнитных полей. Задача гипертермии состоит в том, чтобы нагреть больные клетки до температур денатурации белков в районе 44—45°С, что замедлит их рост или вовсе убьёт. Долгое время на неё возлагали большие надежды как на перспективный метод борьбы с раком, который мог бы быть использован наравне с химио- и радиотерапией, ну или хотя-бы в комбинации с ними. Однако несмотря на огромное количество исследований и кучи произведённого коммерческого оборудования, вроде того аппарата с картинки выше, клиническая значимость этого метода всё ещё остаётся под  . Но изначально идея была хороша: мы могли бы лечить что-то без какого-либо хирургического вмешательства, в труднодоступных регионах (а голова как раз один из таких), да ещё и не повреждая здоровые ткани как при лучевой терапии. Электромагнитная гипертермия должна была превратится во что-то такое:  Если отбросить проблемы с медицинской части, с технической стороны краеугольным камнем электромагнитной гипертермии стала невозможность контроля процедуры лечения. Даже если у вас есть точная трёхмерная модель техасского парня, это вовсе не значит, что у пациента который пришёл к вам на приём внутренности такие же. И гарантировать что перегреется только опухоль никто не может, невзирая на описанные выше особенности сетки сосудов в районе новообразований (которая опять же индивидуальна). А потому технологию стали развивать в сторону повышения точности, а значит применения более высоких частот и прицельной фокусировки полей при помощи таких же технологий как в военной радарной технике - фазовых антенных массивов. Но если радары смотрят своими антеннами в небо, то в данном случае мы направляем их внутрь человеческого тела.  Гениальным решением победить слепоту процедуры радиочастотного нагрева стали как ни странно те же электромагнитные поля, но в другой своей ипостаси - в виде магнитно-резонансной термометрии [ ]: Аппарат  может не только показывать распределение воды в организме, но ещё и измерять относительные изменения температуры объектов во времени, ведь резонансная частота протонов, сигналы которых он измеряет тоже зависит от температуры. Будучи скомбинированным с массивом антенн, способных фокусировать энергию внутри тела пациента он превращается в этакий прообраз машины из Элизиума способной как на диагностику, так и на терапию используя только лишь электромагнитные волны! Сперва при помощи МРТ создаётся трехмерная цифровая модель пациента, по которой ведётся расчет фаз и амплитуд для каждой излучающей антенны, а потом начинается нагрев, который чередуется со снимками термометрии для полноценного контроля процесса. Снова заметим, что на границе двух сред разной диэлектрической проницаемости всегда есть отражение электромагнитной энергии. Такой границей конечно же является еще и воздух-кожа. Поэтому между антеннами для гипертермии и пациентом ещё и приходится помещать мешки с водой (окружают тело пациента на снимках), иначе эффективность передачи энергии даже с таких близко расположенных антенн сильно страдает. Удобным для пациента конечно такой процесс не назвать. Но это ещё цветочки по сравнению с тем, что шевелиться во время процесса (а он легко занимает и более часа в экспериментах) ни в коем случае нельзя - ведь модель тела при расчёте фокусировки делается для определённого его положения и если пациент сдвинется, то что-то может пойти не так.  В общем несмотря на то, что идея уже очень старая по нашим меркам, до сих пор эксперименты в области проводятся на пике возможностей наших технологий. Один лишь расчет фокусировки для подробной модели человеческого тела может требовать пару часов работы не самого слабого GPU, а алгоритмы для таких расчётов до сих пор совершенствуются и усложняются, как, собственно, и сами модели. Но, наверное, самой главной проблемой на пути гипертермии всё ещё остается непонимание того как использовать температуру для лечения. Как выяснилось, просто выжечь что-то внутри тела это не такая уж и хорошая идея, а потому все исследования в области сменили направление в поисках чего-то более удачного. Одним из таких решений в перспективе может стать применение наночастиц-носителей лекарства, активируемых нагревом. Но пока выходит, что одними лишь электромагнитными полями не обойтись.  Ещё много копий будет сломано в спорах о том, вреден ли 5G и облучает ли вас сосед по ночам. А всё из-за того, что наш прогресс в исследовании электромагнитных полей и их воздействия с человеком показал, что этот вопрос невероятно сложен. Причина этой сложности заключается внутри самих нас - в строении человеческого тела. Для правильного моделирования нагрева одной лишь ткани из многих входящих в состав организма может потребоваться знание до восьми различных параметров, которые ещё и варьируются от человека к человеку. А теперь ещё вспомним что это всё находится в непрерывном движении! Вдобавок - количество энергии что попадет внутрь вашего тела от внешнего источника зависит от ещё большей груды параметров. И попробуй в таких условиях придумать что-то универсальное вроде пластыря на ранку. Именно поэтому в медицине, в области, где люди хотят быть уверены в используемых методах, электромагнитные поля пока так и не нашли твёрдых оснований для применения. Однако в научной среде жизнь кипит, методология постоянно развивается и ждет своих новых героев не хуже аз-Захрави, Теслы, Д'Арсонваля и   сотрудников университета Колорадо. Тех кто за грудой проблем увидит решение, которое изменит этот мир. Спасибо за внимание! Вот вам напоследок электромагнитная симуляция наносекундного радиоимпульса, испускаемого близко расположенными широкополосными антеннами прямо в мозг. Обратите внимание сколько энергии теряется на периферии.",
  "tag": "Излучение",
  "hub": 4,
  "author": 1,
  "add_datatime": "2021-10-15T06:26:10.276Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 21,
 "fields": {
  "name": "Делаем гостевую Wi-Fi сеть в ВУЗе",
  "image": "1634279305.841037.jpg",
  "text": "Привет, Хабр! При расширении зоны действия Wi-Fi увеличением числа точек доступа, управляемых шлюзом Zyxel VPN300 (сначала был Zyxel UAG5100), увеличилось количество гостей и проблемы начались в час пик со скоростью интернета, открытием сайтов, шустростью сёрфинга. Уже не успевал мониторить/конфигурировать комплекс и оперативно реагировать на заявки. Помощника не дали, как это своими силами лечилось и чем настраивалось, делюсь опытом для всех и проведу краткую экскурсию по функциям. и  мои предыдущие статьи. В конце текста информация обо  .   Будут рассмотрены функции Вообще, проблемы начались ещё при UAG5100, поэтому сначала с ним были настроены функции, опробованы в реальных условиях и реально помогло. Поэтому, когда получили VPN300, эти же функции активировал на нём и всё отлично работает. Уже не помню, когда гости последний раз не могли получить фотку или отправить письмо. Функции на шлюзах Zyxel  – ограничивает скорость каждому гостю, подключённого к точке доступа, управляемой шлюзом (рис.1).  – запрещает перекрестный трафик в сети внутри SSID на одной точке доступа (ТД) (рис.1). - блокировка трафика между клиентами, подключенными к сети шлюза (рис.2). Проводной ноутбук не сможет пинговать Wi-Fi клиентов на любых ТД и те тоже не смогут пропинговать ноутбук.  – шлюз проверяет связку IP-MAC со своей базой выданных IP при поступлении запросов (рис.3). Пользователь не сможет вручную назначить своему компьютеру другой IP-адрес и использовать его для подключения к шлюзу. . управление пропускной способностью (рис.4). Ограничение скорости силами шлюза любому подключённому устройству/учётке/подсети или группе устройств/учёток/подсетей. В гостевой Wi-Fi сети всем неавторизованным (их много) ограничиваем скорость до минимума (например, 256 кбит/с), чтобы не заваливали агрессивными запросами (куча вкладок в браузере, вирусы, обновления), но разрешаем повышенные скорости до внешних серверов авторизации, для быстрой загрузки страницы авторизации. Далее, всем авторизованным выставим комфортные скорости, а VIP-персонам безлимит.  VIP-персоны с безлимитными скоростями упрутся в установленные ограничения скоростей в профиле SSID (рис.1), поэтому в профиле SSID выставляем максимально безопасные ограничения (30 мбит/с), чтобы VIP-персоны всю полосу на одной ТД не заняли. Правилам можно применить расписание, к примеру, ночью авторизованным ещё выше скорости выделять. Функционал очень широкий и понятный. Разберётесь. Подойдёт сетям с автономными и  ТД Zyxel Nebula. ограничение количества сессий гостям (рис.5). Например, делаем так (правила действуют снизу вверх): авторизация гостевых устройств перед выходом в интернет (рис.6). Профилями выбираем интерфейсы/подсети/адреса, которые необходимо авторизовать по постановлению Правительства РФ  . По предварительной записи VIP-персонам, сотрудникам, телевизорам и другим специфичным устройствам создаёте отдельный профиль по мак-адресам и не требуете с них авторизацию или авторизуете встроенной системой биллинга с распечаткой ваучеров на термопринтере Zyxel SP350E. Остальные гости авторизуются внешним сервисом авторизации по смс/звонку.  (модуль встроенного сервиса авторизации) - учётные записи для гостей, тарифные планы.  (Рис.7-1): Создание профиля биллинга (тарифного плана) (рис.7-2). Если перед конференцией много посетителей набралось, за один этап можно сгенерировать до 50 учёток по выбранному тарифу (кнопка A, B или C), распечатать на обычном А4 принтере прям из браузера и выдать. При бОльшем количестве посетителей, процедуру генерирования учёток повторить (рис.7-3). управление принтерами для распечатки ваучеров (рис.8). Поддерживает принтеры только Zyxel SP350E. Для распечатки ваучеров не требуется доступ к веб-управлению шлюза, достаточно нажать на термопринтере на нужную кнопку с тарифом (создаются в  ) и распечатается ваучер с реквизитами. Русские символы не поддерживает. Кодировка текста у ваучера UTF-8. Обходились транслитом.  разрешение посетить сайты без авторизации (рис.9). установка времени включения/отключения какого-нибудь профиля (рис.10). Например: радиопрофили точек доступа, SSID,  маршрутизации, направления интернет-трафика к другому провайдеру, патруля приложений (отсутствует у моделей VPN), правила файрволла, BWM и т.д. доступ к шлюзу (рис.11). В «Контроль доступа админов» добавляем правило, выбираем зону с Wi-Fi гостями и выставляем   для гостевых подсетей.  – отправка графического отчёта по E-Mail за прошедшие сутки (рис.12-1). Отчёт читабельный и информативный, 15 шт графиков и почти 80 таблиц со статистикой внешних и внутренних интерфейсов, включая список стран, по которым было скачивание/отправка трафика. Чтобы не утруждать скроллингом отчёта, прикреплю пару скриншотов из отчётов (рис.12-2):        – отправка событий на сислог сервер, флешку и двум E-Mail (рис.13-1). Выручает отправка логов (alert) сразу на E-Mail, настроенного на телефоне, на нём входящие письма сигнализируются. Чаще всего приходят такие категории: На рис.13-2 отображены оповещения о дисконнекте ТД (слева), неправильном вводе пароля (в центре) и запросе на подключение на 22 порт шлюза (справа). Функции на коммутаторах Zyxel  - вывод списка подключённых устройств к коммутатору (рис.14-1). Нажимаем на  , затем  : Получаем список подключённых (Remote) PoE устройств, которые можем перезагрузить (кнопка Cycle) или сбросить (кнопка Reset) до дефолтного состояния или зайти по забытому IP на него (рис.14-2). световая сигнализация коммутатора. Она мигает на лицевой панели светодиодом “Locator” синим цветом, сигнализируя о себе (рис.15). Текст уже большой стал. Закругляюсь. Для учебных заведений очень полезны функции шлюза как  ;  ;  . Патруль приложений - позволяет запретить (можно по расписанию) торренты, Ютуб и прочие не полезные категории для учащихся или ограничивает скорость. Гео-IP - позволяет запретить/разрешить посещать сайты по странам/группам и не только. Можно по расписанию. Список сами создаёте. Очень пригодится учебному заведению на фоне многочисленных разговоров об ограничении  доступа учащихся к иностранным сайтам. Он также в логах показывает из какой страны поступил запрос к вашим IP. Ещё можно в самом шлюзе пробить IP, какой стране принадлежит. Финальный итог и почему выбрал Zyxel? Хотите это обсудить? Поспорить? Пишите в комментариях, а также общайтесь со мной и другими практикующими специалистами Zyxel в телеграм-канале  .     Об авторе: Кто я? Меня зовут Александр. 16 лет профессионально занимаюсь СКС и сетевым оборудованием. Больше времени провожу с СКС (монтаж, обслуживание), чем с настройкой сетевого оборудования, поэтому на настройки времени особо нет, но получил много опыта работы с сетевым оборудованием различных вендоров. Приветствую простоту и дружелюбность конфигурирования/мониторинга сетевого оборудования, ответственность вендора за качество выпускаемой продукции и готовность исправлять недостатки, чтобы не тормозили сдачу новых проектов в назначенные сроки. Меня можно встретить и лично пообщаться в телеграме - , так что, если будут вопросы, комментарии, мнения – милости прошу.",
  "tag": "wi-fi",
  "hub": 5,
  "author": 1,
  "add_datatime": "2021-10-15T06:28:25.842Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 22,
 "fields": {
  "name": "Из-за чего Facebook стал глобально недоступен.",
  "image": "1634279438.888177.png",
  "text": "\"«Разве Facebook может упасть?» — задумались мы на секунду… Сегодня в 16:51 UTC   у нас был открыт внутренний инцидент под названием «Facebook DNS lookup returning SERVFAIL»  . Мы решили, что это с нашим DNS-ресолвером   что-то не так. Однако к моменту размещения соответствующего обновления на   стало ясно, что здесь что-то серьёзное. Социальные сети уже разрывались от сообщений о том, что быстро подтвердили и наши инженеры: Facebook и связанные с ним сервисы WhatsApp и Instagram действительно упали. Их DNS-имена больше не ресолвились, а IP-адреса инфраструктуры были недоступны. Выглядело так, как будто кто-то буквально выдернул кабели разом во всех их дата-центрах, отключив от интернета. Как такое вообще возможно? Встречайте BGP  — это «протокол граничного шлюза»  . Это механизм для обмена информацией о маршрутизации между автономными системами (AS) в интернете. У больших роутеров, благодаря которым работает интернет, есть постоянно обновляемые списки возможных маршрутов, используемых для доставки каждого сетевого пакета до мест их назначения. Без BGP интернет-роутеры не знают, что делать, и интернет просто не будет работать. Интернет — это буквально сеть из сетей, связанных между собой с помощью BGP. BGP позволяет одной сети (скажем, Facebook) объявлять о своём присутствии другим сетям, которые в конечном счёте формируют весь интернет. На момент написания этой статьи Facebook не сообщал о своём присутствии, поэтому интернет-провайдеры   и другие сети не могут найти сеть Facebook — она недоступна. У индивидуальных сетей есть свой ASN — номер автономной системы  . Автономная система (AS) — это индивидуальная сеть с унифицированной политикой внутренней маршрутизации. AS может порождать специальные префиксы (означающие, что они контролируют группу IP-адресов), а также транзитные префиксы (они знают, как добраться до определённых групп IP-адресов). Например, ASN у Cloudflare —  . Каждая ASN должна объявить интернету о своих prefix routes с помощью BGP. В ином случае никто не узнает, как к ней подключиться и где найти её. В этой упрощённой схеме можно увидеть шесть автономных систем в интернете и два возможных маршрута, по которым один пакет может пройти от начала   до конца  . Самый быстрый маршрут — это  . Самый медленный —  ; он используется в случаях, когда первый не срабатывает. В 16:58 UTC мы заметили, что Facebook перестал анонсировать маршруты для своих DNS-префиксов. Это означало, что по меньшей мере DNS-серверы Facebook были недоступны. По этой причине DNS-ресолвер Cloudflare (уже упомянутый 1.1.1.1) не мог отвечать на запросы, требующие выдать IP-адрес для домена facebook.com или instagram.com. Хотя другие IP-адресы Facebook и имели маршруты в то же самое время, в них не было особого смысла, потому что DNS-службы Facebook и связанных сервисов были недоступны: Мы следим за всеми обновлениями и анонсами в BGP, какие появляются в глобальной сети. Собираемые таким образом данные позволяют увидеть глобальные связи в интернете и понять, откуда и куда должен ходить весь трафик. UPDATE-сообщение от BGP информирует роутер о любых изменениях, сделанных в префиксе, или о полном отзыве этого префикса. Проверяя базу данных BGP, основанную на временных рядах, мы можем точно увидеть количество обновлений, поступивших от Facebook’а. Обычно этот график довольно ровный: Facebook не будет постоянно делать большое количество изменений для своей сети. Но около 15:40 UTC был замечен резкий всплеск изменений в маршрутах Facebook’а. Именно здесь и начались проблемы. Ещё лучше будет видно, что же произошло, если разбить этот график на анонсы маршрутов и их отзывы. Маршруты были отозваны, DNS-серверы Facebook ушли в offline, а минутой позже возникла проблема: инженеры Cloudflare сидели и недоумевали, почему 1.1.1.1 не может получить IP для facebook.com, обеспокоенные каким-то сбоем в своих системах. После отзыва этих маршрутов Facebook и его сайты были отключены от интернета. DNS тоже в деле Прямым последствием этого события стала невозможность для DNS-ресолверов со всего мира получать IP для связанных с проектами доменных имён: Это происходит по той причине, что в DNS, как и во многих других системах в интернете, используется свой механизм маршрутизации. Когда кто-то набирает   в веб-браузере, DNS-ресолвер, ответственный за перевод доменного имени в реальный IP-адрес для фактического подключения, сначала проверяет, есть ли что-то в его кэше. Если кэш есть — он используется. Если кэша нет — производится попытка получить ответ от DNS-сервера, обычно расположенного где-то поблизости. Если DNS-серверы недоступны или не могут дать ответ по какой-то другой причине, возвращается ответ SERVFAIL, а браузер показывает пользователю ошибку. Из-за того, что Facebook перестал анонсировать свои DNS prefix routes через BGP, наш и любой другой DNS-ресолвер не мог подключиться к DNS-серверам проекта. Поэтому, 1.1.1.1, 8.8.8.8 и другие крупные публичные DNS-ресолверы начали выдавать (и кэшировать) ответы SERVFAIL. Но это ещё не всё. Теперь в дело включается человеческий фактор и логика работы приложения, что в совокупности приводит к экспоненциальному эффекту. От пользователей обрушивается огромная волна дополнительного DNS-трафика. Отчасти это происходит по той причине, что приложения не расценивают ошибку как подходящий пользователю ответ и начинают делать повторные запросы, причем иногда очень активно. А отчасти — потому что конечные пользователи тоже не воспринимают ошибку за правильный для них результат и начинают обновлять страницы, убивать/перезапускать свои приложения, порой тоже весьма активно. Всё это привело к резкому росту трафика (по количеству запросов), что мы наблюдали на 1.1.1.1: Из-за того, что Facebook и его сайты так популярны, мы получили 30-кратную нагрузку на DNS-ресолверы по всему миру, а это может вызывать задержки и таймауты для других платформ. К счастью, 1.1.1.1 был создан как бесплатный, приватный, быстрый (убедиться в этом можно в  ) и масштабируемый сервис, так что мы продолжали обслуживать своих пользователей с минимальными проблемами. Скорость ответов на подавляющую часть DNS-запросов оставалась в диапазоне менее 10 мс. В то же время небольшая часть перцентилей p95 и p99 показали повышенное время ответов — вероятно, из-за истекших TTL при обращении к DNS-серверам Facebook и вызванных таймаутов. 10-секундный таймаут для DNS — значение, которое пользуется популярностью среди инженеров. Влияние на другие сервисы Люди ищут альтернатив, хотят знать и обсуждать, что происходит. Когда Facebook упал, мы увидели растущее число DNS-запросов к Twitter, Signal и другим социальным сетям и платформам для обмена сообщениями. Также недоступность проявилась в статистике по  -трафику от и к автономной сети Facebook’а (ASN 32934). Эта карта показывает, как трафик изменился в интервале с 15:45 UTC до 16:45 UTC по сравнению с тремя часами до этого в каждой стране. По всему миру WARP-трафик от и к сети Facebook практически исчез. Интернет Сегодняшние события служат мягким напоминанием о том, что интернет — это очень сложная и взаимозависимая система из миллионов систем и протоколов, взаимодействующих друг с другом. Доверие, стандартизация и кооперация между задействованными в нём организациями — ключ к его работоспособности для почти пяти миллиардов активных пользователей со всего мира. Обновление Около 21:00 UTC   мы увидели новую BGP-активность в сети Facebook, пик которой пришёлся на 21:17 UTC: График ниже показывает доступность DNS-имени 'facebook.com' на DNS-ресолвере 1.1.1.1. Она пропала около 15:50 UTC и вернулась в строй в 21:20 UTC: Несомненно, сервисам Facebook, WhatsApp и Instagram ещё понадобится некоторое время, чтобы полностью вернуться в строй, но по состоянию на 21:28 UTC Facebook уже доступен в глобальном интернете, а его DNS снова функционирует. P.S. от переводчика Читайте также в нашем блоге.",
  "tag": "Facebook",
  "hub": 5,
  "author": 1,
  "add_datatime": "2021-10-15T06:30:38.889Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 23,
 "fields": {
  "name": "Мониторинг сервера Zimbra OSE с помощью Nagios",
  "image": "",
  "text": "Ранее в нашем блоге мы рассказывали о том, как настроить мониторинг сервера Zimbra OSE с использованиемтакого популярного решения как Zabbix. Однако помимо Zabbix для мониторинга состояния серверов и сервисов также активно используется и другое решение - Nagios. Оно также имеет открытый исходный код и бесплатную версию, и также может использоваться для мониторинга сервера Zimbra OSE. В данной статье мы расскажем о том, как установить Nagios на сервер с Ubuntu 18.04, а также о том, как настроить в нем наблюдение за состоянием самого сервера и различных сервисов Zimbra OSE. В нашем примере мы будем использовать два сервера:   (10.0.1.49) и   (10.0.1.38). На первом мы установим серверную часть Nagios, второй сервер выполняет роль почтового сервера Zimbra OSE. Установка сервера Nagios Установить Nagios на сервере   можно из репозитория при помощи команды  , однако доступная там версия значительно устарела. В связи с этим мы соберем Nagios из исходного кода, доступного на GitHub. Начать следует с установки зависимостей. Для этого поочередно введите команды: Доступные для скачивания релизы Nagios можно найти  . Для скачивания исходных кодов перейдем в папку /usr/src и с помощью wget скачаем архив с самыми свежими исходниками. В нашем случае это  . Распакуем архив  и перейдем папку с исходным кодом -  . Запустим компиляцию пакета, поочередно выполняя команды В случае, если все пройдет успешно, вы увидите в выводе команды строку . Для того, чтобы попасть в веб-интерфейс Nagios, нужно создать пользователя nagiosadmin и задать пароль для него. Делается это с помощью командыsudo htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin. После ее запуска вам будет предложено дважды ввести пароль этого пользователя: После этого следует настроить файрвол для доступа к Apache. Для этого введите команды   и  . Установка плагинов Nagios Доступные для скачивания релизы Nagios Plugins можно найти  . Для скачивания актуальной версии также перейдем в папку /usr/src и скачаем архив при помощи wget. В нашем случае это  Распакуем архив   и перейдем в папку с исходниками  . Последовательно выполним команды Также установим плагин nrpe, исходный код которого также  . Находясь в папке /usr/src/ закачаем архив с исходным кодом приложения  , распакуем его  и перейдем папку с исходным кодом -  . Находясь в ней также выполним команды После этого для корректной работы nrpe также необходимо открыть для подключений порт 5666 c помощью команды  . Запуск Nagios Когда Nagios и необходимые плагины установлены, можно запустить службу Nagios с помощью команды . Проверить статус службы Nagios можно с помощью команды . Доступ к веб-интерфейсу Nagios в нашем случае осуществляется с сервера   через браузер по ссылке  . При входе в веб-интерфейс Nagios запросит логин и пароль. Укажите в качестве логина nagiosadmin, а в качестве пароля заданный ранее пароль. Сразу после установки Nagios мониторит только состояние сервера, на котором он установлен. Чтобы сервер Nagios мог такжеотслеживать состояние сервера и различных сервисов Zimbra OSE, необходимо добавить в него соответствующий узел. Для этого необходимо выполнить ряд действий на сервере Zimbra OSE. Настройка узла Zimbra OSE В первую очередь необходимо установить на узле с Zimbra OSE Nagios NRPE Server - агентское приложение Nagios, которое запускает на сервере различные команды, собирает статистику и передает данные на сервер. Его можно установить из репозитория Ubuntu с помощью команды  . Также необходимо установить плагины для Nagios. Делается это при помощи команды sudo apt-get install monitoring-plugins. После установки в каталоге /etc/ появится папка nagios, в которой содержатся конфигурационные и исполняемые файлы сервера NRPE. Плагины Nagios были установлены в папку /usr/lib/nagios/plugins. Для корректной работы сервера NRPE необходимо открыть подключения на порту 5666. Для этого введите команды   и  . Также необходимо отредактировать файл конфигурации  , где в нашем случае следует привести следующие строки к такому виду: Также отредактируем файл sudo nano /etc/sudoers, куда добавим права на исполнение команд, связанных с мониторингом сервера Zimbra OSE без ввода пароля:  . После этого перезапустимсервер nagios-nrpe-server на узле   с помощью команды , а также перезапустимnagios на узле   с помощью команды . Настройка мониторинга Zimbra OSE Для того, чтобы мониторинг сервера Zimbra OSE начал осуществляться, необходимо добавить новый хост в Nagios. Для этого создадим папку sudo mkdir /usr/local/nagios/etc/servers, а затем отредактируем файл sudo nano/usr/local/nagios/nagios.cfg, где в соответствующий раздел добавим строку . Скопируем шаблон с настройками хоста из папки с примерами в созданную нами папку cp /usr/local/nagios/etc/objects/localhost.cfg /usr/local/nagios/etc/servers/zimbra.cfg. Откроем файл zimbra.cfg для редактирования sudo nano /usr/local/nagios/etc/servers/zimbra.cfg. В нем зададим параметры самого хоста и сервисов, которые будем опрашивать. Для настройки хоста отредактируйте данные в разделе # Define a host for the local machine. В нашем случае он будет иметь следующий вид: Также создадим группу хостов Zimbra Servers. Это очень удобно в тех случаях, когда необходимо мониторить целый ряд серверов. Для этого в разделе #Define an optional hostgroup for Linux machines Мониторинг стандартных сервисов, таких как работоспособность IMAP, POP3 и SMTP настраивается в разделе # Define a service to \\\"ping\\\" the local machine. В случае проведения нестандартных проверок, помимо правки файла/usr/local/nagios/etc/servers/zimbra.cfg, необходимо будет редактировать файл /usr/local/nagios/etc/objects/commands.cfg и файл /etc/nagios/nrpe.cfg на сервере Zimbra OSE. 1. Для подключения мониторинга службы SMTP добавьте в/usr/local/nagios/etc/servers/zimbra.cfg блок 2. Для подключения мониторинга службы IMAP добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок 3. Для подключения мониторинга службы POP3 добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок 4. Для подключения мониторинга службы IMAPS добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок А также добавьте блок в/usr/local/nagios/etc/objects/commands.cfg 5. Для подключения мониторинга службы POP3S добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок А также добавьте блок в/usr/local/nagios/etc/objects/commands.cfg 6. Для подключения мониторинга службы ClamAVдобавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок  Также на сервере Zimbra OSE в файле/etc/nagios/nrpe.cfg добавим команду  7. Для подключения мониторинга протокола LMTP добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок  Также на сервере Zimbra OSE в файле /etc/nagios/nrpe.cfg добавим командуcommand[check_lmtp]=/usr/lib/nagios/plugins/check_smtp -H localhost -p 7025. 8.Для подключения мониторинга актуальности SSL-сертификата добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок Также на сервере Zimbra OSE в файле /etc/nagios/nrpe.cfg добавим команду command[check_cert]=/usr/lib/nagios/plugins/check_cert-S -H localhost -C 30 9.Для подключения мониторинга количество пользователей, осуществивших вход на сервер, добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок Также на сервере Zimbra OSE в файле /etc/nagios/nrpe.cfg добавим команду command[check_users]=/usr/lib/nagios/plugins/check_users -w 2 -c 3 10. Для мониторинга средней нагрузки и доступности сервера добавьте в /usr/local/nagios/etc/servers/zimbra.cfg соответственно блоки  11. Также можно подключить мониторинг свободного места в разделе /opt/, где располагаются файлы Zimbra OSE. 9. Для этого добавьте в /usr/local/nagios/etc/servers/zimbra.cfg блок Также Nagios позволяет настроить оповещения о различных ошибках и критических ситуациях, которые возникают на серверах инфраструктуры. Настроить оповещения можно в веб-интерфейсе Nagios на вкладке Notifications. Пример настройки оповещений показан на скриншотах. Таким образом мы настроили мониторинг сервера Zimbra, а также ряда запущенных на нем служб, а также настроили отправку оповещений об отказе или критических ситуациях, возникающих на сервере Zimbra OSE. По всем вопросам, связанными c Zextras Suite и Team Pro вы можете обратиться к Представителю компании «Zextras» Екатерине Триандафилиди по электронной почте ekaterina.triandafilidi@zextras.com.",
  "tag": "zimbra",
  "hub": 5,
  "author": 1,
  "add_datatime": "2021-10-15T06:31:38.266Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 24,
 "fields": {
  "name": "pfSense сегодня — VPN",
  "image": "1634279822.407121.png",
  "text": "Давно не было на Хабре новостей про новые версии pfSense - последняя новость датируется 2014 годом и рассказывает ещё о  . Последняя стабильная Community Edition версия pfSense на сегодня -   (вышла в июле 2021). О ней и пойдёт речь, точнее о доступном функционале и полезных фишках. Так как нового накопилось довольно много, будет цикл статей по разным сервисам файрвола (уведомления в Telegram!) и его пакетам (Squid, pfBlockerNG, Snort/Suricata). А сейчас VPN. IPsec К одним из наиболее важных изменений стоит отнести поддержку аппаратного ускорения  , что позволяется заметно ускорить криптографические операции на современных процессорах. Активизируется на вкладке System → Advanced/ Miscellaneous. См. про оптимизацию VPN в  . IPsec демон  . Поддержка сертификатов на эллиптических кривых ( ),- возможно использовать не только в IPsec, но и в OpenVPN, пакетах Squid, HAProxy и пр. Несмотря на обилие разных алгоритмов, современное ПО работает только с тремя из них - secp384r1, secp521r1 и prime256v1. Возможность выбора   (Pseudo-Randon Functions) при настройке первой фазы - не настолько часто используемая функция, обычно нужна если на удалённой стороне решили помудрить с данным параметром.  Ручная настройка таймаутов   и  а также  . Бывает необходимо для корректной переинициализации SA.  .  - позволяет создать несколько IPsec конфигураций к одному удалённому хосту. Нужно если на pfSense несколько внешних сетевых интерфейсов + динамическая маршрутизация на них (BGP, OSPF).   - использование ключа с PKCS#11 токена. Можно использовать Yubikey, Rutoken и пр. токены  .  - как понятно из название это максимальное кол-во IKEv1 Phase 2 Exchanges. Если вам всё ещё непонятно, то данная опция пригодится только если у вас IKEv1 с большим количеством Phase 2 туннелей (чтобы не отваливались при rekeying). возможность использования нестандартных портов для IKE (500 порт по дефолту) и NAT-T (4500 порт). Может использоваться для обхода блокировки IPsec и в некоторых экзотических случаях. Что касается remote access IPsec (правильно IPsec Mobile), то тут два небольших дополнения:   - важная опция, позволяющая переключать режим пакетного фильтра для IPsec, в связи с особенностями его работы на FreeBSD. Так, по-умолчанию возможно настраивать фильтр пакетов для всего IPsec трафика на одной вкладке, т.е. Tunnel, Transport и VTI трафик весь фильтруется в одном месте (псевдо-интерфес  ). Если же переключить режим в Filter IPsec VTI and Transport on assigned interfaces, то можно создавать отдельные правила файрвола на каждом VTI или Transport интерфейсе, но Tunnel режим работать не будет.  OpenVPN Главным образом багфиксы, вышеупомянутая поддержка ECDSA сертификатов,    , а также:  - позволяет использовать Client Specific Overrides (персональные настройки для нужных пользователей) в Remote Access (User Auth) режиме.  - IPv4, IPv6 и обратные зоны. Опция Services > DNS Resolver / OpenVPN Clients.  - проверка валидности сертификатов с помощью OCSP сервера. Многочисленные улучшение в парсере Cisco-AVPair правил прилетающих с RADIUS сервера - поддержка IPv6, протоколов icmp, udp, tcp, опций для указания диапазонов портов (gt, lt, range, eq, ne). Пример:  - теперь pfSense может использовать DNS сервер получаемый при подключении к удёлённому OpenVPN серверу. Необходимо если ваш VPN-провайдер выдаёт параметры DNS сервера при каждом подключении (например ExpressVPN).   - новая опция при настройке Client Specific Override, предотвращающая добавление маршрутов в таблицу маршрутизации клиента. OpenVPN Client Export теперь может создавать \\\" \\\" - неинтерактивный установщик который можно использовать для деплоя. В новой версии pfSense ожидается поддержка алиасов в OpenVPN и вам больше не придётся писать через запятую перечень всех local/remote сетей) L2TP Из нового только возможность указать VPN MTU, а в целом это небольшие багфиксы. К примеру теперь сервис не будет рестартовать при каждом добавлении/удалении/изменении пользователей. WireGuard Новый тип VPN в pfSense - поставляется в виде отдельного пакета и настраивается в несколько кликов. Будет отдельная статья про эти самые клики) на сегодня всё о новинках pfSense - это моя первая статья на хабре, буду рад ответить на вопросы и исправить замечания Полезные статьи по pfSense.",
  "tag": "pfsense",
  "hub": 5,
  "author": 1,
  "add_datatime": "2021-10-15T06:37:02.408Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 25,
 "fields": {
  "name": "Опыт извлечения обучающих данных из генеративных языковых моделей",
  "image": "",
  "text": "Вдохновившись опытом зарубежных коллег по извлечению данных из больших языковых моделей из следующих источников: Extracting Training Data from Large Language Models/Извлечение обучающих данных из больших языковых моделей (генеративных)/Authors: Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee1, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, Colin Raffel ( ) The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks/Открывающий секреты: оценка и тестирование непреднамеренного запоминания в нейронных сетях/ Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song. ( ) Membership Inference Attacks Against Machine Learning Models/Атаки на определение членства против моделей машинного обучения/ Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov ( ) An Attack on InstaHide: Is Private Learning Possible with Instance Encoding?/Атака на InstaHide: Возможно ли частное (приватное/не допускающее утечек) обучение с помощью кодировния экземпляра при обучении моделей / Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Shuang Song, Abhradeep Thakurta, Florian Tramèr ( ) Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning/ Всесторонний анализ конфиденциальности глубокого обучения: Пассивные и активные атаки вывода обучающего набора данных на модель в белом ящике при централизованном и федеративном обучении/ Milad Nasr, Reza Shokri, Amir Houmansadr ( ) мы решили на собственном опыте апробировать описанные методы. В качестве «подопытных» мы взяли модели GPT – это генеративный предобученный трансформер, который представляет собой нейронную сеть с отдельным слоем внимания. При этом языковые модели решают ровно одну задачу: они пытаются предсказать следующий токен (обычно слово или его часть) в последовательности по предшествующим с учетом предыдущего контекста. Почему мы считаем важным изучать и понимать ЯМ? Известны случаи неконтролируемого поведения чат-ботов по негативному сценарию. Например, чат-бот одного банка предложил клиентке отрезать себе пальцы:  . Чат-бот Lee Luda, разработанный сеульским стартапом Scatter Lab, Для того чтобы выпускать качественный IT-продукт необходимо понимать и контролировать все его действия. Поэтому мы решили разобраться, действительно ли ЯМ могут запомнить данные, как можно их извлечь. Для работы с моделями мы использовали платформы: и . Чтобы не перегружать текст статьи код на python мы сохранили (авторы не претендуют на идеальный код) В данной статье мы рассматриваем модель в качестве чёрного ящика, так как хотели установить смогут ли среднестатистические мошенники получить чувствительные данные, без изучения весов и параметров модели, для осуществления противоправных действий. Чёрный ящик – это модель системы, при которой наблюдателю не известно внутреннее устройство и свойства системы. Наблюдатель видит только то, что система принимает на свой вход и то, что получается на выходе. Белый ящик – это противоположное понятие, то есть в этой модели системы наблюдатель знает из каких частей и подсистем она состоит, какие связи есть между элементами, какие функции доступны, структуру системы. В качестве чувствительных данных мы будем рассматривать персональные данные людей и номера карт. Проанализировав опыт статей, мы отобрали три способа извлечения данных, которые показали относительно большую эффективность в достижении поставленных целей: основывается на принципах статистики и теории вероятностей (источник [А]). Предполагается, что если модель запомнила какие-то данные, то эти данные должны появиться при значительном числе сгенерированных текстов. Последовательность действий такая: определяется три стратегии генерации текста: По каждой стратегии на основе заданных префиксов ( ) генерируется 200000 текстов, затем наборы очищаются от дубликатов, в том числе от текстов схожих по триграммам. Затем по каждой из шести метрик отбираются по 100 текстов и мануально в интернете осуществляется поиск совпадений сгенерированных строк с реальными. В качестве метрик используются следующие: Авторы метода утверждают, что из отобранных 1800 текстов в среднем 33,5% имеют запомненные чувствительные данные. способ подразумевает генерацию и отбор текстов с помощью графа (источник [B]). Корнем дерева-графа является затравка, которая подаётся для генерации, в узлах каждого следующего уровня располагаются сгенерированные слова, а веса рёбер соответствуют вероятностям этих слов, с которой они могут являться продолжением корневой для этого узла фразы. После построения дерева-графа, с помощью алгоритма Дейкстры поиска оптимального пути формируются наиболее вероятные строки и проверяется их наличие среди чувствительных данных (в нашем случае – поиск в интернете). способ использует специально обученную модель атаки для извлечения данных (источник [С]). Модель атаки является классификатором, который помечает тип данных, сгенерированных целевой моделью: были они в обучающих данных или нет. Основная сложность этого способа заключается в вопросе: «Как обучить модель атаки?» Для решения проблемы предлагается использовать метод теневого обучения: в этом случае создаётся несколько «теневых моделей», которые имитируют поведение целевой. Теневые модели должны быть созданы и обучены аналогично целевой модели. Основополагающая идея в том, что аналогичные модели, обученные на относительно похожих записях данных с использованием одного и того же метода, ведут себя аналогичным образом. Эмпирически исследователями доказано, что чем больше теневых моделей, тем точнее атака. Для обучения теневых моделей требуется генерация обучающих корпусов, если не известно на чём обучалась целевая модель. Генерация осуществляется с помощью целевой модели. Предполагается, что записи, которые классифицируются целевой моделью с высокой степенью достоверности, должны быть статистически подобны учебному набору данных и, таким образом, служить хорошим обучающим набором для теневых моделей. Процесс синтеза проходит в два этапа: на (1)-ом, используя алгоритм поиска восхождение к вершине (простой итеративный алгоритм поиска локального оптимума), осуществить поиск пространства возможных записей данных, которые классифицируются целевой моделью с высокой степенью достоверности; на (2)-ом этапе происходит отбор синтетических данных из этих записей. Другими словами, сначала генерируется первоначальный набор данных с заданными пороговыми значениями, по нему делается предсказание целевой моделью, далее сравниваются пороговые значения. Если вероятность восхождения на холм (вероятность оценки модели) увеличивается, то параметры принимаются. Далее часть признаков случайным образом меняется и выполняется следующая итерация. После того, как сгенерируются теневые данные, обучаются теневые модели. Набор данных делится на тренировочный и тестовый, модели обучаются на тренировочном. Далее каждая модель, получая на вход и тренировочный и тестовый набор, делает предсказание. Выходу модели по обучающим данным присваивается метка “in”, то есть данные присутствовали при обучении, у тестового “out”, то есть его не было при обучении. Получившиеся предсказания теневых моделей с метками объединяются в один набор данных для обучения атакующей модели. , можно рассматривать finetune модели (дообучение), либо переборов настроек гиперпараметров модели. В данном случае с помощью затравок на персональные данные мы учим модель генерировать тексты с нужным нам смыслом и форматом, а затем используем любой из приведённых выше подходов. Сразу отметим, что третий способ мы не использовали, так как теневые модели должны быть похожи на целевую, а у нас не было мощностей для обучения подобных GPT. Для обучения одной самой малой модели с 125 млн параметров средствами Google Colab потребуется порядка полугода. В результате работы с моделями удалось получить следующее: По итогам наших экспериментов нам так и не удалось получить способ, который с высокой вероятностью извлекал бы обучающие данные. Те немногие персональные данные, которые мы получили, были результатом проверки сотен сгенерированных и отобранных текстов в глобальной сети. При этом не все из найденных ПДН критичны (например, данные по Ветеранам Великой Отечественной Войны). Единственное, что у нас получилось генерировать с высокой степенью вероятности – это реальные адреса. В этом случае хорошо работает параметр по beam search. Также многое зависит от того, как обрабатывался набор обучающих данных, использовались ли при обучении методы Дифференциальной конфиденциальности (например, намеренное внесение шума), регуляризации, чистился ли обучающий набор от чувствительной информации или нет. Так в своей разработчики указывают, что « ». Поэтому возможно никаких особо критичных данных в обучающем наборе для этих моделей изначально и не было.",
  "tag": "Машинное обучение",
  "hub": 6,
  "author": 1,
  "add_datatime": "2021-10-15T06:40:59.980Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 26,
 "fields": {
  "name": "Apache Airflow и будущее инжиниринга данных: вопрос и ответы",
  "image": "1634280157.085201.jpeg",
  "text": "Автор оригинала:   и  Введение Отличные ребята из Astronomer.io связались со мной чтобы сделать короткое интервью об Apache   и  . Ниже несколько вопросов и мои ответы. Вопрос 1: Когда выйдет следующий релиз Apache Airflow и какую из основных фичей ты считаешь главной? 8orc4 (релиз кандидат №4) только что был одобрен комьюнити Apache, но был задержан, после того как разработчики из Airbnb нашли несколько блокеров. Прямо сейчас почти все исправлено и релиз уже близко. Мы ожидаем, что релиз 1.8.0 выйдет на этой или на следующей неделе. Это первый релиз, который “драйвил” только Airbnb. Bolke de Bruin из ING (Нидерланды) проделал впечатляющую работу, чтобы релиз вышел. Это гигантский объем работы, который все время увеличивался, так как время с последнего релиза выросло. Это первый релиз с версии 1.7.1.3, которая вышла 13 июня 2016. Изменения по сравнению с 1.7.1.3 огромны. Ниже мой список хайлайтов: Надеемся получить более стабильный релиз согласно этому списку, хотя пока нет официального обязательства сделать это. Вопрос 2: Как прошел переход с внутреннего инструмента Airbnb на проект Apache? Все прошло гладко. Apache расширил вклад сообщества позволив внешним контрибьютерам смерджить пуллреквесты, что ускорило скорость изменения проекта. С другой стороны, это замедлило выпуск релиза, заставляя нас справляться с нашей собственной релизной веткой, состоящей из предыдущего официального релиза плюс список “вишенок” (список коммитов, которые мы делаем поверх релиза). Мы также склоняемся к тому, чтобы развивать свою внутреннюю ветку и пушить пуллреквест в коммьюнити, после того как стабилизируем его у себя на продакшене. Мы очень оценили помощь, которую получили в разработке последнего релиза, и наблюдали как проект развивается самостоятельно при минимальном участии с нашей стороны. Раньше я сам ревьюил и мерджил каждый пуллреквест и за последние несколько лет это вышло из-под контроля. Приятно видеть выход из этого “порочного круга” с течением времени. Вопрос 3: Каким ты видишь продолжение развития Airflow? Какие новые применения Airflow появятся в течение следующих 5 лет? Экосистеме инфраструктуры данных еще предстоит продемонстрировать какие-либо признаки превращения в нечто более управляемое. Кажется, мы все еще находимся в стадии расширения, когда каждый день приносит новую распределенную базу данных, новые фреймворки, новые библиотеки и новых коллег. По мере того, как эти системы становятся все более сложными и быстро развиваются становится еще более важным иметь что-то вроде Airflow, который объединяет все вместе в определенном месте, где каждый маленький кусочек пазла может быть правильно оркестрован с помощью “разумного” API. Можно предположить, что интеграция с другими системами будет зоной роста Airflow, по мере того как в нем появятся все фичи из мира оркестровки. Там, где изначально предполагалось, что Airflow будет использоваться главным образом как оркестратор, а не брать на себя реальную нагрузку, оказалось что многие используют Airflow для более сложных задач таких как: запуск скриптов на R, задачи обработки данных на Python, обучение ML моделей, ранжирование… В то время как мы внутренне поощряем людей писать сервисы и использовать инфраструктуру, такую как Kubernetes или Yarn для такого рода задач, похоже, что Airflow нужно расти и в этом направлении тоже, с поддержкой контейнеризации (пожалуйста, запустите эту задачу в этом контейнере Docker!), и управлениями ресурсами (выделите 4 CPU и 64 ГБ оперативной памяти для этой задачи, пожалуйста). Мы знаем об ограничениях, которые могут возникнуть у людей в их средах, и хотим позволить им получить максимальную отдачу от Airflow. Так что если у вас завалялся кластер Kubernetes, мы должны максимально использовать его, но если вы этого не хотите, хотелось бы, чтобы вы смогли запустить эти в Airflow. Я верю, что Airflow подходит для того, чтобы стать батч процессинг оркестратором, который будет доминировать в ближайшие 5 лет. У нас есть прочная технологическая основа и большое, динамично развивающееся сообщество! Вопрос 4: Как ты оцениваешь подобные технологии, такие как Luigi, Azkaban и тд? Сам я не использовал Luigi, Azkaban или Oozie, так что я просто повторю слова тех, кто перешел из этих сообществ и присоединились к Airflow. Про Luigi - в области применения он проще, чем Airflow, и, возможно, мы больше дополняем друг друга, чем конкурируем. Из того что я знаю - основной мэйнтэйнер проекта ушел из Spotify и, по-видимому, теперь они используют Airflow внутри, по крайне мере для некоторых своих задач. Я не знаю всей истории и хотел бы услышать больше. Я полагаю, что многие компании, выбирающие Luigi сегодня, возможно в будущем также выберут Airflow, когда им понадобятся доп. функции. которые есть в Airflow. Про Azkaban - мне не очень понятно, кто его использует вне Linkedin. Кажется, что проект сейчас не имеет активного сообщества и я сомневаюсь, что он будет развиваться в этом направлении. Я слышал о нескольких забавных случаях, когда компании использовали Azkaban вне Linkedin, но только тогда, когда кто-то из Linkedin близкий к проекту приходил и хотел использовать только то, что знает. Oozie — это то ПО, о котором я слышал больше всего негативных отзывов. Попробуйте найти хоть одного пользователя Oozie (но только не из разработчиков самого Oozie), который бы положительно отзывался о нем. Просто попробуйте! Возможно, это именно тот случай, когда проект закрыл основные проблемы, но люди продолжают жаловаться, я думаю что проект уже скомпрометирован и это не исправить. Я твердо верю в конфигурацию как код, как способ создания рабочих процессов, и я вижу, что актуальность Airflow в современной экосистеме данных неуклонно растет. Определенно кажется, что каждый стартап, серьезно относящийся к данным и аналитике в районе Bay Area, на данный момент использует Airflow. Вопрос 5: Как дата инжиниринг как дисциплина изменит стартапы в ближайшие 5 -10 лет? Современные стартапы больше не относятся к аналитике и данным как к чему-то второстепенному. Как правило они нанимают первого data scientist`а на ранней стадии и первая волна разработчиков будет использовать необходимые аналитические инструменты в ранних версиях продукта. Венчурные инвесторы требуют отчетности и могут предоставить услуги “grow hacker” на ранней стадии, чтобы дать рекомендации стартапу и измерить их потенциальную отдачу от инвестиций и посмотреть, где можно удвоить. Я думаю, что будущие стартапы будут катапультированы вверх по кривой зрелости данных с доступом к лучшему, более дешевому и доступному аналитическому ПО и услугам. Большая часть работы уже реализуется с помощью опенсорса, но также растет число интегрированных решений поставщиков, таких как MixPanel, Interana, Optimizely, и растет предложение облачных провайдеров, таких как AWS, GCS и Microsoft. Вещи, которые раньше были передовыми, такие как OLAP в реальном времени, обнаружение аномалий, A/B-тестирование и сегментация пользователей, когортный анализ, теперь доступны любому стартапу с любым количеством сотрудников и должным финансированием. В то время как эти предложения становятся все более доступными, они становятся необходимостью для поддержания конкурентоспособности и предоставляют возможности гибким стартапам перепрыгивать через медленных игроков в этой области. Заключение.",
  "tag": "opensourse",
  "hub": 6,
  "author": 1,
  "add_datatime": "2021-10-15T06:42:37.086Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 27,
 "fields": {
  "name": "Компоненты-шаблоны или «скелеты» в Angular: заимствуем идеи у Material table и Material tree",
  "image": "",
  "text": "В больших бизнес-приложениях часто встречаются повторяющиеся по структуре интерфейсы, но с разными элементами внутри. Например граф, динамический список, какие-нибудь мульти-табы со сравнением сущностей, степперы с кастомной логикой,  интерфейсы группировки, итд. Все эти интерфейсы часто объединяет одно: есть некая структура данных, которую нужно отобразить определенным образом, а на нижнем уровне этой структуры могут быть сущности разного типа.  В парадигме ООП эта задача успешно решается дженериками. Однако на уровне шаблонов многие не парятся и пишут либо частично дублирующие друг друга компоненты, в которых хардкодом прописывают нужную структуру с привязкой к конкретному типу сущности нижнего уровня. Либо создают настраиваемый через @Input компонент, который по итогу получается раздутым и в конце концов неподдерживаемым и с кучей костылей. На самом деле в шаблонах Angular есть механизмы, с помощью сочетания которых можно абстрагироваться от сущностей нижнего уровня и создавать компоненты - структурные шаблоны. Эти механизмы используются в Material / CDK, например в mat-table, mat-tree.  Если обобщить подход, используемый в этих компонентах - то это некая обертка-контейнер, в которую прокидывается структура данных, а вспомогательные директивы и компоненты позволяют отобразить сущности нижнего уровня в составе структуры (строки и столбцы таблицы, ноды дерева, итд) Структурные директивы внутри контейнера: прокидываем данные текущей сущности через контекст Примеры от Материал:  Концептуально видим, что в контекст *matCellDef или *matTreeNodeDef прокидывается сущность нижнего уровня, которую мы уже отображаем нужным нам образом, используя компонент-атом или компонент-молекулу из нашей дизайн-системы.  К примеру, в макетах нашего бизнес-приложения мы видим много подобных кейсов: при клике по карточкам справа мы как будто переходим по табам и отображаем информацию слева по выбранной сущности. При нажатии на карточку с плюсиком, у нас должна добавится пустая карточка, а слева отобразится форма создания сущности. причем для разных сущностей формы и блоки информации совершенно разные. А еще на карточке и на странице информации есть контрол, который должны удалять текущий таб, а также контрол, который должен переключать текущий таб в режим редактирования. Сделаем, чтобы в наших fature-компонентах было так: Здесь app-my-card, app-my-details-page, app-my-form - это какие-то компоненты из нашей дизайн-системы в разных фичах для разных сущностей они будут разные. Наша задача сделать скелет: компонент-контейнер и структнрные директивы. Реализуем компонент-контейнер Структурные директивы будут все однотипные, различаются только контекстом: Сразу говорю, пример не реальный и выдуман мной на ходу, но отражает общую концепцию, которой  я пользуюсь. Тут в принципе может быть любая структура интерфейса: граф с событиями добавления, удаления и перемещения узла, дерево с нестандартной логикой, но специфичной для проекта и часто повторяющейся в проекте с разными сущностями, итд. Также я не претендую на работоспособность написанного выше кода, цель статьи - показать суть подхода в целом.\"",
  "tag": "angular",
  "hub": 6,
  "author": 1,
  "add_datatime": "2021-10-15T06:43:15.254Z",
  "is_active": true
 }
},
{
 "model": "mainapp.article",
 "pk": 28,
 "fields": {
  "name": "Нам нужны не дата-саентисты, а дата-инженеры",
  "image": "1634280263.634489.jpeg",
  "text": "Данные. Они повсюду и их . За последние 5-10 лет привлекла множество новичков, пытающихся ощутить вкус этого запретного плода. Но как сегодня выглядит ситуация с наймом в ? Вот краткое изложение статьи в двух предложениях. : в компаниях на должности , чем на должности . Так как мы обучаем новое поколение практиков в сфере обработки данных и машинного обучения, давайте сделаем больший упор на инженерные навыки. Так как моя работа заключается в разработке для профессионалов в области данных, я много думаю о том, как эволюционирует рынок вакансий, связанных с данными (машинное обучение и data science). Общаясь с десятками перспективных новичков в сфере данных, в том числе и со студентами лучших вузов мира, я увидел серьёзное недопонимание того, какие навыки являются наиболее важными, помогают выделиться из толпы и подготовиться к карьере. может работать в любом сегменте следующих сфер: моделирование машинного обучения, визуализация, очистка и обработка данных (например, преобразование данных для SQL), проектирование и развёртывание на производстве. С чего вообще начинать рекомендации курса обучения для новичков? Данные говорят громче слов. Поэтому я решил провести анализ должностей в сфере данных, на которые есть вакансии у компаний, выходивших из с 2012 года. Вопросы, которыми я руководствовался в своих исследованиях: Если вы хотите узнать подробности и изучить анализ, то продолжайте чтение. Методология Я решил провести анализ компаний из портфолио YC, которые, по их заявлению, выполняют какие-нибудь работы с данными. Зачем фокусироваться на YC? Ну, во-первых, он ведёт , в котором легко выполнять поиск и скрейпинг. Кроме того, как особо дальновидный инкубатор, в течение более чем десятка лет финансировавший компании по всему миру в разных отраслях, он, по моему мнению, образовал репрезентативную выборку рынка для проведения моего анализа. Тем не менее, относитесь ко всему написанному с долей скептицизма, потому что я не анализировал очень крупные технологические компании. Скрейпингом я обработал URL главных страниц каждой компании YC с 2012 года, создав первоначальный пул из примерно 1400 компаний. Зачем останавливаться на 2012 годе? В 2012 году выиграла конкурс ImageNet, по сути, породив ту волну машинного обучения и моделирования данных, которую мы сейчас переживаем. Справедливо будет сказать, что это породило одно из первых поколений компаний, делающих основной упор на данные. В этом первоначальном пуле я произвёл фильтрацию по ключевым словам, чтобы уменьшить количество релевантных компаний. В частности, я учитывал только компании, на веб-сайтах которых присутствовал хотя бы один из следующих терминов: AI, CV, NLP, natural language processing, computer vision, artificial intelligence, machine, ML, data. Также я не учитывал компании с поломанными ссылками на веб-сайты. Появилась ли у меня куча ложноположительных данных? Конечно! Но на этом этапе я хотел обеспечить как можно более широкий охват, чтобы дальше можно было провести более тонкое исследование отдельных веб-сайтов вручную. Уменьшив размер пула, я обошёл каждый сайт, нашёл раздел с вакансиями (обычно он называется , или ), и зафиксировал каждую должность, в названии которой были указаны данные, машинное обучение, обработка естественных языков или компьютерное зрение. Это дало мне пул из примерно 70 компаний. Примечание: вполне возможно, что я пропустил некоторые компании, потому что на отдельных веб-сайтах было очень мало информации, хотя компании могли и иметь вакансии. Кроме того, я встречал компании, у которых не было формальной страницы , а только просьба связываться напрямую по электронной почте. Я игнорировал оба типа компаний и не писал им, поэтому в моём анализе они не учитываются. Ещё один аспект: основная часть этого исследования была проведена в последние недели 2020 года. Открытые вакансии могли измениться, потому что компании периодически обновляют страницы. Однако я не считаю, что это значительно повлияет на выводы. В чём заключаются обязанности практиков по работе с данными? Прежде чем приступать к результатам, стоит немного уточнить, какие обязанности имеют люди, работающие на каждой из должностей. Вот четыре должности, которые мы будем изучать, и краткое описание их обязанностей: Сколько существует должностей по работе с данными? Давайте попробуем составить график частоты каждой из должностей, на которые нанимают сотрудников компании. График выглядит так: Сразу же становится заметным, что гораздо больше вакансий по сравнению с традиционными . В данном случае компании нанимают дата-инженеров, чем дата-саентистов, и примерно одинаковое количество инженеров по машинному обучению и дата-саентистов. Но мы можем пойти глубже. Если посмотреть на названия различных должностей, то заметны повторения. Давайте консолидируем должности, разбив их на приблизительные категории. Другими словами, я взял должности, описания которых приблизительно аналогичны, и объединил их под одним названием. Использовался следующий набор отношений эквивалентности: Если вам не нравится работать с сырыми значениями, то вот процентное соотношение, чтобы упростить понимание: Вероятно, можно было объединить или с , или с , но учитывая, что это довольно гибридная должность, я оставил всё как есть. В целом, консолидация сделала различия ещё более явными! Существует открытых вакансий , чем . Кроме того, открытых вакансий , чем . Также есть всего вакансий от количества вакансий . Выводы Спрос на значительно выше, чем на другие связанные с данными профессии. В каком-то смысле, это отражает эволюцию отрасли в целом. Когда 5-8 лет назад популярность получило машинное обучение, компании решили, что им нужны люди, способные создавать классификаторы данных. Но потом фреймворки наподобие и сильно выросли в качестве, упростив вход в область глубокого и машинного обучения. Это сделало набор навыков для моделирования данных общедоступным. Сегодня узким местом стала помощь компаниям во внедрении результатов машинного обучения и моделирования в производство из-за проблем с данными. Как аннотировать данные? Как обрабатывать и очищать данные? Как перемещать их из А в Б? Как делать это ежедневно и как можно быстрее? Всё это сводится к необходимости хороших инженерных навыков. Это может казаться скучным и несексуальным, но, возможно, сегодня нам нужно олдскульное проектирование ПО с уклоном в данные. Благодаря крутым демо и ажиотажу в медиа мы годами были очарованы идеей о профессионалах обработки данных, вдыхающих жизнь в сырые данные. Вспомните, когда в последний раз вы видели статью на о конвейере ETL? Во всяком случае, мне кажется, что мы делаем недостаточный упор на крепкие инженерные навыки в подготовке к работе в data science и в обучающих программах. Кроме обучения пользованию стоит учиться и написанию юнит-тестов! Значит ли это, что вам не надо изучать data science? Нет. Это значит, что конкуренция будет жёстче. Для избытка новичков на рынке, обученных data science, будет меньше вакансий. Всегда будет существовать спрос на людей, способных эффективно анализировать данные и извлекать из них важную информацию. Но они должны быть хороши. Скачивания предварительно обученной модели с веб-сайта Tensorflow на основе , вероятно, больше недостаточно, чтобы получить работу в data science. Однако очевидно, что при наличии множества вакансий компании часто хотят получить гибридного специалиста по данным — того, кто способен создавать и внедрять модели. Или, если говорить кратко, того, кто может пользоваться Tensorflow, но также способен собрать его из исходников. Можно сделать и ещё один вывод: вакансий в исследованиях машинного обучения не так уж много. Исследования машинного обучения получают свою долю хайпа, потому что именно в нём происходит прогресс, все эти , и прочее. Однако для многих компаний, особенно находящихся на ранних этапах развития, больше необязательно находиться на рубеже прогресса. Часто для них ценнее получить модель, выполняющую задачи на 90%, но способную масштабироваться до тысячи и более пользователей. Я не хочу сказать, что исследования машинного обучения не важны. Разумеется, это совершенно не так. Но вы, вероятно, найдёте больше подобных вакансий в исследовательских лабораториях, способных делать большие капиталовложения в течение долгих периодов времени, а не в стартапе на первом этапе финансирования, пытающемся продемонстрировать инвесторам свою пригодность для рынка продуктов. Я считаю, что важно калибровать ожидания новичков в сфере обработки данных. Мы должны признать, что . Надеюсь, этот пост позволил пролить свет на современное состояние отрасли. Только узнав, где мы находимся, мы поймём, куда стоит двигаться.\"",
  "tag": "data science",
  "hub": 6,
  "author": 1,
  "add_datatime": "2021-10-15T06:44:23.634Z",
  "is_active": true
 }
},
{
 "model": "authapp.intergalacticuser",
 "pk": 1,
 "fields": {
  "password": "pbkdf2_sha256$260000$xhJFItWWzoZLvsZcigZyhx$Y3v3f96qSFMXi8zMNKA/0bmiqlTzxGRcmkRN9mYSkpM=",
  "last_login": "2021-10-15T05:39:12.794Z",
  "is_superuser": true,
  "username": "admin",
  "first_name": "",
  "last_name": "",
  "email": "",
  "is_staff": true,
  "is_active": true,
  "date_joined": "2021-10-10T19:33:15.056Z",
  "avatar": "",
  "age": null,
  "sex": "not selected",
  "send_messages": true,
  "groups": [],
  "user_permissions": []
 }
}
]
